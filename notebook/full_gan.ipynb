{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"full_gan.ipynb","provenance":[],"collapsed_sections":["uJxZOynbvCi0","wBAlTfn2VChf","N_UU7aUm5TNu","uNtm9EigvvGB","Q5MVmmvLa7v-","3ZWeBEM6HJGF","CTbcBnTYXEcs","qpF7cRPSXcwQ","5E05mvxC0-dS"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9gX_t_oXYTZM","colab_type":"text"},"source":["# GAN Project\n","Jimmy Hickey and David Elsheimer\n","\n","ST790\n","\n","01-05-2020\n","\n","In an effort to keep things self contained, we tried to keep all of the code for each section within that section (aside from the Colab Setup). This is why you will see a lot of repeated code.\n","\n","Much of our code is adapted from \n","\n","* https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/\n","* https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uJxZOynbvCi0","colab_type":"text"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"9BpDawTSZ0B_","colab_type":"code","outputId":"4ef49d67-1868-4131-bd0a-c3e2374953fb","executionInfo":{"status":"ok","timestamp":1587928624652,"user_tz":240,"elapsed":3820,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":378}},"source":["!pip install keras==2.3.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 34.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 22.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 14.3MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.18.3)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.1.0)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.3.1\n","    Uninstalling Keras-2.3.1:\n","      Successfully uninstalled Keras-2.3.1\n","Successfully installed keras-2.3.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"X2aIyygWkACq","colab_type":"code","outputId":"cf4293c1-1f6b-418e-e3cf-deaeb8cb8d2f","executionInfo":{"status":"ok","timestamp":1587994200954,"user_tz":240,"elapsed":23137,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wBAlTfn2VChf","colab_type":"text"},"source":["# Vanilla GAN MNIST"]},{"cell_type":"code","metadata":{"id":"eLlHgoBHzB_8","colab_type":"code","cellView":"both","colab":{}},"source":["# example of training a stable gan for generating a handwritten digit\n","from os import makedirs\n","from numpy import expand_dims\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from tensorflow.keras.datasets.mnist import load_data\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.initializers import RandomNormal\n","from matplotlib import pyplot\n","\n","from tensorflow.keras import backend as K\n","\n","\n","# define the standalone discriminator model\n","def define_discriminator(in_shape=(28, 28, 1)):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # define model\n","    model = Sequential()\n","    # downsample to 14x14\n","    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init, input_shape=in_shape))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    # downsample to 7x7\n","    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    # classifier\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","\n","def define_generator(latent_dim):\n","    \"\"\"Creates a generator model that takes a 100-dimensional noise vector as a \"seed\",\n","    and outputs images of size 28x28x1.\"\"\"\n","    model = Sequential()\n","    model.add(Dense(1024, input_dim=latent_dim))\n","    model.add(LeakyReLU())\n","    model.add(Dense(128 * 7 * 7))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","    if K.image_data_format() == 'channels_first':\n","        model.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n","        bn_axis = 1\n","    else:\n","        model.add(Reshape((7, 7, 128), input_shape=(128 * 7 * 7,)))\n","        bn_axis = -1\n","    model.add(Conv2DTranspose(128, (5, 5), strides=2, padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","    model.add(Conv2D(64, (5, 5), padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","    model.add(Conv2DTranspose(64, (5, 5), strides=2, padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","\n","    # Because we normalized training inputs to lie in the range [-1, 1],\n","    # the tanh function should be used for the output of the generator to ensure\n","    # its output also lies in this range.\n","    model.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n","    return model\n","\n","# define the combined generator and discriminator model, for updating the generator\n","def define_gan(generator, discriminator):\n","    # make weights in the discriminator not trainable\n","    discriminator.trainable = False\n","    # connect them\n","    model = Sequential()\n","    # add generator\n","    model.add(generator)\n","    # add the discriminator\n","    model.add(discriminator)\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n","\n","\n","# load mnist images\n","def load_real_samples():\n","    # load dataset\n","    (trainX, trainy), (_, _) = load_data()\n","    # expand to 3d, e.g. add channels\n","    X = expand_dims(trainX, axis=-1)\n","    # select all of the examples for a given class\n","    selected_ix = trainy == 5\n","    X = X[selected_ix]\n","    # convert from ints to floats\n","    X = X.astype('float32')\n","    # scale from [0,255] to [-1,1]\n","    X = (X - 127.5) / 127.5\n","    return X\n","\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","    # choose random instances\n","    ix = randint(0, dataset.shape[0], n_samples)\n","    # select images\n","    X = dataset[ix]\n","    # generate class labels\n","    y = ones((n_samples, 1))\n","    return X, y\n","\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","    # generate points in the latent space\n","    x_input = randn(latent_dim * n_samples)\n","    # reshape into a batch of inputs for the network\n","    x_input = x_input.reshape(n_samples, latent_dim)\n","    return x_input\n","\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","    # generate points in latent space\n","    x_input = generate_latent_points(latent_dim, n_samples)\n","    # predict outputs\n","    X = generator.predict(x_input)\n","    # create class labels\n","    y = zeros((n_samples, 1))\n","    return X, y\n","\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","    # prepare fake examples\n","    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","    # scale from [-1,1] to [0,1]\n","    X = (X + 1) / 2.0\n","    # plot images\n","    for i in range(10 * 10):\n","        # define subplot\n","        pyplot.subplot(10, 10, 1 + i)\n","        # turn off axis\n","        pyplot.axis('off')\n","        # plot raw pixel data\n","        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","    # save plot to file\n","    pyplot.savefig('results_baseline/plots/plot_%04d.png' % (step + 1))\n","    pyplot.close()\n","    # save the generator model\n","    g_model.save('results_baseline/models/model_%04d.h5' % (step + 1))\n","\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n","    # plot loss\n","    pyplot.subplot(2, 1, 1)\n","    pyplot.plot(d1_hist, label='d-real')\n","    pyplot.plot(d2_hist, label='d-fake')\n","    pyplot.plot(g_hist, label='gen')\n","    pyplot.legend()\n","    # plot discriminator accuracy\n","    pyplot.subplot(2, 1, 2)\n","    pyplot.plot(a1_hist, label='acc-real')\n","    pyplot.plot(a2_hist, label='acc-fake')\n","    pyplot.legend()\n","    # save plot to file\n","    pyplot.savefig('results_baseline/plot_line_plot_loss.png')\n","    pyplot.close()\n","\n","\n","# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1, n_batch=128):\n","    # calculate the number of batches per epoch\n","    bat_per_epo = int(dataset.shape[0] / n_batch)\n","    # calculate the total iterations based on batch and epoch\n","    n_steps = bat_per_epo * n_epochs\n","    # calculate the number of samples in half a batch\n","    half_batch = int(n_batch / 2)\n","    # prepare lists for storing stats each iteration\n","    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n","    # manually enumerate epochs\n","    for i in range(n_steps):\n","        # get randomly selected 'real' samples\n","        X_real, y_real = generate_real_samples(dataset, half_batch)\n","        # update discriminator model weights\n","        d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n","        # generate 'fake' examples\n","        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","        # update discriminator model weights\n","        d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n","        # prepare points in latent space as input for the generator\n","        X_gan = generate_latent_points(latent_dim, n_batch)\n","        # create inverted labels for the fake samples\n","        y_gan = ones((n_batch, 1))\n","        # update the generator via the discriminator's error\n","        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n","        # summarize loss on this batch\n","        print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %\n","              (i + 1, d_loss1, d_loss2, g_loss, int(100 * d_acc1), int(100 * d_acc2)))\n","        # record history\n","        d1_hist.append(d_loss1)\n","        d2_hist.append(d_loss2)\n","        g_hist.append(g_loss)\n","        a1_hist.append(d_acc1)\n","        a2_hist.append(d_acc2)\n","        # evaluate the model performance every 'epoch'\n","        if (i + 1) % bat_per_epo == 0:\n","            summarize_performance(i, g_model, latent_dim)\n","    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1NQiqpJBXUg","colab_type":"code","outputId":"cc52d07b-ce10-420c-f3a4-2d27b47d502c","executionInfo":{"status":"ok","timestamp":1587685932253,"user_tz":240,"elapsed":119039,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import shutil\n","\n","shutil.rmtree('results_baseline')\n","\n","# make folder for results\n","makedirs('results_baseline/models', exist_ok=True)\n","makedirs('results_baseline/plots', exist_ok=True)\n","\n","# size of the latent space\n","latent_dim = 100\n","\n","n_batch = 64\n","epochs = 20\n","\n","# create the discriminator\n","discriminator = define_discriminator()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, discriminator)\n","# load image data\n","dataset = load_real_samples()\n","print(dataset.shape)\n","# train model\n","train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=epochs, n_batch=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(5421, 28, 28, 1)\n",">1, d1=0.965, d2=0.873 g=0.683, a1=40, a2=9\n",">2, d1=0.194, d2=1.103 g=0.666, a1=100, a2=3\n",">3, d1=0.140, d2=1.098 g=0.654, a1=100, a2=3\n",">4, d1=0.086, d2=0.974 g=0.653, a1=100, a2=12\n",">5, d1=0.055, d2=0.629 g=0.654, a1=100, a2=59\n",">6, d1=0.098, d2=0.399 g=0.656, a1=100, a2=96\n",">7, d1=0.073, d2=0.252 g=0.658, a1=100, a2=100\n",">8, d1=0.042, d2=0.201 g=0.658, a1=100, a2=100\n",">9, d1=0.070, d2=0.189 g=0.659, a1=100, a2=100\n",">10, d1=0.050, d2=0.207 g=0.658, a1=100, a2=100\n",">11, d1=0.052, d2=0.290 g=0.660, a1=100, a2=100\n",">12, d1=0.049, d2=0.274 g=0.663, a1=100, a2=100\n",">13, d1=0.050, d2=0.183 g=0.666, a1=100, a2=100\n",">14, d1=0.068, d2=0.113 g=0.663, a1=100, a2=100\n",">15, d1=0.070, d2=0.132 g=0.658, a1=100, a2=100\n",">16, d1=0.049, d2=0.189 g=0.658, a1=100, a2=100\n",">17, d1=0.048, d2=0.202 g=0.655, a1=100, a2=100\n",">18, d1=0.037, d2=0.449 g=0.659, a1=100, a2=96\n",">19, d1=0.084, d2=0.300 g=0.665, a1=100, a2=100\n",">20, d1=0.073, d2=0.115 g=0.661, a1=100, a2=100\n",">21, d1=0.076, d2=0.173 g=0.652, a1=100, a2=100\n",">22, d1=0.071, d2=0.126 g=0.643, a1=100, a2=100\n",">23, d1=0.054, d2=0.090 g=0.637, a1=100, a2=100\n",">24, d1=0.059, d2=0.225 g=0.637, a1=100, a2=100\n",">25, d1=0.057, d2=0.077 g=0.632, a1=100, a2=100\n",">26, d1=0.045, d2=0.100 g=0.624, a1=100, a2=100\n",">27, d1=0.070, d2=0.111 g=0.619, a1=96, a2=100\n",">28, d1=0.047, d2=0.057 g=0.613, a1=100, a2=100\n",">29, d1=0.041, d2=0.090 g=0.605, a1=100, a2=100\n",">30, d1=0.033, d2=0.053 g=0.600, a1=100, a2=100\n",">31, d1=0.057, d2=0.025 g=0.585, a1=100, a2=100\n",">32, d1=0.032, d2=0.053 g=0.575, a1=100, a2=100\n",">33, d1=0.045, d2=0.020 g=0.556, a1=100, a2=100\n",">34, d1=0.026, d2=0.037 g=0.541, a1=100, a2=100\n",">35, d1=0.035, d2=0.020 g=0.529, a1=100, a2=100\n",">36, d1=0.025, d2=0.024 g=0.519, a1=100, a2=100\n",">37, d1=0.023, d2=0.021 g=0.506, a1=100, a2=100\n",">38, d1=0.019, d2=0.013 g=0.493, a1=100, a2=100\n",">39, d1=0.017, d2=0.013 g=0.473, a1=100, a2=100\n",">40, d1=0.028, d2=0.010 g=0.459, a1=100, a2=100\n",">41, d1=0.011, d2=0.007 g=0.444, a1=100, a2=100\n",">42, d1=0.012, d2=0.012 g=0.432, a1=100, a2=100\n",">43, d1=0.007, d2=0.007 g=0.422, a1=100, a2=100\n",">44, d1=0.008, d2=0.006 g=0.410, a1=100, a2=100\n",">45, d1=0.014, d2=0.007 g=0.398, a1=100, a2=100\n",">46, d1=0.011, d2=0.007 g=0.382, a1=100, a2=100\n",">47, d1=0.020, d2=0.005 g=0.374, a1=100, a2=100\n",">48, d1=0.014, d2=0.004 g=0.362, a1=100, a2=100\n",">49, d1=0.008, d2=0.007 g=0.349, a1=100, a2=100\n",">50, d1=0.010, d2=0.006 g=0.338, a1=100, a2=100\n",">51, d1=0.011, d2=0.005 g=0.330, a1=100, a2=100\n",">52, d1=0.011, d2=0.005 g=0.319, a1=100, a2=100\n",">53, d1=0.008, d2=0.004 g=0.310, a1=100, a2=100\n",">54, d1=0.005, d2=0.004 g=0.300, a1=100, a2=100\n",">55, d1=0.009, d2=0.006 g=0.291, a1=100, a2=100\n",">56, d1=0.005, d2=0.003 g=0.283, a1=100, a2=100\n",">57, d1=0.006, d2=0.004 g=0.271, a1=100, a2=100\n",">58, d1=0.008, d2=0.004 g=0.265, a1=100, a2=100\n",">59, d1=0.005, d2=0.005 g=0.255, a1=100, a2=100\n",">60, d1=0.006, d2=0.004 g=0.247, a1=100, a2=100\n",">61, d1=0.004, d2=0.004 g=0.240, a1=100, a2=100\n",">62, d1=0.006, d2=0.006 g=0.233, a1=100, a2=100\n",">63, d1=0.004, d2=0.003 g=0.224, a1=100, a2=100\n",">64, d1=0.005, d2=0.004 g=0.215, a1=100, a2=100\n",">65, d1=0.005, d2=0.004 g=0.208, a1=100, a2=100\n",">66, d1=0.008, d2=0.004 g=0.202, a1=100, a2=100\n",">67, d1=0.010, d2=0.004 g=0.194, a1=100, a2=100\n",">68, d1=0.003, d2=0.005 g=0.187, a1=100, a2=100\n",">69, d1=0.004, d2=0.003 g=0.181, a1=100, a2=100\n",">70, d1=0.008, d2=0.004 g=0.174, a1=100, a2=100\n",">71, d1=0.008, d2=0.002 g=0.167, a1=100, a2=100\n",">72, d1=0.003, d2=0.007 g=0.162, a1=100, a2=100\n",">73, d1=0.005, d2=0.003 g=0.156, a1=100, a2=100\n",">74, d1=0.004, d2=0.006 g=0.151, a1=100, a2=100\n",">75, d1=0.005, d2=0.002 g=0.146, a1=100, a2=100\n",">76, d1=0.002, d2=0.003 g=0.141, a1=100, a2=100\n",">77, d1=0.004, d2=0.005 g=0.136, a1=100, a2=100\n",">78, d1=0.004, d2=0.005 g=0.131, a1=100, a2=100\n",">79, d1=0.002, d2=0.003 g=0.127, a1=100, a2=100\n",">80, d1=0.003, d2=0.002 g=0.121, a1=100, a2=100\n",">81, d1=0.003, d2=0.004 g=0.116, a1=100, a2=100\n",">82, d1=0.004, d2=0.004 g=0.114, a1=100, a2=100\n",">83, d1=0.003, d2=0.004 g=0.109, a1=100, a2=100\n",">84, d1=0.003, d2=0.003 g=0.104, a1=100, a2=100\n",">85, d1=0.002, d2=0.004 g=0.101, a1=100, a2=100\n",">86, d1=0.004, d2=0.003 g=0.097, a1=100, a2=100\n",">87, d1=0.006, d2=0.003 g=0.093, a1=100, a2=100\n",">88, d1=0.006, d2=0.009 g=0.091, a1=100, a2=100\n",">89, d1=0.005, d2=0.003 g=0.088, a1=100, a2=100\n",">90, d1=0.004, d2=0.003 g=0.085, a1=100, a2=100\n",">91, d1=0.006, d2=0.004 g=0.081, a1=100, a2=100\n",">92, d1=0.003, d2=0.003 g=0.079, a1=100, a2=100\n",">93, d1=0.003, d2=0.004 g=0.075, a1=100, a2=100\n",">94, d1=0.005, d2=0.003 g=0.072, a1=100, a2=100\n",">95, d1=0.002, d2=0.004 g=0.070, a1=100, a2=100\n",">96, d1=0.009, d2=0.005 g=0.066, a1=100, a2=100\n",">97, d1=0.003, d2=0.005 g=0.066, a1=100, a2=100\n",">98, d1=0.004, d2=0.005 g=0.063, a1=100, a2=100\n",">99, d1=0.003, d2=0.010 g=0.063, a1=100, a2=100\n",">100, d1=0.006, d2=0.004 g=0.062, a1=100, a2=100\n",">101, d1=0.005, d2=0.003 g=0.060, a1=100, a2=100\n",">102, d1=0.004, d2=0.002 g=0.057, a1=100, a2=100\n",">103, d1=0.003, d2=0.006 g=0.055, a1=100, a2=100\n",">104, d1=0.004, d2=0.002 g=0.054, a1=100, a2=100\n",">105, d1=0.002, d2=0.004 g=0.051, a1=100, a2=100\n",">106, d1=0.006, d2=0.002 g=0.049, a1=100, a2=100\n",">107, d1=0.003, d2=0.008 g=0.047, a1=100, a2=100\n",">108, d1=0.006, d2=0.003 g=0.046, a1=100, a2=100\n",">109, d1=0.003, d2=0.003 g=0.045, a1=100, a2=100\n",">110, d1=0.003, d2=0.006 g=0.044, a1=100, a2=100\n",">111, d1=0.004, d2=0.004 g=0.043, a1=100, a2=100\n",">112, d1=0.006, d2=0.002 g=0.041, a1=100, a2=100\n",">113, d1=0.002, d2=0.005 g=0.039, a1=100, a2=100\n",">114, d1=0.008, d2=0.009 g=0.040, a1=100, a2=100\n",">115, d1=0.003, d2=0.005 g=0.039, a1=100, a2=100\n",">116, d1=0.010, d2=0.002 g=0.038, a1=100, a2=100\n",">117, d1=0.007, d2=0.003 g=0.035, a1=100, a2=100\n",">118, d1=0.007, d2=0.004 g=0.034, a1=100, a2=100\n",">119, d1=0.004, d2=0.005 g=0.034, a1=100, a2=100\n",">120, d1=0.004, d2=0.002 g=0.032, a1=100, a2=100\n",">121, d1=0.004, d2=0.005 g=0.031, a1=100, a2=100\n",">122, d1=0.003, d2=0.005 g=0.031, a1=100, a2=100\n",">123, d1=0.004, d2=0.004 g=0.031, a1=100, a2=100\n",">124, d1=0.005, d2=0.014 g=0.034, a1=100, a2=100\n",">125, d1=0.009, d2=0.002 g=0.035, a1=100, a2=100\n",">126, d1=0.007, d2=0.002 g=0.033, a1=100, a2=100\n",">127, d1=0.005, d2=0.002 g=0.031, a1=100, a2=100\n",">128, d1=0.004, d2=0.002 g=0.029, a1=100, a2=100\n",">129, d1=0.002, d2=0.002 g=0.028, a1=100, a2=100\n",">130, d1=0.004, d2=0.005 g=0.028, a1=100, a2=100\n",">131, d1=0.005, d2=0.009 g=0.030, a1=100, a2=100\n",">132, d1=0.007, d2=0.003 g=0.031, a1=100, a2=100\n",">133, d1=0.002, d2=0.002 g=0.030, a1=100, a2=100\n",">134, d1=0.008, d2=0.003 g=0.029, a1=100, a2=100\n",">135, d1=0.004, d2=0.004 g=0.028, a1=100, a2=100\n",">136, d1=0.009, d2=0.006 g=0.028, a1=100, a2=100\n",">137, d1=0.005, d2=0.003 g=0.029, a1=100, a2=100\n",">138, d1=0.006, d2=0.002 g=0.028, a1=100, a2=100\n",">139, d1=0.005, d2=0.006 g=0.028, a1=100, a2=100\n",">140, d1=0.005, d2=0.003 g=0.027, a1=100, a2=100\n",">141, d1=0.008, d2=0.005 g=0.026, a1=100, a2=100\n",">142, d1=0.006, d2=0.003 g=0.026, a1=100, a2=100\n",">143, d1=0.005, d2=0.003 g=0.025, a1=100, a2=100\n",">144, d1=0.005, d2=0.003 g=0.025, a1=100, a2=100\n",">145, d1=0.003, d2=0.003 g=0.024, a1=100, a2=100\n",">146, d1=0.003, d2=0.005 g=0.024, a1=100, a2=100\n",">147, d1=0.004, d2=0.009 g=0.027, a1=100, a2=100\n",">148, d1=0.004, d2=0.003 g=0.029, a1=100, a2=100\n",">149, d1=0.008, d2=0.002 g=0.029, a1=100, a2=100\n",">150, d1=0.006, d2=0.001 g=0.026, a1=100, a2=100\n",">151, d1=0.008, d2=0.001 g=0.023, a1=100, a2=100\n",">152, d1=0.005, d2=0.002 g=0.021, a1=100, a2=100\n",">153, d1=0.005, d2=0.004 g=0.021, a1=100, a2=100\n",">154, d1=0.005, d2=0.002 g=0.020, a1=100, a2=100\n",">155, d1=0.004, d2=0.004 g=0.020, a1=100, a2=100\n",">156, d1=0.003, d2=0.004 g=0.019, a1=100, a2=100\n",">157, d1=0.004, d2=0.002 g=0.020, a1=100, a2=100\n",">158, d1=0.003, d2=0.003 g=0.020, a1=100, a2=100\n",">159, d1=0.005, d2=0.002 g=0.018, a1=100, a2=100\n",">160, d1=0.004, d2=0.004 g=0.018, a1=100, a2=100\n",">161, d1=0.002, d2=0.003 g=0.018, a1=100, a2=100\n",">162, d1=0.004, d2=0.003 g=0.019, a1=100, a2=100\n",">163, d1=0.005, d2=0.001 g=0.018, a1=100, a2=100\n",">164, d1=0.004, d2=0.009 g=0.020, a1=100, a2=100\n",">165, d1=0.003, d2=0.002 g=0.022, a1=100, a2=100\n",">166, d1=0.005, d2=0.002 g=0.021, a1=100, a2=100\n",">167, d1=0.003, d2=0.001 g=0.020, a1=100, a2=100\n",">168, d1=0.004, d2=0.005 g=0.020, a1=100, a2=100\n",">169, d1=0.007, d2=0.004 g=0.019, a1=100, a2=100\n",">170, d1=0.010, d2=0.005 g=0.019, a1=100, a2=100\n",">171, d1=0.002, d2=0.004 g=0.020, a1=100, a2=100\n",">172, d1=0.005, d2=0.005 g=0.022, a1=100, a2=100\n",">173, d1=0.003, d2=0.005 g=0.023, a1=100, a2=100\n",">174, d1=0.002, d2=0.001 g=0.023, a1=100, a2=100\n",">175, d1=0.009, d2=0.002 g=0.019, a1=100, a2=100\n",">176, d1=0.003, d2=0.004 g=0.019, a1=100, a2=100\n",">177, d1=0.004, d2=0.006 g=0.021, a1=100, a2=100\n",">178, d1=0.008, d2=0.004 g=0.021, a1=100, a2=100\n",">179, d1=0.004, d2=0.004 g=0.021, a1=100, a2=100\n",">180, d1=0.005, d2=0.009 g=0.025, a1=100, a2=100\n",">181, d1=0.004, d2=0.011 g=0.034, a1=100, a2=100\n",">182, d1=0.012, d2=0.006 g=0.040, a1=100, a2=100\n",">183, d1=0.009, d2=0.003 g=0.038, a1=100, a2=100\n",">184, d1=0.007, d2=0.002 g=0.032, a1=100, a2=100\n",">185, d1=0.013, d2=0.017 g=0.036, a1=100, a2=100\n",">186, d1=0.003, d2=0.002 g=0.044, a1=100, a2=100\n",">187, d1=0.008, d2=0.001 g=0.035, a1=100, a2=100\n",">188, d1=0.009, d2=0.001 g=0.024, a1=100, a2=100\n",">189, d1=0.004, d2=0.003 g=0.021, a1=100, a2=100\n",">190, d1=0.008, d2=0.004 g=0.018, a1=100, a2=100\n",">191, d1=0.005, d2=0.005 g=0.018, a1=100, a2=100\n",">192, d1=0.003, d2=0.009 g=0.024, a1=100, a2=100\n",">193, d1=0.007, d2=0.005 g=0.027, a1=100, a2=100\n",">194, d1=0.007, d2=0.002 g=0.026, a1=100, a2=100\n",">195, d1=0.002, d2=0.002 g=0.025, a1=100, a2=100\n",">196, d1=0.003, d2=0.003 g=0.025, a1=100, a2=100\n",">197, d1=0.004, d2=0.003 g=0.024, a1=100, a2=100\n",">198, d1=0.009, d2=0.001 g=0.020, a1=100, a2=100\n",">199, d1=0.002, d2=0.003 g=0.019, a1=100, a2=100\n",">200, d1=0.004, d2=0.002 g=0.017, a1=100, a2=100\n",">201, d1=0.003, d2=0.003 g=0.017, a1=100, a2=100\n",">202, d1=0.002, d2=0.009 g=0.023, a1=100, a2=100\n",">203, d1=0.007, d2=0.004 g=0.030, a1=100, a2=100\n",">204, d1=0.008, d2=0.008 g=0.033, a1=100, a2=100\n",">205, d1=0.003, d2=0.003 g=0.040, a1=100, a2=100\n",">206, d1=0.009, d2=0.005 g=0.035, a1=100, a2=100\n",">207, d1=0.006, d2=0.005 g=0.035, a1=100, a2=100\n",">208, d1=0.005, d2=0.025 g=0.082, a1=100, a2=100\n",">209, d1=0.011, d2=0.002 g=0.141, a1=100, a2=100\n",">210, d1=0.018, d2=0.193 g=3.842, a1=100, a2=100\n",">211, d1=2.337, d2=1.263 g=0.569, a1=0, a2=15\n",">212, d1=0.001, d2=0.370 g=1.944, a1=100, a2=100\n",">213, d1=0.003, d2=0.010 g=2.048, a1=100, a2=100\n",">214, d1=0.013, d2=0.390 g=2.660, a1=100, a2=96\n",">215, d1=0.017, d2=0.003 g=2.058, a1=100, a2=100\n",">216, d1=0.060, d2=1.633 g=3.045, a1=100, a2=9\n",">217, d1=0.840, d2=0.000 g=2.390, a1=50, a2=100\n",">218, d1=0.117, d2=0.000 g=1.709, a1=93, a2=100\n",">219, d1=0.021, d2=0.000 g=1.179, a1=100, a2=100\n",">220, d1=0.012, d2=0.000 g=0.980, a1=100, a2=100\n",">221, d1=0.017, d2=0.000 g=0.796, a1=100, a2=100\n",">222, d1=0.007, d2=0.000 g=0.629, a1=100, a2=100\n",">223, d1=0.021, d2=0.000 g=0.490, a1=100, a2=100\n",">224, d1=0.015, d2=0.001 g=0.393, a1=100, a2=100\n",">225, d1=0.010, d2=0.004 g=0.346, a1=100, a2=100\n",">226, d1=0.013, d2=0.012 g=0.281, a1=100, a2=100\n",">227, d1=0.011, d2=0.023 g=0.276, a1=100, a2=100\n",">228, d1=0.008, d2=0.014 g=0.247, a1=100, a2=100\n",">229, d1=0.035, d2=0.108 g=0.289, a1=96, a2=100\n",">230, d1=0.013, d2=0.188 g=0.627, a1=100, a2=100\n",">231, d1=0.018, d2=0.058 g=0.832, a1=100, a2=100\n",">232, d1=0.057, d2=0.018 g=0.773, a1=100, a2=100\n",">233, d1=0.035, d2=0.233 g=1.078, a1=100, a2=100\n",">234, d1=0.043, d2=0.022 g=1.137, a1=100, a2=100\n",">235, d1=0.075, d2=0.639 g=1.852, a1=100, a2=59\n",">236, d1=0.365, d2=0.014 g=1.344, a1=81, a2=100\n",">237, d1=0.133, d2=0.284 g=1.415, a1=96, a2=96\n",">238, d1=0.072, d2=0.153 g=1.840, a1=100, a2=100\n",">239, d1=0.141, d2=0.037 g=1.365, a1=96, a2=100\n",">240, d1=0.096, d2=0.124 g=0.957, a1=100, a2=100\n",">241, d1=0.087, d2=0.262 g=1.156, a1=100, a2=93\n",">242, d1=0.128, d2=0.006 g=0.758, a1=100, a2=100\n",">243, d1=0.211, d2=0.038 g=0.267, a1=100, a2=100\n",">244, d1=0.046, d2=0.319 g=0.572, a1=100, a2=96\n",">245, d1=0.126, d2=0.021 g=0.746, a1=93, a2=100\n",">246, d1=0.101, d2=0.003 g=0.485, a1=100, a2=100\n",">247, d1=0.078, d2=0.006 g=0.259, a1=96, a2=100\n",">248, d1=0.038, d2=0.062 g=0.245, a1=100, a2=100\n",">249, d1=0.019, d2=0.060 g=0.383, a1=100, a2=100\n",">250, d1=0.023, d2=0.002 g=0.414, a1=100, a2=100\n",">251, d1=0.050, d2=0.001 g=0.277, a1=100, a2=100\n",">252, d1=0.032, d2=0.004 g=0.220, a1=100, a2=100\n",">253, d1=0.021, d2=0.011 g=0.173, a1=100, a2=100\n",">254, d1=0.091, d2=0.008 g=0.096, a1=96, a2=100\n",">255, d1=0.006, d2=0.013 g=0.093, a1=100, a2=100\n",">256, d1=0.026, d2=0.021 g=0.093, a1=100, a2=100\n",">257, d1=0.013, d2=0.009 g=0.098, a1=100, a2=100\n",">258, d1=0.008, d2=0.004 g=0.103, a1=100, a2=100\n",">259, d1=0.010, d2=0.011 g=0.103, a1=100, a2=100\n",">260, d1=0.009, d2=0.015 g=0.111, a1=100, a2=100\n",">261, d1=0.013, d2=0.018 g=0.122, a1=100, a2=100\n",">262, d1=0.006, d2=0.018 g=0.147, a1=100, a2=100\n",">263, d1=0.011, d2=0.003 g=0.146, a1=100, a2=100\n",">264, d1=0.019, d2=0.004 g=0.139, a1=100, a2=100\n",">265, d1=0.007, d2=0.001 g=0.117, a1=100, a2=100\n",">266, d1=0.016, d2=0.007 g=0.107, a1=100, a2=100\n",">267, d1=0.011, d2=0.006 g=0.102, a1=100, a2=100\n",">268, d1=0.006, d2=0.042 g=0.148, a1=100, a2=100\n",">269, d1=0.024, d2=0.011 g=0.185, a1=100, a2=100\n",">270, d1=0.006, d2=0.002 g=0.186, a1=100, a2=100\n",">271, d1=0.013, d2=0.015 g=0.186, a1=100, a2=100\n",">272, d1=0.016, d2=0.001 g=0.173, a1=100, a2=100\n",">273, d1=0.008, d2=0.001 g=0.134, a1=100, a2=100\n",">274, d1=0.014, d2=0.010 g=0.119, a1=100, a2=100\n",">275, d1=0.011, d2=0.031 g=0.155, a1=100, a2=100\n",">276, d1=0.038, d2=0.023 g=0.173, a1=100, a2=100\n",">277, d1=0.014, d2=0.003 g=0.167, a1=100, a2=100\n",">278, d1=0.011, d2=0.005 g=0.147, a1=100, a2=100\n",">279, d1=0.042, d2=0.014 g=0.130, a1=100, a2=100\n",">280, d1=0.003, d2=0.014 g=0.144, a1=100, a2=100\n",">281, d1=0.023, d2=0.059 g=0.225, a1=100, a2=100\n",">282, d1=0.014, d2=0.006 g=0.345, a1=100, a2=100\n",">283, d1=0.038, d2=0.018 g=0.245, a1=100, a2=100\n",">284, d1=0.006, d2=0.063 g=0.431, a1=100, a2=100\n",">285, d1=0.012, d2=0.005 g=0.584, a1=100, a2=100\n",">286, d1=0.024, d2=0.024 g=0.589, a1=100, a2=100\n",">287, d1=0.058, d2=0.005 g=0.317, a1=100, a2=100\n",">288, d1=0.007, d2=0.035 g=0.358, a1=100, a2=100\n",">289, d1=0.007, d2=0.075 g=0.753, a1=100, a2=100\n",">290, d1=0.046, d2=0.021 g=0.736, a1=100, a2=100\n",">291, d1=0.052, d2=0.182 g=1.549, a1=100, a2=100\n",">292, d1=0.148, d2=0.002 g=0.701, a1=100, a2=100\n",">293, d1=0.031, d2=0.021 g=0.397, a1=100, a2=100\n",">294, d1=0.033, d2=0.262 g=1.347, a1=100, a2=100\n",">295, d1=0.243, d2=0.001 g=0.429, a1=96, a2=100\n",">296, d1=0.018, d2=0.002 g=0.197, a1=100, a2=100\n",">297, d1=0.015, d2=0.003 g=0.152, a1=100, a2=100\n",">298, d1=0.035, d2=0.060 g=0.156, a1=100, a2=100\n",">299, d1=0.003, d2=0.013 g=0.222, a1=100, a2=100\n",">300, d1=0.014, d2=0.011 g=0.241, a1=100, a2=100\n",">301, d1=0.009, d2=0.011 g=0.243, a1=100, a2=100\n",">302, d1=0.008, d2=0.010 g=0.262, a1=100, a2=100\n",">303, d1=0.014, d2=0.025 g=0.292, a1=100, a2=100\n",">304, d1=0.013, d2=0.013 g=0.312, a1=100, a2=100\n",">305, d1=0.016, d2=0.005 g=0.284, a1=100, a2=100\n",">306, d1=0.011, d2=0.008 g=0.261, a1=100, a2=100\n",">307, d1=0.016, d2=0.012 g=0.229, a1=100, a2=100\n",">308, d1=0.007, d2=0.010 g=0.246, a1=100, a2=100\n",">309, d1=0.012, d2=0.037 g=0.324, a1=100, a2=100\n",">310, d1=0.083, d2=0.033 g=0.176, a1=100, a2=100\n",">311, d1=0.006, d2=0.004 g=0.179, a1=100, a2=100\n",">312, d1=0.011, d2=0.064 g=0.339, a1=100, a2=100\n",">313, d1=0.010, d2=0.005 g=0.512, a1=100, a2=100\n",">314, d1=0.008, d2=0.006 g=0.531, a1=100, a2=100\n",">315, d1=0.024, d2=0.001 g=0.425, a1=100, a2=100\n",">316, d1=0.052, d2=0.001 g=0.148, a1=100, a2=100\n",">317, d1=0.006, d2=0.010 g=0.118, a1=100, a2=100\n",">318, d1=0.011, d2=0.008 g=0.115, a1=100, a2=100\n",">319, d1=0.007, d2=0.013 g=0.119, a1=100, a2=100\n",">320, d1=0.020, d2=0.028 g=0.169, a1=100, a2=100\n",">321, d1=0.010, d2=0.009 g=0.200, a1=100, a2=100\n",">322, d1=0.003, d2=0.026 g=0.296, a1=100, a2=100\n",">323, d1=0.013, d2=0.012 g=0.394, a1=100, a2=100\n",">324, d1=0.010, d2=0.018 g=0.417, a1=100, a2=100\n",">325, d1=0.007, d2=0.009 g=0.515, a1=100, a2=100\n",">326, d1=0.010, d2=0.005 g=0.419, a1=100, a2=100\n",">327, d1=0.007, d2=0.002 g=0.355, a1=100, a2=100\n",">328, d1=0.006, d2=0.012 g=0.374, a1=100, a2=100\n",">329, d1=0.045, d2=0.016 g=0.193, a1=100, a2=100\n",">330, d1=0.007, d2=0.046 g=0.328, a1=100, a2=100\n",">331, d1=0.006, d2=0.002 g=0.462, a1=100, a2=100\n",">332, d1=0.006, d2=0.011 g=0.495, a1=100, a2=100\n",">333, d1=0.003, d2=0.002 g=0.484, a1=100, a2=100\n",">334, d1=0.003, d2=0.006 g=0.469, a1=100, a2=100\n",">335, d1=0.031, d2=0.004 g=0.231, a1=100, a2=100\n",">336, d1=0.005, d2=0.028 g=0.249, a1=100, a2=100\n",">337, d1=0.002, d2=0.021 g=0.434, a1=100, a2=100\n",">338, d1=0.009, d2=0.010 g=0.505, a1=100, a2=100\n",">339, d1=0.009, d2=0.011 g=0.518, a1=100, a2=100\n",">340, d1=0.020, d2=0.033 g=0.652, a1=100, a2=100\n",">341, d1=0.028, d2=0.019 g=0.628, a1=100, a2=100\n",">342, d1=0.006, d2=0.002 g=0.603, a1=100, a2=100\n",">343, d1=0.011, d2=0.112 g=1.968, a1=100, a2=100\n",">344, d1=0.225, d2=0.001 g=0.311, a1=96, a2=100\n",">345, d1=0.008, d2=0.551 g=4.296, a1=100, a2=81\n",">346, d1=0.075, d2=0.000 g=6.679, a1=96, a2=100\n",">347, d1=0.453, d2=0.000 g=0.228, a1=81, a2=100\n",">348, d1=0.003, d2=0.000 g=0.036, a1=100, a2=100\n",">349, d1=0.003, d2=0.000 g=0.019, a1=100, a2=100\n",">350, d1=0.001, d2=0.002 g=0.017, a1=100, a2=100\n",">351, d1=0.001, d2=0.008 g=0.017, a1=100, a2=100\n",">352, d1=0.001, d2=0.036 g=0.025, a1=100, a2=100\n",">353, d1=0.005, d2=0.005 g=0.034, a1=100, a2=100\n",">354, d1=0.002, d2=0.020 g=0.044, a1=100, a2=100\n",">355, d1=0.004, d2=0.004 g=0.053, a1=100, a2=100\n",">356, d1=0.003, d2=0.036 g=0.076, a1=100, a2=100\n",">357, d1=0.011, d2=0.155 g=0.709, a1=100, a2=100\n",">358, d1=0.011, d2=0.030 g=1.977, a1=100, a2=100\n",">359, d1=0.210, d2=0.109 g=0.782, a1=90, a2=100\n",">360, d1=0.010, d2=1.763 g=13.237, a1=100, a2=0\n",">361, d1=5.368, d2=0.000 g=1.631, a1=0, a2=100\n",">362, d1=0.094, d2=0.001 g=0.102, a1=100, a2=100\n",">363, d1=0.008, d2=0.002 g=0.035, a1=100, a2=100\n",">364, d1=0.010, d2=0.000 g=0.025, a1=100, a2=100\n",">365, d1=0.014, d2=0.001 g=0.021, a1=100, a2=100\n",">366, d1=0.005, d2=0.002 g=0.021, a1=100, a2=100\n",">367, d1=0.005, d2=0.002 g=0.017, a1=100, a2=100\n",">368, d1=0.002, d2=0.003 g=0.017, a1=100, a2=100\n",">369, d1=0.004, d2=0.005 g=0.019, a1=100, a2=100\n",">370, d1=0.003, d2=0.006 g=0.022, a1=100, a2=100\n",">371, d1=0.007, d2=0.024 g=0.024, a1=100, a2=100\n",">372, d1=0.009, d2=0.011 g=0.024, a1=100, a2=100\n",">373, d1=0.002, d2=0.003 g=0.027, a1=100, a2=100\n",">374, d1=0.003, d2=0.005 g=0.024, a1=100, a2=100\n",">375, d1=0.001, d2=0.009 g=0.027, a1=100, a2=100\n",">376, d1=0.002, d2=0.011 g=0.029, a1=100, a2=100\n",">377, d1=0.013, d2=0.012 g=0.032, a1=100, a2=100\n",">378, d1=0.002, d2=0.028 g=0.038, a1=100, a2=100\n",">379, d1=0.004, d2=0.023 g=0.056, a1=100, a2=100\n",">380, d1=0.005, d2=0.009 g=0.065, a1=100, a2=100\n",">381, d1=0.004, d2=0.024 g=0.080, a1=100, a2=100\n",">382, d1=0.005, d2=0.032 g=0.116, a1=100, a2=100\n",">383, d1=0.006, d2=0.017 g=0.139, a1=100, a2=100\n",">384, d1=0.009, d2=0.020 g=0.164, a1=100, a2=100\n",">385, d1=0.012, d2=0.045 g=0.242, a1=100, a2=100\n",">386, d1=0.009, d2=0.047 g=0.358, a1=100, a2=100\n",">387, d1=0.030, d2=0.078 g=0.627, a1=100, a2=100\n",">388, d1=0.059, d2=0.026 g=0.577, a1=100, a2=100\n",">389, d1=0.060, d2=0.012 g=0.337, a1=100, a2=100\n",">390, d1=0.007, d2=0.010 g=0.260, a1=100, a2=100\n",">391, d1=0.014, d2=0.008 g=0.223, a1=100, a2=100\n",">392, d1=0.022, d2=0.077 g=0.337, a1=100, a2=100\n",">393, d1=0.005, d2=0.009 g=0.470, a1=100, a2=100\n",">394, d1=0.008, d2=0.022 g=0.527, a1=100, a2=100\n",">395, d1=0.028, d2=0.008 g=0.454, a1=100, a2=100\n",">396, d1=0.012, d2=0.012 g=0.368, a1=100, a2=100\n",">397, d1=0.030, d2=0.003 g=0.191, a1=100, a2=100\n",">398, d1=0.009, d2=0.017 g=0.199, a1=100, a2=100\n",">399, d1=0.005, d2=0.022 g=0.210, a1=100, a2=100\n",">400, d1=0.105, d2=0.018 g=0.094, a1=96, a2=100\n",">401, d1=0.006, d2=0.041 g=0.113, a1=100, a2=100\n",">402, d1=0.005, d2=0.068 g=0.359, a1=100, a2=100\n",">403, d1=0.017, d2=0.028 g=0.635, a1=100, a2=100\n",">404, d1=0.017, d2=0.083 g=1.464, a1=100, a2=100\n",">405, d1=0.027, d2=0.005 g=1.702, a1=100, a2=100\n",">406, d1=0.111, d2=0.045 g=0.835, a1=96, a2=100\n",">407, d1=0.021, d2=0.329 g=4.396, a1=100, a2=100\n",">408, d1=0.254, d2=0.057 g=4.049, a1=93, a2=100\n",">409, d1=0.041, d2=0.009 g=3.333, a1=100, a2=100\n",">410, d1=0.053, d2=0.010 g=2.234, a1=100, a2=100\n",">411, d1=0.008, d2=0.012 g=1.686, a1=100, a2=100\n",">412, d1=0.016, d2=0.196 g=3.785, a1=100, a2=100\n",">413, d1=0.080, d2=0.014 g=4.221, a1=100, a2=100\n",">414, d1=0.324, d2=0.024 g=0.397, a1=90, a2=100\n",">415, d1=0.008, d2=1.009 g=6.185, a1=100, a2=9\n",">416, d1=0.161, d2=0.000 g=8.393, a1=100, a2=100\n",">417, d1=0.993, d2=0.012 g=0.043, a1=31, a2=100\n",">418, d1=0.004, d2=0.126 g=0.012, a1=100, a2=96\n",">419, d1=0.003, d2=0.020 g=0.021, a1=100, a2=100\n",">420, d1=0.004, d2=0.046 g=0.040, a1=100, a2=100\n",">421, d1=0.003, d2=0.032 g=0.073, a1=100, a2=100\n",">422, d1=0.002, d2=0.047 g=0.140, a1=100, a2=100\n",">423, d1=0.004, d2=0.113 g=0.476, a1=100, a2=100\n",">424, d1=0.011, d2=0.223 g=2.765, a1=100, a2=100\n",">425, d1=0.105, d2=0.027 g=3.282, a1=96, a2=100\n",">426, d1=0.115, d2=0.046 g=2.513, a1=96, a2=100\n",">427, d1=0.058, d2=0.002 g=1.479, a1=100, a2=100\n",">428, d1=0.011, d2=0.013 g=1.100, a1=100, a2=100\n",">429, d1=0.041, d2=0.192 g=2.648, a1=100, a2=100\n",">430, d1=0.026, d2=0.001 g=3.709, a1=100, a2=100\n",">431, d1=0.161, d2=0.018 g=1.461, a1=96, a2=100\n",">432, d1=0.008, d2=0.080 g=1.710, a1=100, a2=100\n",">433, d1=0.043, d2=0.066 g=2.572, a1=100, a2=100\n",">434, d1=0.018, d2=0.000 g=2.627, a1=100, a2=100\n",">435, d1=0.036, d2=0.001 g=1.892, a1=100, a2=100\n",">436, d1=0.011, d2=0.033 g=1.881, a1=100, a2=100\n",">437, d1=0.028, d2=0.026 g=1.866, a1=100, a2=100\n",">438, d1=0.072, d2=0.001 g=0.966, a1=100, a2=100\n",">439, d1=0.006, d2=0.002 g=0.732, a1=100, a2=100\n",">440, d1=0.009, d2=0.004 g=0.517, a1=100, a2=100\n",">441, d1=0.013, d2=0.023 g=0.546, a1=100, a2=100\n",">442, d1=0.003, d2=0.081 g=1.291, a1=100, a2=100\n",">443, d1=0.010, d2=0.002 g=1.953, a1=100, a2=100\n",">444, d1=0.079, d2=0.003 g=0.928, a1=100, a2=100\n",">445, d1=0.003, d2=0.009 g=0.750, a1=100, a2=100\n",">446, d1=0.013, d2=0.052 g=0.998, a1=100, a2=100\n",">447, d1=0.011, d2=0.040 g=1.689, a1=100, a2=100\n",">448, d1=0.082, d2=0.061 g=1.356, a1=100, a2=100\n",">449, d1=0.009, d2=0.029 g=1.870, a1=100, a2=100\n",">450, d1=0.003, d2=0.010 g=2.162, a1=100, a2=100\n",">451, d1=0.021, d2=0.058 g=2.620, a1=100, a2=100\n",">452, d1=0.020, d2=0.012 g=2.891, a1=100, a2=100\n",">453, d1=0.027, d2=0.002 g=2.355, a1=100, a2=100\n",">454, d1=0.004, d2=0.006 g=2.168, a1=100, a2=100\n",">455, d1=0.025, d2=0.084 g=2.739, a1=100, a2=100\n",">456, d1=0.321, d2=1.583 g=8.452, a1=90, a2=0\n",">457, d1=0.250, d2=0.000 g=10.246, a1=93, a2=100\n",">458, d1=0.991, d2=0.000 g=0.136, a1=21, a2=100\n",">459, d1=0.004, d2=0.000 g=0.013, a1=100, a2=100\n",">460, d1=0.001, d2=0.000 g=0.007, a1=100, a2=100\n",">461, d1=0.002, d2=0.000 g=0.007, a1=100, a2=100\n",">462, d1=0.000, d2=0.000 g=0.007, a1=100, a2=100\n",">463, d1=0.001, d2=0.000 g=0.008, a1=100, a2=100\n",">464, d1=0.003, d2=0.000 g=0.008, a1=100, a2=100\n",">465, d1=0.001, d2=0.000 g=0.009, a1=100, a2=100\n",">466, d1=0.002, d2=0.000 g=0.008, a1=100, a2=100\n",">467, d1=0.001, d2=0.000 g=0.008, a1=100, a2=100\n",">468, d1=0.001, d2=0.000 g=0.008, a1=100, a2=100\n",">469, d1=0.002, d2=0.000 g=0.009, a1=100, a2=100\n",">470, d1=0.002, d2=0.000 g=0.009, a1=100, a2=100\n",">471, d1=0.001, d2=0.000 g=0.009, a1=100, a2=100\n",">472, d1=0.001, d2=0.000 g=0.010, a1=100, a2=100\n",">473, d1=0.001, d2=0.000 g=0.010, a1=100, a2=100\n",">474, d1=0.000, d2=0.000 g=0.010, a1=100, a2=100\n",">475, d1=0.001, d2=0.001 g=0.010, a1=100, a2=100\n",">476, d1=0.001, d2=0.001 g=0.011, a1=100, a2=100\n",">477, d1=0.003, d2=0.001 g=0.011, a1=100, a2=100\n",">478, d1=0.001, d2=0.000 g=0.011, a1=100, a2=100\n",">479, d1=0.001, d2=0.001 g=0.011, a1=100, a2=100\n",">480, d1=0.001, d2=0.001 g=0.011, a1=100, a2=100\n",">481, d1=0.001, d2=0.003 g=0.012, a1=100, a2=100\n",">482, d1=0.003, d2=0.001 g=0.014, a1=100, a2=100\n",">483, d1=0.001, d2=0.001 g=0.013, a1=100, a2=100\n",">484, d1=0.001, d2=0.006 g=0.013, a1=100, a2=100\n",">485, d1=0.002, d2=0.004 g=0.015, a1=100, a2=100\n",">486, d1=0.001, d2=0.009 g=0.019, a1=100, a2=100\n",">487, d1=0.002, d2=0.004 g=0.019, a1=100, a2=100\n",">488, d1=0.001, d2=0.008 g=0.021, a1=100, a2=100\n",">489, d1=0.001, d2=0.017 g=0.027, a1=100, a2=100\n",">490, d1=0.001, d2=0.013 g=0.033, a1=100, a2=100\n",">491, d1=0.002, d2=0.012 g=0.043, a1=100, a2=100\n",">492, d1=0.003, d2=0.015 g=0.054, a1=100, a2=100\n",">493, d1=0.003, d2=0.025 g=0.072, a1=100, a2=100\n",">494, d1=0.001, d2=0.020 g=0.104, a1=100, a2=100\n",">495, d1=0.001, d2=0.039 g=0.170, a1=100, a2=100\n",">496, d1=0.001, d2=0.010 g=0.235, a1=100, a2=100\n",">497, d1=0.003, d2=0.039 g=0.362, a1=100, a2=100\n",">498, d1=0.002, d2=0.023 g=0.482, a1=100, a2=100\n",">499, d1=0.005, d2=0.010 g=0.552, a1=100, a2=100\n",">500, d1=0.007, d2=0.037 g=0.726, a1=100, a2=100\n",">501, d1=0.003, d2=0.014 g=0.809, a1=100, a2=100\n",">502, d1=0.009, d2=0.010 g=0.772, a1=100, a2=100\n",">503, d1=0.005, d2=0.005 g=0.669, a1=100, a2=100\n",">504, d1=0.009, d2=0.006 g=0.566, a1=100, a2=100\n",">505, d1=0.040, d2=0.125 g=1.257, a1=100, a2=100\n",">506, d1=0.008, d2=0.077 g=3.513, a1=100, a2=100\n",">507, d1=0.065, d2=0.001 g=3.359, a1=100, a2=100\n",">508, d1=0.018, d2=0.003 g=2.811, a1=100, a2=100\n",">509, d1=0.034, d2=0.018 g=2.115, a1=100, a2=100\n",">510, d1=0.003, d2=0.063 g=3.077, a1=100, a2=100\n",">511, d1=0.131, d2=0.011 g=1.499, a1=96, a2=100\n",">512, d1=0.011, d2=0.192 g=4.090, a1=100, a2=100\n",">513, d1=0.014, d2=0.024 g=6.361, a1=100, a2=100\n",">514, d1=0.434, d2=0.000 g=0.172, a1=87, a2=100\n",">515, d1=0.002, d2=0.001 g=0.029, a1=100, a2=100\n",">516, d1=0.001, d2=0.003 g=0.020, a1=100, a2=100\n",">517, d1=0.002, d2=0.003 g=0.019, a1=100, a2=100\n",">518, d1=0.001, d2=0.016 g=0.021, a1=100, a2=100\n",">519, d1=0.001, d2=0.013 g=0.033, a1=100, a2=100\n",">520, d1=0.001, d2=0.016 g=0.054, a1=100, a2=100\n",">521, d1=0.001, d2=0.012 g=0.072, a1=100, a2=100\n",">522, d1=0.001, d2=0.066 g=0.281, a1=100, a2=100\n",">523, d1=0.011, d2=0.072 g=1.381, a1=100, a2=100\n",">524, d1=0.008, d2=0.307 g=7.648, a1=100, a2=100\n",">525, d1=0.178, d2=0.000 g=7.468, a1=96, a2=100\n",">526, d1=0.067, d2=0.000 g=5.616, a1=100, a2=100\n",">527, d1=0.027, d2=0.001 g=4.213, a1=100, a2=100\n",">528, d1=0.004, d2=0.003 g=3.410, a1=100, a2=100\n",">529, d1=0.011, d2=0.024 g=3.157, a1=100, a2=100\n",">530, d1=0.007, d2=0.329 g=8.419, a1=100, a2=96\n",">531, d1=0.458, d2=0.004 g=2.599, a1=81, a2=100\n",">532, d1=0.010, d2=0.143 g=2.650, a1=100, a2=100\n",">533, d1=0.003, d2=0.413 g=9.612, a1=100, a2=93\n",">534, d1=0.712, d2=0.649 g=7.682, a1=50, a2=75\n",">535, d1=0.112, d2=0.393 g=12.966, a1=96, a2=87\n",">536, d1=1.957, d2=1.449 g=4.206, a1=0, a2=15\n",">537, d1=0.059, d2=1.017 g=9.009, a1=100, a2=59\n",">538, d1=2.351, d2=0.341 g=1.953, a1=0, a2=96\n",">539, d1=0.112, d2=1.061 g=5.220, a1=100, a2=12\n",">540, d1=0.616, d2=0.024 g=3.211, a1=68, a2=100\n",">541, d1=0.173, d2=0.426 g=3.463, a1=96, a2=87\n",">542, d1=0.346, d2=0.026 g=1.621, a1=90, a2=100\n",">543, d1=0.024, d2=0.043 g=1.309, a1=100, a2=100\n",">544, d1=0.028, d2=0.001 g=0.973, a1=100, a2=100\n",">545, d1=0.021, d2=0.003 g=0.648, a1=100, a2=100\n",">546, d1=0.036, d2=0.002 g=0.393, a1=100, a2=100\n",">547, d1=0.041, d2=0.000 g=0.212, a1=100, a2=100\n",">548, d1=0.006, d2=0.000 g=0.159, a1=100, a2=100\n",">549, d1=0.006, d2=0.000 g=0.139, a1=100, a2=100\n",">550, d1=0.030, d2=0.000 g=0.099, a1=100, a2=100\n",">551, d1=0.006, d2=0.000 g=0.071, a1=100, a2=100\n",">552, d1=0.008, d2=0.000 g=0.065, a1=100, a2=100\n",">553, d1=0.007, d2=0.000 g=0.060, a1=100, a2=100\n",">554, d1=0.002, d2=0.000 g=0.055, a1=100, a2=100\n",">555, d1=0.010, d2=0.000 g=0.048, a1=100, a2=100\n",">556, d1=0.003, d2=0.000 g=0.049, a1=100, a2=100\n",">557, d1=0.006, d2=0.000 g=0.042, a1=100, a2=100\n",">558, d1=0.005, d2=0.000 g=0.040, a1=100, a2=100\n",">559, d1=0.003, d2=0.000 g=0.037, a1=100, a2=100\n",">560, d1=0.014, d2=0.002 g=0.035, a1=100, a2=100\n",">561, d1=0.001, d2=0.000 g=0.040, a1=100, a2=100\n",">562, d1=0.002, d2=0.000 g=0.034, a1=100, a2=100\n",">563, d1=0.004, d2=0.001 g=0.031, a1=100, a2=100\n",">564, d1=0.010, d2=0.000 g=0.029, a1=100, a2=100\n",">565, d1=0.005, d2=0.000 g=0.029, a1=100, a2=100\n",">566, d1=0.005, d2=0.001 g=0.025, a1=100, a2=100\n",">567, d1=0.004, d2=0.001 g=0.027, a1=100, a2=100\n",">568, d1=0.005, d2=0.001 g=0.027, a1=100, a2=100\n",">569, d1=0.001, d2=0.000 g=0.023, a1=100, a2=100\n",">570, d1=0.005, d2=0.000 g=0.025, a1=100, a2=100\n",">571, d1=0.010, d2=0.001 g=0.021, a1=100, a2=100\n",">572, d1=0.003, d2=0.009 g=0.023, a1=100, a2=100\n",">573, d1=0.006, d2=0.004 g=0.022, a1=100, a2=100\n",">574, d1=0.008, d2=0.001 g=0.024, a1=100, a2=100\n",">575, d1=0.004, d2=0.002 g=0.023, a1=100, a2=100\n",">576, d1=0.003, d2=0.002 g=0.024, a1=100, a2=100\n",">577, d1=0.003, d2=0.009 g=0.024, a1=100, a2=100\n",">578, d1=0.003, d2=0.000 g=0.025, a1=100, a2=100\n",">579, d1=0.004, d2=0.005 g=0.027, a1=100, a2=100\n",">580, d1=0.002, d2=0.000 g=0.034, a1=100, a2=100\n",">581, d1=0.005, d2=0.001 g=0.031, a1=100, a2=100\n",">582, d1=0.001, d2=0.005 g=0.028, a1=100, a2=100\n",">583, d1=0.007, d2=0.002 g=0.028, a1=100, a2=100\n",">584, d1=0.001, d2=0.001 g=0.026, a1=100, a2=100\n",">585, d1=0.003, d2=0.002 g=0.028, a1=100, a2=100\n",">586, d1=0.001, d2=0.003 g=0.025, a1=100, a2=100\n",">587, d1=0.003, d2=0.003 g=0.029, a1=100, a2=100\n",">588, d1=0.002, d2=0.002 g=0.031, a1=100, a2=100\n",">589, d1=0.004, d2=0.004 g=0.031, a1=100, a2=100\n",">590, d1=0.004, d2=0.004 g=0.032, a1=100, a2=100\n",">591, d1=0.003, d2=0.002 g=0.032, a1=100, a2=100\n",">592, d1=0.003, d2=0.006 g=0.030, a1=100, a2=100\n",">593, d1=0.002, d2=0.002 g=0.035, a1=100, a2=100\n",">594, d1=0.003, d2=0.002 g=0.032, a1=100, a2=100\n",">595, d1=0.001, d2=0.009 g=0.037, a1=100, a2=100\n",">596, d1=0.001, d2=0.003 g=0.041, a1=100, a2=100\n",">597, d1=0.007, d2=0.004 g=0.037, a1=100, a2=100\n",">598, d1=0.003, d2=0.003 g=0.038, a1=100, a2=100\n",">599, d1=0.017, d2=0.005 g=0.034, a1=100, a2=100\n",">600, d1=0.002, d2=0.005 g=0.033, a1=100, a2=100\n",">601, d1=0.003, d2=0.002 g=0.032, a1=100, a2=100\n",">602, d1=0.002, d2=0.004 g=0.032, a1=100, a2=100\n",">603, d1=0.003, d2=0.001 g=0.033, a1=100, a2=100\n",">604, d1=0.003, d2=0.003 g=0.033, a1=100, a2=100\n",">605, d1=0.004, d2=0.003 g=0.033, a1=100, a2=100\n",">606, d1=0.002, d2=0.005 g=0.033, a1=100, a2=100\n",">607, d1=0.002, d2=0.015 g=0.037, a1=100, a2=100\n",">608, d1=0.004, d2=0.003 g=0.041, a1=100, a2=100\n",">609, d1=0.002, d2=0.003 g=0.038, a1=100, a2=100\n",">610, d1=0.001, d2=0.011 g=0.044, a1=100, a2=100\n",">611, d1=0.005, d2=0.006 g=0.053, a1=100, a2=100\n",">612, d1=0.003, d2=0.008 g=0.054, a1=100, a2=100\n",">613, d1=0.007, d2=0.007 g=0.059, a1=100, a2=100\n",">614, d1=0.005, d2=0.020 g=0.071, a1=100, a2=100\n",">615, d1=0.004, d2=0.004 g=0.085, a1=100, a2=100\n",">616, d1=0.004, d2=0.016 g=0.104, a1=100, a2=100\n",">617, d1=0.004, d2=0.030 g=0.160, a1=100, a2=100\n",">618, d1=0.004, d2=0.020 g=0.259, a1=100, a2=100\n",">619, d1=0.002, d2=0.004 g=0.290, a1=100, a2=100\n",">620, d1=0.007, d2=0.007 g=0.280, a1=100, a2=100\n",">621, d1=0.011, d2=0.000 g=0.166, a1=100, a2=100\n",">622, d1=0.016, d2=0.001 g=0.092, a1=100, a2=100\n",">623, d1=0.007, d2=0.002 g=0.057, a1=100, a2=100\n",">624, d1=0.022, d2=0.002 g=0.046, a1=100, a2=100\n",">625, d1=0.018, d2=0.002 g=0.037, a1=100, a2=100\n",">626, d1=0.005, d2=0.003 g=0.032, a1=100, a2=100\n",">627, d1=0.003, d2=0.003 g=0.034, a1=100, a2=100\n",">628, d1=0.002, d2=0.008 g=0.030, a1=100, a2=100\n",">629, d1=0.001, d2=0.005 g=0.035, a1=100, a2=100\n",">630, d1=0.002, d2=0.015 g=0.044, a1=100, a2=100\n",">631, d1=0.010, d2=0.029 g=0.070, a1=100, a2=100\n",">632, d1=0.001, d2=0.011 g=0.114, a1=100, a2=100\n",">633, d1=0.005, d2=0.005 g=0.131, a1=100, a2=100\n",">634, d1=0.007, d2=0.003 g=0.115, a1=100, a2=100\n",">635, d1=0.006, d2=0.003 g=0.121, a1=100, a2=100\n",">636, d1=0.008, d2=0.002 g=0.100, a1=100, a2=100\n",">637, d1=0.003, d2=0.001 g=0.100, a1=100, a2=100\n",">638, d1=0.007, d2=0.001 g=0.079, a1=100, a2=100\n",">639, d1=0.003, d2=0.001 g=0.065, a1=100, a2=100\n",">640, d1=0.016, d2=0.002 g=0.058, a1=100, a2=100\n",">641, d1=0.003, d2=0.001 g=0.052, a1=100, a2=100\n",">642, d1=0.004, d2=0.003 g=0.048, a1=100, a2=100\n",">643, d1=0.005, d2=0.002 g=0.046, a1=100, a2=100\n",">644, d1=0.002, d2=0.002 g=0.054, a1=100, a2=100\n",">645, d1=0.001, d2=0.003 g=0.050, a1=100, a2=100\n",">646, d1=0.005, d2=0.008 g=0.053, a1=100, a2=100\n",">647, d1=0.001, d2=0.007 g=0.073, a1=100, a2=100\n",">648, d1=0.002, d2=0.011 g=0.082, a1=100, a2=100\n",">649, d1=0.002, d2=0.005 g=0.108, a1=100, a2=100\n",">650, d1=0.003, d2=0.005 g=0.109, a1=100, a2=100\n",">651, d1=0.005, d2=0.004 g=0.121, a1=100, a2=100\n",">652, d1=0.013, d2=0.011 g=0.115, a1=100, a2=100\n",">653, d1=0.007, d2=0.008 g=0.125, a1=100, a2=100\n",">654, d1=0.001, d2=0.005 g=0.151, a1=100, a2=100\n",">655, d1=0.007, d2=0.007 g=0.154, a1=100, a2=100\n",">656, d1=0.007, d2=0.006 g=0.187, a1=100, a2=100\n",">657, d1=0.005, d2=0.010 g=0.215, a1=100, a2=100\n",">658, d1=0.004, d2=0.013 g=0.276, a1=100, a2=100\n",">659, d1=0.014, d2=0.010 g=0.292, a1=100, a2=100\n",">660, d1=0.004, d2=0.022 g=0.416, a1=100, a2=100\n",">661, d1=0.003, d2=0.007 g=0.514, a1=100, a2=100\n",">662, d1=0.020, d2=0.033 g=0.665, a1=100, a2=100\n",">663, d1=0.023, d2=0.014 g=0.749, a1=100, a2=100\n",">664, d1=0.006, d2=0.019 g=0.851, a1=100, a2=100\n",">665, d1=0.006, d2=0.028 g=1.175, a1=100, a2=100\n",">666, d1=0.004, d2=0.005 g=0.976, a1=100, a2=100\n",">667, d1=0.007, d2=0.002 g=0.423, a1=100, a2=100\n",">668, d1=0.024, d2=0.000 g=0.135, a1=100, a2=100\n",">669, d1=0.005, d2=0.000 g=0.104, a1=100, a2=100\n",">670, d1=0.008, d2=0.000 g=0.071, a1=100, a2=100\n",">671, d1=0.014, d2=0.001 g=0.050, a1=100, a2=100\n",">672, d1=0.004, d2=0.001 g=0.036, a1=100, a2=100\n",">673, d1=0.009, d2=0.002 g=0.030, a1=100, a2=100\n",">674, d1=0.005, d2=0.001 g=0.027, a1=100, a2=100\n",">675, d1=0.044, d2=0.002 g=0.014, a1=100, a2=100\n",">676, d1=0.001, d2=0.006 g=0.014, a1=100, a2=100\n",">677, d1=0.004, d2=0.002 g=0.019, a1=100, a2=100\n",">678, d1=0.030, d2=0.003 g=0.012, a1=100, a2=100\n",">679, d1=0.004, d2=0.021 g=0.016, a1=100, a2=100\n",">680, d1=0.001, d2=0.023 g=0.024, a1=100, a2=100\n",">681, d1=0.004, d2=0.003 g=0.037, a1=100, a2=100\n",">682, d1=0.007, d2=0.002 g=0.037, a1=100, a2=100\n",">683, d1=0.004, d2=0.009 g=0.047, a1=100, a2=100\n",">684, d1=0.004, d2=0.004 g=0.052, a1=100, a2=100\n",">685, d1=0.001, d2=0.006 g=0.053, a1=100, a2=100\n",">686, d1=0.003, d2=0.006 g=0.059, a1=100, a2=100\n",">687, d1=0.008, d2=0.003 g=0.069, a1=100, a2=100\n",">688, d1=0.001, d2=0.002 g=0.075, a1=100, a2=100\n",">689, d1=0.003, d2=0.008 g=0.081, a1=100, a2=100\n",">690, d1=0.001, d2=0.004 g=0.102, a1=100, a2=100\n",">691, d1=0.002, d2=0.014 g=0.112, a1=100, a2=100\n",">692, d1=0.002, d2=0.018 g=0.165, a1=100, a2=100\n",">693, d1=0.029, d2=0.050 g=0.300, a1=100, a2=100\n",">694, d1=0.008, d2=0.007 g=0.483, a1=100, a2=100\n",">695, d1=0.002, d2=0.006 g=0.552, a1=100, a2=100\n",">696, d1=0.003, d2=0.005 g=0.431, a1=100, a2=100\n",">697, d1=0.003, d2=0.003 g=0.464, a1=100, a2=100\n",">698, d1=0.001, d2=0.004 g=0.461, a1=100, a2=100\n",">699, d1=0.012, d2=0.004 g=0.379, a1=100, a2=100\n",">700, d1=0.008, d2=0.012 g=0.373, a1=100, a2=100\n",">701, d1=0.003, d2=0.008 g=0.423, a1=100, a2=100\n",">702, d1=0.012, d2=0.011 g=0.454, a1=100, a2=100\n",">703, d1=0.002, d2=0.005 g=0.461, a1=100, a2=100\n",">704, d1=0.008, d2=0.006 g=0.464, a1=100, a2=100\n",">705, d1=0.006, d2=0.015 g=0.549, a1=100, a2=100\n",">706, d1=0.001, d2=0.019 g=0.689, a1=100, a2=100\n",">707, d1=0.055, d2=0.007 g=0.401, a1=100, a2=100\n",">708, d1=0.004, d2=0.019 g=0.522, a1=100, a2=100\n",">709, d1=0.002, d2=0.030 g=0.733, a1=100, a2=100\n",">710, d1=0.002, d2=0.054 g=1.618, a1=100, a2=100\n",">711, d1=0.003, d2=0.000 g=2.542, a1=100, a2=100\n",">712, d1=0.005, d2=0.075 g=4.331, a1=100, a2=100\n",">713, d1=0.014, d2=0.001 g=4.006, a1=100, a2=100\n",">714, d1=0.090, d2=0.000 g=2.208, a1=100, a2=100\n",">715, d1=0.011, d2=0.014 g=1.324, a1=100, a2=100\n",">716, d1=0.014, d2=0.026 g=1.011, a1=100, a2=100\n",">717, d1=0.001, d2=0.033 g=1.216, a1=100, a2=100\n",">718, d1=0.002, d2=0.117 g=2.608, a1=100, a2=100\n",">719, d1=0.082, d2=0.003 g=2.038, a1=100, a2=100\n",">720, d1=0.008, d2=0.001 g=1.777, a1=100, a2=100\n",">721, d1=0.008, d2=0.001 g=1.291, a1=100, a2=100\n",">722, d1=0.023, d2=0.015 g=1.004, a1=100, a2=100\n",">723, d1=0.004, d2=0.004 g=0.915, a1=100, a2=100\n",">724, d1=0.029, d2=0.006 g=0.647, a1=100, a2=100\n",">725, d1=0.008, d2=0.090 g=1.278, a1=100, a2=100\n",">726, d1=0.052, d2=0.007 g=1.536, a1=96, a2=100\n",">727, d1=0.010, d2=0.009 g=1.406, a1=100, a2=100\n",">728, d1=0.005, d2=0.008 g=1.383, a1=100, a2=100\n",">729, d1=0.003, d2=0.005 g=1.313, a1=100, a2=100\n",">730, d1=0.006, d2=0.004 g=1.155, a1=100, a2=100\n",">731, d1=0.005, d2=0.043 g=1.671, a1=100, a2=100\n",">732, d1=0.010, d2=0.004 g=2.067, a1=100, a2=100\n",">733, d1=0.030, d2=0.012 g=1.742, a1=100, a2=100\n",">734, d1=0.009, d2=0.003 g=1.573, a1=100, a2=100\n",">735, d1=0.001, d2=0.012 g=1.647, a1=100, a2=100\n",">736, d1=0.009, d2=0.002 g=1.616, a1=100, a2=100\n",">737, d1=0.021, d2=0.007 g=1.245, a1=100, a2=100\n",">738, d1=0.004, d2=0.022 g=1.434, a1=100, a2=100\n",">739, d1=0.015, d2=0.007 g=1.480, a1=100, a2=100\n",">740, d1=0.007, d2=0.023 g=1.792, a1=100, a2=100\n",">741, d1=0.004, d2=0.007 g=2.063, a1=100, a2=100\n",">742, d1=0.018, d2=0.019 g=2.148, a1=100, a2=100\n",">743, d1=0.015, d2=0.019 g=2.449, a1=100, a2=100\n",">744, d1=0.003, d2=0.002 g=2.594, a1=100, a2=100\n",">745, d1=0.005, d2=0.003 g=2.544, a1=100, a2=100\n",">746, d1=0.024, d2=0.002 g=1.908, a1=100, a2=100\n",">747, d1=0.014, d2=0.054 g=2.542, a1=100, a2=100\n",">748, d1=0.019, d2=0.002 g=2.816, a1=100, a2=100\n",">749, d1=0.008, d2=0.004 g=2.663, a1=100, a2=100\n",">750, d1=0.009, d2=0.002 g=2.210, a1=100, a2=100\n",">751, d1=0.008, d2=0.004 g=1.811, a1=100, a2=100\n",">752, d1=0.004, d2=0.003 g=1.384, a1=100, a2=100\n",">753, d1=0.008, d2=0.024 g=1.425, a1=100, a2=100\n",">754, d1=0.013, d2=0.009 g=1.446, a1=100, a2=100\n",">755, d1=0.030, d2=0.006 g=1.068, a1=100, a2=100\n",">756, d1=0.007, d2=0.013 g=1.007, a1=100, a2=100\n",">757, d1=0.003, d2=0.007 g=1.036, a1=100, a2=100\n",">758, d1=0.008, d2=0.005 g=0.868, a1=100, a2=100\n",">759, d1=0.005, d2=0.008 g=0.806, a1=100, a2=100\n",">760, d1=0.004, d2=0.004 g=0.790, a1=100, a2=100\n",">761, d1=0.012, d2=0.002 g=0.581, a1=100, a2=100\n",">762, d1=0.004, d2=0.007 g=0.560, a1=100, a2=100\n",">763, d1=0.014, d2=0.004 g=0.474, a1=100, a2=100\n",">764, d1=0.003, d2=0.012 g=0.496, a1=100, a2=100\n",">765, d1=0.005, d2=0.020 g=0.805, a1=100, a2=100\n",">766, d1=0.001, d2=0.007 g=0.994, a1=100, a2=100\n",">767, d1=0.004, d2=0.009 g=1.202, a1=100, a2=100\n",">768, d1=0.007, d2=0.006 g=1.275, a1=100, a2=100\n",">769, d1=0.011, d2=0.008 g=1.314, a1=100, a2=100\n",">770, d1=0.005, d2=0.017 g=1.599, a1=100, a2=100\n",">771, d1=0.005, d2=0.031 g=2.053, a1=100, a2=100\n",">772, d1=0.004, d2=0.010 g=2.528, a1=100, a2=100\n",">773, d1=0.014, d2=0.011 g=2.509, a1=100, a2=100\n",">774, d1=0.011, d2=0.004 g=2.179, a1=100, a2=100\n",">775, d1=0.013, d2=0.026 g=2.223, a1=100, a2=100\n",">776, d1=0.003, d2=0.010 g=2.362, a1=100, a2=100\n",">777, d1=0.047, d2=0.116 g=3.571, a1=100, a2=100\n",">778, d1=0.048, d2=0.003 g=3.679, a1=100, a2=100\n",">779, d1=0.017, d2=0.027 g=3.365, a1=100, a2=100\n",">780, d1=0.007, d2=0.003 g=3.284, a1=100, a2=100\n",">781, d1=0.022, d2=0.118 g=4.882, a1=100, a2=100\n",">782, d1=0.031, d2=0.000 g=5.303, a1=100, a2=100\n",">783, d1=0.020, d2=0.000 g=4.571, a1=100, a2=100\n",">784, d1=0.023, d2=0.000 g=3.575, a1=100, a2=100\n",">785, d1=0.030, d2=0.000 g=2.514, a1=100, a2=100\n",">786, d1=0.011, d2=0.000 g=1.911, a1=100, a2=100\n",">787, d1=0.012, d2=0.006 g=1.560, a1=100, a2=100\n",">788, d1=0.001, d2=0.001 g=1.479, a1=100, a2=100\n",">789, d1=0.005, d2=0.003 g=1.343, a1=100, a2=100\n",">790, d1=0.002, d2=0.007 g=1.263, a1=100, a2=100\n",">791, d1=0.007, d2=0.001 g=1.214, a1=100, a2=100\n",">792, d1=0.002, d2=0.012 g=1.223, a1=100, a2=100\n",">793, d1=0.005, d2=0.054 g=2.076, a1=100, a2=100\n",">794, d1=0.013, d2=0.004 g=2.597, a1=100, a2=100\n",">795, d1=0.004, d2=0.004 g=2.839, a1=100, a2=100\n",">796, d1=0.005, d2=0.000 g=2.787, a1=100, a2=100\n",">797, d1=0.010, d2=0.001 g=2.333, a1=100, a2=100\n",">798, d1=0.004, d2=0.027 g=2.690, a1=100, a2=100\n",">799, d1=0.021, d2=0.000 g=2.395, a1=100, a2=100\n",">800, d1=0.008, d2=0.002 g=1.926, a1=100, a2=100\n",">801, d1=0.017, d2=0.026 g=1.835, a1=100, a2=100\n",">802, d1=0.013, d2=0.001 g=1.749, a1=100, a2=100\n",">803, d1=0.002, d2=0.001 g=1.536, a1=100, a2=100\n",">804, d1=0.003, d2=0.024 g=1.849, a1=100, a2=100\n",">805, d1=0.007, d2=0.001 g=1.931, a1=100, a2=100\n",">806, d1=0.002, d2=0.019 g=2.255, a1=100, a2=100\n",">807, d1=0.004, d2=0.000 g=2.781, a1=100, a2=100\n",">808, d1=0.007, d2=0.034 g=3.510, a1=100, a2=100\n",">809, d1=0.020, d2=0.000 g=3.180, a1=100, a2=100\n",">810, d1=0.007, d2=0.000 g=3.231, a1=100, a2=100\n",">811, d1=0.017, d2=0.001 g=2.177, a1=100, a2=100\n",">812, d1=0.020, d2=0.006 g=1.469, a1=100, a2=100\n",">813, d1=0.006, d2=0.005 g=1.189, a1=100, a2=100\n",">814, d1=0.001, d2=0.010 g=1.217, a1=100, a2=100\n",">815, d1=0.003, d2=0.029 g=1.727, a1=100, a2=100\n",">816, d1=0.023, d2=0.001 g=1.432, a1=100, a2=100\n",">817, d1=0.000, d2=0.009 g=1.402, a1=100, a2=100\n",">818, d1=0.004, d2=0.001 g=1.447, a1=100, a2=100\n",">819, d1=0.001, d2=0.066 g=3.333, a1=100, a2=100\n",">820, d1=0.034, d2=0.000 g=3.334, a1=100, a2=100\n",">821, d1=0.089, d2=0.000 g=0.445, a1=100, a2=100\n",">822, d1=0.002, d2=0.033 g=0.375, a1=100, a2=100\n",">823, d1=0.001, d2=0.077 g=1.823, a1=100, a2=100\n",">824, d1=0.005, d2=0.000 g=3.458, a1=100, a2=100\n",">825, d1=0.010, d2=0.003 g=3.545, a1=100, a2=100\n",">826, d1=0.006, d2=0.001 g=3.118, a1=100, a2=100\n",">827, d1=0.009, d2=0.001 g=2.638, a1=100, a2=100\n",">828, d1=0.028, d2=0.004 g=1.558, a1=100, a2=100\n",">829, d1=0.003, d2=0.000 g=1.305, a1=100, a2=100\n",">830, d1=0.002, d2=0.026 g=1.753, a1=100, a2=100\n",">831, d1=0.002, d2=0.000 g=2.128, a1=100, a2=100\n",">832, d1=0.031, d2=0.018 g=1.425, a1=100, a2=100\n",">833, d1=0.003, d2=0.001 g=1.314, a1=100, a2=100\n",">834, d1=0.003, d2=0.002 g=1.147, a1=100, a2=100\n",">835, d1=0.011, d2=0.009 g=1.025, a1=100, a2=100\n",">836, d1=0.056, d2=0.004 g=0.292, a1=100, a2=100\n",">837, d1=0.001, d2=0.001 g=0.207, a1=100, a2=100\n",">838, d1=0.001, d2=0.017 g=0.275, a1=100, a2=100\n",">839, d1=0.004, d2=0.008 g=0.401, a1=100, a2=100\n",">840, d1=0.003, d2=0.008 g=0.497, a1=100, a2=100\n",">841, d1=0.001, d2=0.012 g=0.759, a1=100, a2=100\n",">842, d1=0.003, d2=0.010 g=1.077, a1=100, a2=100\n",">843, d1=0.002, d2=0.002 g=1.263, a1=100, a2=100\n",">844, d1=0.007, d2=0.005 g=1.331, a1=100, a2=100\n",">845, d1=0.007, d2=0.000 g=1.213, a1=100, a2=100\n",">846, d1=0.001, d2=0.210 g=7.383, a1=100, a2=100\n",">847, d1=0.240, d2=0.000 g=0.789, a1=96, a2=100\n",">848, d1=0.000, d2=0.000 g=0.176, a1=100, a2=100\n",">849, d1=0.001, d2=0.000 g=0.112, a1=100, a2=100\n",">850, d1=0.002, d2=0.000 g=0.081, a1=100, a2=100\n",">851, d1=0.010, d2=0.001 g=0.061, a1=100, a2=100\n",">852, d1=0.001, d2=0.003 g=0.054, a1=100, a2=100\n",">853, d1=0.003, d2=0.006 g=0.062, a1=100, a2=100\n",">854, d1=0.001, d2=0.020 g=0.107, a1=100, a2=100\n",">855, d1=0.003, d2=0.032 g=0.400, a1=100, a2=100\n",">856, d1=0.000, d2=0.005 g=0.762, a1=100, a2=100\n",">857, d1=0.057, d2=0.009 g=0.378, a1=96, a2=100\n",">858, d1=0.004, d2=0.019 g=0.521, a1=100, a2=100\n",">859, d1=0.001, d2=0.027 g=1.205, a1=100, a2=100\n",">860, d1=0.002, d2=0.005 g=1.848, a1=100, a2=100\n",">861, d1=0.016, d2=0.001 g=1.684, a1=100, a2=100\n",">862, d1=0.002, d2=0.000 g=1.779, a1=100, a2=100\n",">863, d1=0.006, d2=0.001 g=1.412, a1=100, a2=100\n",">864, d1=0.001, d2=0.001 g=1.164, a1=100, a2=100\n",">865, d1=0.002, d2=0.004 g=1.100, a1=100, a2=100\n",">866, d1=0.004, d2=0.001 g=1.002, a1=100, a2=100\n",">867, d1=0.005, d2=0.097 g=3.841, a1=100, a2=100\n",">868, d1=0.008, d2=0.000 g=6.367, a1=100, a2=100\n",">869, d1=0.061, d2=0.000 g=4.401, a1=100, a2=100\n",">870, d1=0.035, d2=0.000 g=1.772, a1=100, a2=100\n",">871, d1=0.013, d2=0.004 g=0.894, a1=100, a2=100\n",">872, d1=0.002, d2=0.004 g=0.796, a1=100, a2=100\n",">873, d1=0.001, d2=0.005 g=0.799, a1=100, a2=100\n",">874, d1=0.001, d2=0.001 g=0.841, a1=100, a2=100\n",">875, d1=0.001, d2=0.005 g=0.883, a1=100, a2=100\n",">876, d1=0.002, d2=0.004 g=1.010, a1=100, a2=100\n",">877, d1=0.004, d2=0.003 g=1.067, a1=100, a2=100\n",">878, d1=0.003, d2=0.001 g=1.009, a1=100, a2=100\n",">879, d1=0.014, d2=0.005 g=0.661, a1=100, a2=100\n",">880, d1=0.001, d2=0.011 g=0.864, a1=100, a2=100\n",">881, d1=0.006, d2=0.002 g=0.956, a1=100, a2=100\n",">882, d1=0.012, d2=0.005 g=0.733, a1=100, a2=100\n",">883, d1=0.002, d2=0.003 g=0.756, a1=100, a2=100\n",">884, d1=0.005, d2=0.003 g=0.715, a1=100, a2=100\n",">885, d1=0.008, d2=0.018 g=0.981, a1=100, a2=100\n",">886, d1=0.003, d2=0.003 g=1.311, a1=100, a2=100\n",">887, d1=0.002, d2=0.158 g=8.198, a1=100, a2=100\n",">888, d1=0.116, d2=0.000 g=6.026, a1=100, a2=100\n",">889, d1=0.018, d2=0.000 g=3.791, a1=100, a2=100\n",">890, d1=0.002, d2=0.005 g=3.416, a1=100, a2=100\n",">891, d1=0.058, d2=0.000 g=0.769, a1=100, a2=100\n",">892, d1=0.003, d2=0.000 g=0.336, a1=100, a2=100\n",">893, d1=0.001, d2=0.000 g=0.240, a1=100, a2=100\n",">894, d1=0.001, d2=0.000 g=0.205, a1=100, a2=100\n",">895, d1=0.001, d2=0.001 g=0.184, a1=100, a2=100\n",">896, d1=0.001, d2=0.002 g=0.169, a1=100, a2=100\n",">897, d1=0.001, d2=0.003 g=0.191, a1=100, a2=100\n",">898, d1=0.000, d2=0.009 g=0.266, a1=100, a2=100\n",">899, d1=0.003, d2=0.001 g=0.313, a1=100, a2=100\n",">900, d1=0.000, d2=0.000 g=0.314, a1=100, a2=100\n",">901, d1=0.013, d2=0.001 g=0.188, a1=100, a2=100\n",">902, d1=0.001, d2=0.005 g=0.194, a1=100, a2=100\n",">903, d1=0.001, d2=0.006 g=0.280, a1=100, a2=100\n",">904, d1=0.001, d2=0.002 g=0.333, a1=100, a2=100\n",">905, d1=0.004, d2=0.002 g=0.326, a1=100, a2=100\n",">906, d1=0.000, d2=0.003 g=0.363, a1=100, a2=100\n",">907, d1=0.002, d2=0.003 g=0.402, a1=100, a2=100\n",">908, d1=0.002, d2=0.006 g=0.490, a1=100, a2=100\n",">909, d1=0.001, d2=0.002 g=0.571, a1=100, a2=100\n",">910, d1=0.001, d2=0.002 g=0.632, a1=100, a2=100\n",">911, d1=0.002, d2=0.001 g=0.611, a1=100, a2=100\n",">912, d1=0.004, d2=0.005 g=0.655, a1=100, a2=100\n",">913, d1=0.002, d2=0.000 g=0.719, a1=100, a2=100\n",">914, d1=0.001, d2=0.022 g=1.544, a1=100, a2=100\n",">915, d1=0.002, d2=0.000 g=2.588, a1=100, a2=100\n",">916, d1=0.005, d2=0.022 g=3.819, a1=100, a2=100\n",">917, d1=0.049, d2=0.000 g=0.974, a1=100, a2=100\n",">918, d1=0.004, d2=0.000 g=0.478, a1=100, a2=100\n",">919, d1=0.001, d2=0.000 g=0.303, a1=100, a2=100\n",">920, d1=0.000, d2=0.000 g=0.285, a1=100, a2=100\n",">921, d1=0.001, d2=0.000 g=0.247, a1=100, a2=100\n",">922, d1=0.001, d2=0.000 g=0.226, a1=100, a2=100\n",">923, d1=0.003, d2=0.001 g=0.199, a1=100, a2=100\n",">924, d1=0.001, d2=0.001 g=0.177, a1=100, a2=100\n",">925, d1=0.005, d2=0.000 g=0.139, a1=100, a2=100\n",">926, d1=0.003, d2=0.002 g=0.125, a1=100, a2=100\n",">927, d1=0.002, d2=0.002 g=0.134, a1=100, a2=100\n",">928, d1=0.000, d2=0.003 g=0.151, a1=100, a2=100\n",">929, d1=0.001, d2=0.007 g=0.229, a1=100, a2=100\n",">930, d1=0.002, d2=0.001 g=0.300, a1=100, a2=100\n",">931, d1=0.002, d2=0.000 g=0.308, a1=100, a2=100\n",">932, d1=0.004, d2=0.001 g=0.263, a1=100, a2=100\n",">933, d1=0.006, d2=0.001 g=0.187, a1=100, a2=100\n",">934, d1=0.000, d2=0.001 g=0.169, a1=100, a2=100\n",">935, d1=0.001, d2=0.001 g=0.162, a1=100, a2=100\n",">936, d1=0.001, d2=0.001 g=0.166, a1=100, a2=100\n",">937, d1=0.001, d2=0.003 g=0.196, a1=100, a2=100\n",">938, d1=0.000, d2=0.003 g=0.252, a1=100, a2=100\n",">939, d1=0.002, d2=0.003 g=0.305, a1=100, a2=100\n",">940, d1=0.001, d2=0.001 g=0.354, a1=100, a2=100\n",">941, d1=0.003, d2=0.004 g=0.350, a1=100, a2=100\n",">942, d1=0.000, d2=0.000 g=0.399, a1=100, a2=100\n",">943, d1=0.001, d2=0.001 g=0.378, a1=100, a2=100\n",">944, d1=0.003, d2=0.002 g=0.400, a1=100, a2=100\n",">945, d1=0.005, d2=0.002 g=0.342, a1=100, a2=100\n",">946, d1=0.003, d2=0.001 g=0.318, a1=100, a2=100\n",">947, d1=0.000, d2=0.005 g=0.387, a1=100, a2=100\n",">948, d1=0.063, d2=0.004 g=0.008, a1=100, a2=100\n",">949, d1=0.001, d2=0.006 g=0.005, a1=100, a2=100\n",">950, d1=0.000, d2=0.005 g=0.007, a1=100, a2=100\n",">951, d1=0.000, d2=0.003 g=0.010, a1=100, a2=100\n",">952, d1=0.000, d2=0.004 g=0.015, a1=100, a2=100\n",">953, d1=0.000, d2=0.010 g=0.032, a1=100, a2=100\n",">954, d1=0.000, d2=0.007 g=0.087, a1=100, a2=100\n",">955, d1=0.001, d2=0.019 g=0.447, a1=100, a2=100\n",">956, d1=0.001, d2=0.002 g=1.009, a1=100, a2=100\n",">957, d1=0.015, d2=0.005 g=0.832, a1=100, a2=100\n",">958, d1=0.001, d2=0.000 g=0.890, a1=100, a2=100\n",">959, d1=0.001, d2=1.075 g=31.337, a1=100, a2=12\n",">960, d1=14.653, d2=0.000 g=10.497, a1=0, a2=100\n",">961, d1=0.544, d2=0.000 g=0.064, a1=71, a2=100\n",">962, d1=0.002, d2=0.000 g=0.005, a1=100, a2=100\n",">963, d1=0.000, d2=0.000 g=0.003, a1=100, a2=100\n",">964, d1=0.000, d2=0.000 g=0.003, a1=100, a2=100\n",">965, d1=0.000, d2=0.000 g=0.003, a1=100, a2=100\n",">966, d1=0.001, d2=0.000 g=0.004, a1=100, a2=100\n",">967, d1=0.000, d2=0.000 g=0.004, a1=100, a2=100\n",">968, d1=0.001, d2=0.000 g=0.004, a1=100, a2=100\n",">969, d1=0.001, d2=0.000 g=0.004, a1=100, a2=100\n",">970, d1=0.000, d2=0.000 g=0.005, a1=100, a2=100\n",">971, d1=0.000, d2=0.000 g=0.006, a1=100, a2=100\n",">972, d1=0.000, d2=0.000 g=0.006, a1=100, a2=100\n",">973, d1=0.001, d2=0.000 g=0.007, a1=100, a2=100\n",">974, d1=0.000, d2=0.000 g=0.008, a1=100, a2=100\n",">975, d1=0.001, d2=0.000 g=0.009, a1=100, a2=100\n",">976, d1=0.000, d2=0.000 g=0.009, a1=100, a2=100\n",">977, d1=0.000, d2=0.000 g=0.010, a1=100, a2=100\n",">978, d1=0.000, d2=0.000 g=0.011, a1=100, a2=100\n",">979, d1=0.000, d2=0.000 g=0.012, a1=100, a2=100\n",">980, d1=0.000, d2=0.000 g=0.013, a1=100, a2=100\n",">981, d1=0.001, d2=0.000 g=0.014, a1=100, a2=100\n",">982, d1=0.001, d2=0.000 g=0.015, a1=100, a2=100\n",">983, d1=0.000, d2=0.000 g=0.016, a1=100, a2=100\n",">984, d1=0.000, d2=0.000 g=0.018, a1=100, a2=100\n",">985, d1=0.000, d2=0.000 g=0.019, a1=100, a2=100\n",">986, d1=0.000, d2=0.000 g=0.021, a1=100, a2=100\n",">987, d1=0.000, d2=0.000 g=0.023, a1=100, a2=100\n",">988, d1=0.002, d2=0.000 g=0.024, a1=100, a2=100\n",">989, d1=0.001, d2=0.000 g=0.026, a1=100, a2=100\n",">990, d1=0.001, d2=0.000 g=0.028, a1=100, a2=100\n",">991, d1=0.000, d2=0.000 g=0.029, a1=100, a2=100\n",">992, d1=0.000, d2=0.000 g=0.031, a1=100, a2=100\n",">993, d1=0.000, d2=0.000 g=0.032, a1=100, a2=100\n",">994, d1=0.000, d2=0.000 g=0.034, a1=100, a2=100\n",">995, d1=0.001, d2=0.000 g=0.037, a1=100, a2=100\n",">996, d1=0.000, d2=0.000 g=0.039, a1=100, a2=100\n",">997, d1=0.000, d2=0.000 g=0.041, a1=100, a2=100\n",">998, d1=0.001, d2=0.000 g=0.045, a1=100, a2=100\n",">999, d1=0.000, d2=0.000 g=0.048, a1=100, a2=100\n",">1000, d1=0.001, d2=0.000 g=0.050, a1=100, a2=100\n",">1001, d1=0.000, d2=0.000 g=0.053, a1=100, a2=100\n",">1002, d1=0.000, d2=0.000 g=0.056, a1=100, a2=100\n",">1003, d1=0.000, d2=0.000 g=0.060, a1=100, a2=100\n",">1004, d1=0.000, d2=0.000 g=0.062, a1=100, a2=100\n",">1005, d1=0.001, d2=0.000 g=0.066, a1=100, a2=100\n",">1006, d1=0.000, d2=0.000 g=0.067, a1=100, a2=100\n",">1007, d1=0.000, d2=0.000 g=0.071, a1=100, a2=100\n",">1008, d1=0.000, d2=0.000 g=0.075, a1=100, a2=100\n",">1009, d1=0.000, d2=0.000 g=0.077, a1=100, a2=100\n",">1010, d1=0.000, d2=0.000 g=0.082, a1=100, a2=100\n",">1011, d1=0.000, d2=0.000 g=0.085, a1=100, a2=100\n",">1012, d1=0.000, d2=0.000 g=0.088, a1=100, a2=100\n",">1013, d1=0.001, d2=0.000 g=0.093, a1=100, a2=100\n",">1014, d1=0.000, d2=0.000 g=0.098, a1=100, a2=100\n",">1015, d1=0.001, d2=0.000 g=0.102, a1=100, a2=100\n",">1016, d1=0.000, d2=0.000 g=0.107, a1=100, a2=100\n",">1017, d1=0.000, d2=0.000 g=0.109, a1=100, a2=100\n",">1018, d1=0.000, d2=0.000 g=0.115, a1=100, a2=100\n",">1019, d1=0.000, d2=0.000 g=0.117, a1=100, a2=100\n",">1020, d1=0.001, d2=0.000 g=0.121, a1=100, a2=100\n",">1021, d1=0.000, d2=0.000 g=0.127, a1=100, a2=100\n",">1022, d1=0.000, d2=0.000 g=0.131, a1=100, a2=100\n",">1023, d1=0.001, d2=0.000 g=0.134, a1=100, a2=100\n",">1024, d1=0.000, d2=0.000 g=0.140, a1=100, a2=100\n",">1025, d1=0.000, d2=0.000 g=0.142, a1=100, a2=100\n",">1026, d1=0.001, d2=0.000 g=0.147, a1=100, a2=100\n",">1027, d1=0.000, d2=0.000 g=0.153, a1=100, a2=100\n",">1028, d1=0.000, d2=0.000 g=0.153, a1=100, a2=100\n",">1029, d1=0.000, d2=0.000 g=0.158, a1=100, a2=100\n",">1030, d1=0.000, d2=0.000 g=0.160, a1=100, a2=100\n",">1031, d1=0.000, d2=0.000 g=0.165, a1=100, a2=100\n",">1032, d1=0.001, d2=0.000 g=0.169, a1=100, a2=100\n",">1033, d1=0.000, d2=0.000 g=0.173, a1=100, a2=100\n",">1034, d1=0.001, d2=0.000 g=0.174, a1=100, a2=100\n",">1035, d1=0.000, d2=0.000 g=0.179, a1=100, a2=100\n",">1036, d1=0.000, d2=0.000 g=0.175, a1=100, a2=100\n",">1037, d1=0.000, d2=0.000 g=0.183, a1=100, a2=100\n",">1038, d1=0.001, d2=0.000 g=0.186, a1=100, a2=100\n",">1039, d1=0.000, d2=0.000 g=0.192, a1=100, a2=100\n",">1040, d1=0.001, d2=0.000 g=0.193, a1=100, a2=100\n",">1041, d1=0.000, d2=0.000 g=0.193, a1=100, a2=100\n",">1042, d1=0.000, d2=0.000 g=0.188, a1=100, a2=100\n",">1043, d1=0.000, d2=0.000 g=0.192, a1=100, a2=100\n",">1044, d1=0.000, d2=0.000 g=0.200, a1=100, a2=100\n",">1045, d1=0.000, d2=0.000 g=0.197, a1=100, a2=100\n",">1046, d1=0.000, d2=0.000 g=0.207, a1=100, a2=100\n",">1047, d1=0.000, d2=0.000 g=0.206, a1=100, a2=100\n",">1048, d1=0.000, d2=0.000 g=0.198, a1=100, a2=100\n",">1049, d1=0.000, d2=0.000 g=0.200, a1=100, a2=100\n",">1050, d1=0.000, d2=0.000 g=0.201, a1=100, a2=100\n",">1051, d1=0.000, d2=0.000 g=0.205, a1=100, a2=100\n",">1052, d1=0.000, d2=0.000 g=0.213, a1=100, a2=100\n",">1053, d1=0.000, d2=0.000 g=0.210, a1=100, a2=100\n",">1054, d1=0.000, d2=0.000 g=0.197, a1=100, a2=100\n",">1055, d1=0.000, d2=0.000 g=0.186, a1=100, a2=100\n",">1056, d1=0.000, d2=0.000 g=0.199, a1=100, a2=100\n",">1057, d1=0.000, d2=0.000 g=0.187, a1=100, a2=100\n",">1058, d1=0.000, d2=0.000 g=0.194, a1=100, a2=100\n",">1059, d1=0.000, d2=0.000 g=0.180, a1=100, a2=100\n",">1060, d1=0.000, d2=0.000 g=0.189, a1=100, a2=100\n",">1061, d1=0.000, d2=0.000 g=0.181, a1=100, a2=100\n",">1062, d1=0.000, d2=0.000 g=0.168, a1=100, a2=100\n",">1063, d1=0.000, d2=0.000 g=0.182, a1=100, a2=100\n",">1064, d1=0.001, d2=0.000 g=0.162, a1=100, a2=100\n",">1065, d1=0.000, d2=0.000 g=0.160, a1=100, a2=100\n",">1066, d1=0.000, d2=0.000 g=0.150, a1=100, a2=100\n",">1067, d1=0.000, d2=0.000 g=0.130, a1=100, a2=100\n",">1068, d1=0.000, d2=0.000 g=0.142, a1=100, a2=100\n",">1069, d1=0.000, d2=0.000 g=0.123, a1=100, a2=100\n",">1070, d1=0.000, d2=0.000 g=0.125, a1=100, a2=100\n",">1071, d1=0.000, d2=0.000 g=0.112, a1=100, a2=100\n",">1072, d1=0.000, d2=0.000 g=0.106, a1=100, a2=100\n",">1073, d1=0.000, d2=0.000 g=0.117, a1=100, a2=100\n",">1074, d1=0.000, d2=0.000 g=0.102, a1=100, a2=100\n",">1075, d1=0.000, d2=0.000 g=0.090, a1=100, a2=100\n",">1076, d1=0.000, d2=0.000 g=0.083, a1=100, a2=100\n",">1077, d1=0.000, d2=0.000 g=0.073, a1=100, a2=100\n",">1078, d1=0.000, d2=0.000 g=0.069, a1=100, a2=100\n",">1079, d1=0.000, d2=0.000 g=0.062, a1=100, a2=100\n",">1080, d1=0.001, d2=0.000 g=0.074, a1=100, a2=100\n",">1081, d1=0.000, d2=0.000 g=0.065, a1=100, a2=100\n",">1082, d1=0.000, d2=0.000 g=0.068, a1=100, a2=100\n",">1083, d1=0.000, d2=0.000 g=0.052, a1=100, a2=100\n",">1084, d1=0.000, d2=0.000 g=0.061, a1=100, a2=100\n",">1085, d1=0.000, d2=0.000 g=0.045, a1=100, a2=100\n",">1086, d1=0.000, d2=0.000 g=0.050, a1=100, a2=100\n",">1087, d1=0.000, d2=0.000 g=0.044, a1=100, a2=100\n",">1088, d1=0.000, d2=0.000 g=0.045, a1=100, a2=100\n",">1089, d1=0.000, d2=0.000 g=0.036, a1=100, a2=100\n",">1090, d1=0.000, d2=0.000 g=0.040, a1=100, a2=100\n",">1091, d1=0.000, d2=0.000 g=0.043, a1=100, a2=100\n",">1092, d1=0.000, d2=0.000 g=0.036, a1=100, a2=100\n",">1093, d1=0.000, d2=0.000 g=0.035, a1=100, a2=100\n",">1094, d1=0.000, d2=0.000 g=0.032, a1=100, a2=100\n",">1095, d1=0.000, d2=0.000 g=0.036, a1=100, a2=100\n",">1096, d1=0.000, d2=0.000 g=0.032, a1=100, a2=100\n",">1097, d1=0.000, d2=0.000 g=0.028, a1=100, a2=100\n",">1098, d1=0.000, d2=0.000 g=0.029, a1=100, a2=100\n",">1099, d1=0.000, d2=0.000 g=0.027, a1=100, a2=100\n",">1100, d1=0.001, d2=0.001 g=0.025, a1=100, a2=100\n",">1101, d1=0.000, d2=0.000 g=0.026, a1=100, a2=100\n",">1102, d1=0.000, d2=0.002 g=0.023, a1=100, a2=100\n",">1103, d1=0.001, d2=0.000 g=0.023, a1=100, a2=100\n",">1104, d1=0.000, d2=0.001 g=0.023, a1=100, a2=100\n",">1105, d1=0.000, d2=0.000 g=0.023, a1=100, a2=100\n",">1106, d1=0.000, d2=0.003 g=0.024, a1=100, a2=100\n",">1107, d1=0.000, d2=0.001 g=0.022, a1=100, a2=100\n",">1108, d1=0.001, d2=0.003 g=0.023, a1=100, a2=100\n",">1109, d1=0.000, d2=0.001 g=0.024, a1=100, a2=100\n",">1110, d1=0.000, d2=0.000 g=0.022, a1=100, a2=100\n",">1111, d1=0.000, d2=0.002 g=0.024, a1=100, a2=100\n",">1112, d1=0.000, d2=0.001 g=0.024, a1=100, a2=100\n",">1113, d1=0.000, d2=0.000 g=0.022, a1=100, a2=100\n",">1114, d1=0.000, d2=0.001 g=0.022, a1=100, a2=100\n",">1115, d1=0.000, d2=0.000 g=0.023, a1=100, a2=100\n",">1116, d1=0.000, d2=0.000 g=0.021, a1=100, a2=100\n",">1117, d1=0.001, d2=0.002 g=0.021, a1=100, a2=100\n",">1118, d1=0.000, d2=0.001 g=0.018, a1=100, a2=100\n",">1119, d1=0.000, d2=0.001 g=0.019, a1=100, a2=100\n",">1120, d1=0.000, d2=0.001 g=0.018, a1=100, a2=100\n",">1121, d1=0.000, d2=0.001 g=0.019, a1=100, a2=100\n",">1122, d1=0.000, d2=0.001 g=0.019, a1=100, a2=100\n",">1123, d1=0.000, d2=0.001 g=0.018, a1=100, a2=100\n",">1124, d1=0.000, d2=0.001 g=0.018, a1=100, a2=100\n",">1125, d1=0.000, d2=0.001 g=0.020, a1=100, a2=100\n",">1126, d1=0.000, d2=0.002 g=0.020, a1=100, a2=100\n",">1127, d1=0.001, d2=0.003 g=0.020, a1=100, a2=100\n",">1128, d1=0.000, d2=0.002 g=0.020, a1=100, a2=100\n",">1129, d1=0.000, d2=0.004 g=0.023, a1=100, a2=100\n",">1130, d1=0.000, d2=0.005 g=0.025, a1=100, a2=100\n",">1131, d1=0.001, d2=0.001 g=0.026, a1=100, a2=100\n",">1132, d1=0.000, d2=0.023 g=0.037, a1=100, a2=100\n",">1133, d1=0.000, d2=0.001 g=0.050, a1=100, a2=100\n",">1134, d1=0.000, d2=0.004 g=0.052, a1=100, a2=100\n",">1135, d1=0.001, d2=0.001 g=0.052, a1=100, a2=100\n",">1136, d1=0.001, d2=0.007 g=0.058, a1=100, a2=100\n",">1137, d1=0.001, d2=0.002 g=0.063, a1=100, a2=100\n",">1138, d1=0.000, d2=0.006 g=0.066, a1=100, a2=100\n",">1139, d1=0.001, d2=0.003 g=0.065, a1=100, a2=100\n",">1140, d1=0.004, d2=0.018 g=0.091, a1=100, a2=100\n",">1141, d1=0.000, d2=0.007 g=0.122, a1=100, a2=100\n",">1142, d1=0.001, d2=0.006 g=0.149, a1=100, a2=100\n",">1143, d1=0.000, d2=0.019 g=0.203, a1=100, a2=100\n",">1144, d1=0.001, d2=0.012 g=0.299, a1=100, a2=100\n",">1145, d1=0.001, d2=0.017 g=0.440, a1=100, a2=100\n",">1146, d1=0.003, d2=0.039 g=0.844, a1=100, a2=100\n",">1147, d1=0.001, d2=0.037 g=1.685, a1=100, a2=100\n",">1148, d1=0.005, d2=0.138 g=5.038, a1=100, a2=100\n",">1149, d1=0.041, d2=0.026 g=6.748, a1=100, a2=100\n",">1150, d1=0.048, d2=0.299 g=12.316, a1=100, a2=100\n",">1151, d1=0.643, d2=0.229 g=8.608, a1=62, a2=100\n",">1152, d1=0.012, d2=0.319 g=15.246, a1=100, a2=100\n",">1153, d1=1.226, d2=1.450 g=13.382, a1=12, a2=3\n",">1154, d1=0.501, d2=0.078 g=11.085, a1=78, a2=100\n",">1155, d1=0.102, d2=0.225 g=10.391, a1=100, a2=93\n",">1156, d1=0.476, d2=0.487 g=9.061, a1=71, a2=75\n",">1157, d1=0.083, d2=0.001 g=9.698, a1=100, a2=100\n",">1158, d1=0.319, d2=0.000 g=5.980, a1=90, a2=100\n",">1159, d1=0.009, d2=0.001 g=4.761, a1=100, a2=100\n",">1160, d1=0.005, d2=0.003 g=4.103, a1=100, a2=100\n",">1161, d1=0.003, d2=0.004 g=3.588, a1=100, a2=100\n",">1162, d1=0.006, d2=0.010 g=3.482, a1=100, a2=100\n",">1163, d1=0.014, d2=0.001 g=2.800, a1=100, a2=100\n",">1164, d1=0.002, d2=0.001 g=2.450, a1=100, a2=100\n",">1165, d1=0.004, d2=0.000 g=2.028, a1=100, a2=100\n",">1166, d1=0.001, d2=0.000 g=1.689, a1=100, a2=100\n",">1167, d1=0.003, d2=0.005 g=1.370, a1=100, a2=100\n",">1168, d1=0.003, d2=0.011 g=1.180, a1=100, a2=100\n",">1169, d1=0.003, d2=0.038 g=1.289, a1=100, a2=100\n",">1170, d1=0.005, d2=0.016 g=1.389, a1=100, a2=100\n",">1171, d1=0.002, d2=0.299 g=4.851, a1=100, a2=93\n",">1172, d1=0.041, d2=0.020 g=7.078, a1=100, a2=100\n",">1173, d1=0.214, d2=0.023 g=4.242, a1=96, a2=100\n",">1174, d1=0.011, d2=0.283 g=7.459, a1=100, a2=100\n",">1175, d1=0.078, d2=0.004 g=8.409, a1=100, a2=100\n",">1176, d1=0.346, d2=0.011 g=2.208, a1=90, a2=100\n",">1177, d1=0.013, d2=0.947 g=9.776, a1=100, a2=37\n",">1178, d1=0.521, d2=0.044 g=7.888, a1=81, a2=100\n",">1179, d1=0.067, d2=0.004 g=5.485, a1=100, a2=100\n",">1180, d1=0.182, d2=0.212 g=4.269, a1=100, a2=100\n",">1181, d1=0.042, d2=3.100 g=18.185, a1=100, a2=0\n",">1182, d1=4.657, d2=1.529 g=12.906, a1=0, a2=43\n",">1183, d1=1.561, d2=2.591 g=9.115, a1=3, a2=3\n",">1184, d1=0.615, d2=0.511 g=8.711, a1=71, a2=65\n",">1185, d1=1.059, d2=0.679 g=7.090, a1=56, a2=46\n",">1186, d1=0.473, d2=0.191 g=5.949, a1=78, a2=100\n",">1187, d1=0.426, d2=0.129 g=4.369, a1=81, a2=100\n",">1188, d1=0.116, d2=0.138 g=4.100, a1=100, a2=100\n",">1189, d1=0.153, d2=0.039 g=3.336, a1=96, a2=100\n",">1190, d1=0.111, d2=0.056 g=2.651, a1=96, a2=100\n",">1191, d1=0.045, d2=0.024 g=2.153, a1=100, a2=100\n",">1192, d1=0.052, d2=0.074 g=2.090, a1=100, a2=100\n",">1193, d1=0.088, d2=0.015 g=1.614, a1=100, a2=100\n",">1194, d1=0.053, d2=0.052 g=1.460, a1=100, a2=100\n",">1195, d1=0.026, d2=0.115 g=1.969, a1=100, a2=100\n",">1196, d1=0.019, d2=0.013 g=2.336, a1=100, a2=100\n",">1197, d1=0.059, d2=0.006 g=1.778, a1=100, a2=100\n",">1198, d1=0.111, d2=0.011 g=0.758, a1=100, a2=100\n",">1199, d1=0.070, d2=0.028 g=0.375, a1=100, a2=100\n",">1200, d1=0.011, d2=0.086 g=0.555, a1=100, a2=100\n",">1201, d1=0.011, d2=0.017 g=0.725, a1=100, a2=100\n",">1202, d1=0.008, d2=0.050 g=1.069, a1=100, a2=100\n",">1203, d1=0.021, d2=0.050 g=1.344, a1=100, a2=100\n",">1204, d1=0.053, d2=0.027 g=1.367, a1=100, a2=100\n",">1205, d1=0.092, d2=0.031 g=0.863, a1=100, a2=100\n",">1206, d1=0.023, d2=0.083 g=1.161, a1=100, a2=100\n",">1207, d1=0.016, d2=0.077 g=1.972, a1=100, a2=100\n",">1208, d1=0.032, d2=0.031 g=2.351, a1=100, a2=100\n",">1209, d1=0.030, d2=0.141 g=3.331, a1=100, a2=100\n",">1210, d1=0.077, d2=0.024 g=3.438, a1=96, a2=100\n",">1211, d1=0.122, d2=0.122 g=3.131, a1=100, a2=100\n",">1212, d1=0.166, d2=0.352 g=4.698, a1=96, a2=100\n",">1213, d1=0.158, d2=0.034 g=4.855, a1=100, a2=100\n",">1214, d1=0.089, d2=0.014 g=3.472, a1=100, a2=100\n",">1215, d1=0.063, d2=0.007 g=2.202, a1=100, a2=100\n",">1216, d1=0.050, d2=0.006 g=1.257, a1=100, a2=100\n",">1217, d1=0.063, d2=0.011 g=0.579, a1=100, a2=100\n",">1218, d1=0.025, d2=0.050 g=0.503, a1=100, a2=100\n",">1219, d1=0.007, d2=0.010 g=0.519, a1=100, a2=100\n",">1220, d1=0.008, d2=0.098 g=0.973, a1=100, a2=100\n",">1221, d1=0.014, d2=0.030 g=1.493, a1=100, a2=100\n",">1222, d1=0.028, d2=0.026 g=1.535, a1=100, a2=100\n",">1223, d1=0.021, d2=0.015 g=1.540, a1=100, a2=100\n",">1224, d1=0.061, d2=0.049 g=1.318, a1=100, a2=100\n",">1225, d1=0.027, d2=0.017 g=1.237, a1=100, a2=100\n",">1226, d1=0.067, d2=0.030 g=0.871, a1=100, a2=100\n",">1227, d1=0.008, d2=0.013 g=0.799, a1=100, a2=100\n",">1228, d1=0.005, d2=0.059 g=1.173, a1=100, a2=100\n",">1229, d1=0.038, d2=0.009 g=1.191, a1=100, a2=100\n",">1230, d1=0.018, d2=0.002 g=0.911, a1=100, a2=100\n",">1231, d1=0.038, d2=0.030 g=0.700, a1=100, a2=100\n",">1232, d1=0.030, d2=0.008 g=0.612, a1=100, a2=100\n",">1233, d1=0.022, d2=0.016 g=0.472, a1=100, a2=100\n",">1234, d1=0.016, d2=0.062 g=0.812, a1=100, a2=100\n",">1235, d1=0.019, d2=0.005 g=1.098, a1=100, a2=100\n",">1236, d1=0.021, d2=0.009 g=0.945, a1=100, a2=100\n",">1237, d1=0.015, d2=0.008 g=0.835, a1=100, a2=100\n",">1238, d1=0.008, d2=0.020 g=0.872, a1=100, a2=100\n",">1239, d1=0.035, d2=0.016 g=0.890, a1=100, a2=100\n",">1240, d1=0.005, d2=0.040 g=1.155, a1=100, a2=100\n",">1241, d1=0.010, d2=0.033 g=1.787, a1=100, a2=100\n",">1242, d1=0.017, d2=0.005 g=1.952, a1=100, a2=100\n",">1243, d1=0.005, d2=0.028 g=2.185, a1=100, a2=100\n",">1244, d1=0.016, d2=0.001 g=2.097, a1=100, a2=100\n",">1245, d1=0.068, d2=0.005 g=1.184, a1=100, a2=100\n",">1246, d1=0.005, d2=0.020 g=1.055, a1=100, a2=100\n",">1247, d1=0.006, d2=0.105 g=2.012, a1=100, a2=100\n",">1248, d1=0.008, d2=0.030 g=2.870, a1=100, a2=100\n",">1249, d1=0.074, d2=0.078 g=2.580, a1=100, a2=100\n",">1250, d1=0.090, d2=0.291 g=4.419, a1=100, a2=100\n",">1251, d1=0.466, d2=0.098 g=1.183, a1=78, a2=100\n",">1252, d1=0.014, d2=0.522 g=4.616, a1=100, a2=90\n",">1253, d1=0.076, d2=0.020 g=6.727, a1=100, a2=100\n",">1254, d1=0.222, d2=0.075 g=4.115, a1=96, a2=100\n",">1255, d1=0.059, d2=0.059 g=3.071, a1=100, a2=100\n",">1256, d1=0.068, d2=0.114 g=3.304, a1=100, a2=100\n",">1257, d1=0.055, d2=0.227 g=5.338, a1=100, a2=100\n",">1258, d1=0.504, d2=0.192 g=2.429, a1=81, a2=100\n",">1259, d1=0.035, d2=0.451 g=5.864, a1=100, a2=96\n",">1260, d1=0.181, d2=0.014 g=6.014, a1=96, a2=100\n",">1261, d1=0.530, d2=0.146 g=1.387, a1=78, a2=100\n",">1262, d1=0.006, d2=0.289 g=3.253, a1=100, a2=100\n",">1263, d1=0.053, d2=0.022 g=4.254, a1=100, a2=100\n",">1264, d1=0.247, d2=0.008 g=0.913, a1=93, a2=100\n",">1265, d1=0.022, d2=0.003 g=0.358, a1=100, a2=100\n",">1266, d1=0.013, d2=0.002 g=0.206, a1=100, a2=100\n",">1267, d1=0.014, d2=0.009 g=0.156, a1=100, a2=100\n",">1268, d1=0.021, d2=0.016 g=0.135, a1=100, a2=100\n",">1269, d1=0.012, d2=0.008 g=0.138, a1=100, a2=100\n",">1270, d1=0.001, d2=0.005 g=0.131, a1=100, a2=100\n",">1271, d1=0.008, d2=0.007 g=0.130, a1=100, a2=100\n",">1272, d1=0.009, d2=0.007 g=0.138, a1=100, a2=100\n",">1273, d1=0.010, d2=0.005 g=0.134, a1=100, a2=100\n",">1274, d1=0.009, d2=0.003 g=0.108, a1=100, a2=100\n",">1275, d1=0.007, d2=0.003 g=0.107, a1=100, a2=100\n",">1276, d1=0.004, d2=0.005 g=0.102, a1=100, a2=100\n",">1277, d1=0.017, d2=0.004 g=0.080, a1=100, a2=100\n",">1278, d1=0.006, d2=0.005 g=0.083, a1=100, a2=100\n",">1279, d1=0.003, d2=0.003 g=0.079, a1=100, a2=100\n",">1280, d1=0.016, d2=0.009 g=0.072, a1=100, a2=100\n",">1281, d1=0.008, d2=0.013 g=0.082, a1=100, a2=100\n",">1282, d1=0.001, d2=0.008 g=0.092, a1=100, a2=100\n",">1283, d1=0.005, d2=0.011 g=0.115, a1=100, a2=100\n",">1284, d1=0.004, d2=0.002 g=0.123, a1=100, a2=100\n",">1285, d1=0.006, d2=0.002 g=0.124, a1=100, a2=100\n",">1286, d1=0.006, d2=0.018 g=0.137, a1=100, a2=100\n",">1287, d1=0.003, d2=0.030 g=0.235, a1=100, a2=100\n",">1288, d1=0.008, d2=0.006 g=0.302, a1=100, a2=100\n",">1289, d1=0.008, d2=0.014 g=0.363, a1=100, a2=100\n",">1290, d1=0.061, d2=0.013 g=0.167, a1=100, a2=100\n",">1291, d1=0.008, d2=0.048 g=0.286, a1=100, a2=100\n",">1292, d1=0.005, d2=0.011 g=0.468, a1=100, a2=100\n",">1293, d1=0.005, d2=0.014 g=0.602, a1=100, a2=100\n",">1294, d1=0.009, d2=0.089 g=1.587, a1=100, a2=100\n",">1295, d1=0.039, d2=0.003 g=1.799, a1=100, a2=100\n",">1296, d1=0.019, d2=0.021 g=1.688, a1=100, a2=100\n",">1297, d1=0.028, d2=0.066 g=2.405, a1=100, a2=100\n",">1298, d1=0.026, d2=0.003 g=2.616, a1=100, a2=100\n",">1299, d1=0.119, d2=0.291 g=5.298, a1=100, a2=100\n",">1300, d1=0.156, d2=0.000 g=3.990, a1=96, a2=100\n",">1301, d1=0.071, d2=0.298 g=6.441, a1=100, a2=100\n",">1302, d1=0.376, d2=0.372 g=7.157, a1=93, a2=100\n",">1303, d1=0.265, d2=0.011 g=3.399, a1=100, a2=100\n",">1304, d1=0.031, d2=0.011 g=1.956, a1=100, a2=100\n",">1305, d1=0.012, d2=0.002 g=1.304, a1=100, a2=100\n",">1306, d1=0.016, d2=0.007 g=0.954, a1=100, a2=100\n",">1307, d1=0.022, d2=0.021 g=0.835, a1=100, a2=100\n",">1308, d1=0.013, d2=0.021 g=0.965, a1=100, a2=100\n",">1309, d1=0.033, d2=0.018 g=0.904, a1=100, a2=100\n",">1310, d1=0.017, d2=0.057 g=1.522, a1=100, a2=100\n",">1311, d1=0.017, d2=0.055 g=3.021, a1=100, a2=100\n",">1312, d1=0.162, d2=0.101 g=1.745, a1=100, a2=100\n",">1313, d1=0.007, d2=1.006 g=16.184, a1=100, a2=18\n",">1314, d1=5.064, d2=0.016 g=0.616, a1=0, a2=100\n",">1315, d1=0.096, d2=2.034 g=5.846, a1=100, a2=0\n",">1316, d1=4.258, d2=0.023 g=0.007, a1=0, a2=100\n",">1317, d1=0.117, d2=0.186 g=0.001, a1=96, a2=96\n",">1318, d1=0.070, d2=0.011 g=0.001, a1=100, a2=100\n",">1319, d1=0.039, d2=0.023 g=0.001, a1=100, a2=100\n",">1320, d1=0.049, d2=0.006 g=0.001, a1=100, a2=100\n",">1321, d1=0.011, d2=0.007 g=0.001, a1=100, a2=100\n",">1322, d1=0.008, d2=0.014 g=0.001, a1=100, a2=100\n",">1323, d1=0.008, d2=0.021 g=0.002, a1=100, a2=100\n",">1324, d1=0.005, d2=0.020 g=0.002, a1=100, a2=100\n",">1325, d1=0.010, d2=0.005 g=0.002, a1=100, a2=100\n",">1326, d1=0.019, d2=0.024 g=0.003, a1=100, a2=100\n",">1327, d1=0.021, d2=0.019 g=0.004, a1=100, a2=100\n",">1328, d1=0.004, d2=0.023 g=0.006, a1=100, a2=100\n",">1329, d1=0.010, d2=0.053 g=0.010, a1=100, a2=100\n",">1330, d1=0.016, d2=0.008 g=0.015, a1=100, a2=100\n",">1331, d1=0.028, d2=0.028 g=0.019, a1=100, a2=100\n",">1332, d1=0.045, d2=0.026 g=0.020, a1=100, a2=100\n",">1333, d1=0.021, d2=0.050 g=0.030, a1=100, a2=100\n",">1334, d1=0.046, d2=0.056 g=0.047, a1=100, a2=100\n",">1335, d1=0.041, d2=0.243 g=0.374, a1=100, a2=100\n",">1336, d1=0.048, d2=0.285 g=2.990, a1=100, a2=100\n",">1337, d1=1.010, d2=1.093 g=3.948, a1=40, a2=12\n",">1338, d1=0.228, d2=0.562 g=8.440, a1=100, a2=62\n",">1339, d1=1.607, d2=1.567 g=8.113, a1=9, a2=0\n",">1340, d1=1.057, d2=0.507 g=6.817, a1=46, a2=81\n",">1341, d1=0.404, d2=0.375 g=5.858, a1=84, a2=87\n",">1342, d1=0.325, d2=0.454 g=5.808, a1=90, a2=87\n",">1343, d1=0.907, d2=0.125 g=1.590, a1=31, a2=100\n",">1344, d1=0.041, d2=0.018 g=0.834, a1=100, a2=100\n",">1345, d1=0.051, d2=0.019 g=0.460, a1=100, a2=100\n",">1346, d1=0.015, d2=0.022 g=0.348, a1=100, a2=100\n",">1347, d1=0.013, d2=0.029 g=0.319, a1=100, a2=100\n",">1348, d1=0.009, d2=0.037 g=0.373, a1=100, a2=100\n",">1349, d1=0.040, d2=0.066 g=0.528, a1=100, a2=100\n",">1350, d1=0.014, d2=0.071 g=0.890, a1=100, a2=100\n",">1351, d1=0.015, d2=0.112 g=1.612, a1=100, a2=100\n",">1352, d1=0.101, d2=0.069 g=1.919, a1=100, a2=100\n",">1353, d1=0.048, d2=0.100 g=2.592, a1=100, a2=100\n",">1354, d1=0.051, d2=0.046 g=2.841, a1=100, a2=100\n",">1355, d1=0.057, d2=0.248 g=4.707, a1=100, a2=100\n",">1356, d1=0.153, d2=0.009 g=4.685, a1=100, a2=100\n",">1357, d1=0.343, d2=0.197 g=2.841, a1=87, a2=100\n",">1358, d1=0.064, d2=0.286 g=5.125, a1=100, a2=100\n",">1359, d1=0.048, d2=0.006 g=5.946, a1=100, a2=100\n",">1360, d1=0.465, d2=0.035 g=1.626, a1=81, a2=100\n",">1361, d1=0.009, d2=1.380 g=7.026, a1=100, a2=0\n",">1362, d1=0.327, d2=0.002 g=8.025, a1=90, a2=100\n",">1363, d1=0.210, d2=0.003 g=5.757, a1=100, a2=100\n",">1364, d1=0.055, d2=0.107 g=4.907, a1=100, a2=100\n",">1365, d1=0.048, d2=0.202 g=5.598, a1=100, a2=100\n",">1366, d1=0.234, d2=0.077 g=4.581, a1=93, a2=100\n",">1367, d1=0.079, d2=0.294 g=5.485, a1=100, a2=100\n",">1368, d1=0.155, d2=0.056 g=5.058, a1=100, a2=100\n",">1369, d1=0.070, d2=0.045 g=4.318, a1=100, a2=100\n",">1370, d1=0.077, d2=0.170 g=4.346, a1=100, a2=100\n",">1371, d1=0.214, d2=0.091 g=3.316, a1=100, a2=100\n",">1372, d1=0.075, d2=0.434 g=5.337, a1=100, a2=87\n",">1373, d1=0.235, d2=0.004 g=4.712, a1=93, a2=100\n",">1374, d1=0.083, d2=0.161 g=4.644, a1=100, a2=100\n",">1375, d1=0.090, d2=0.045 g=4.130, a1=100, a2=100\n",">1376, d1=0.072, d2=0.026 g=3.335, a1=100, a2=100\n",">1377, d1=0.062, d2=0.108 g=3.414, a1=100, a2=100\n",">1378, d1=0.126, d2=0.034 g=2.654, a1=96, a2=100\n",">1379, d1=0.043, d2=0.260 g=4.329, a1=100, a2=100\n",">1380, d1=0.038, d2=0.002 g=4.912, a1=100, a2=100\n",">1381, d1=0.314, d2=0.001 g=1.672, a1=93, a2=100\n",">1382, d1=0.017, d2=0.345 g=2.477, a1=100, a2=93\n",">1383, d1=0.186, d2=0.021 g=2.135, a1=96, a2=100\n",">1384, d1=0.085, d2=0.070 g=1.631, a1=100, a2=100\n",">1385, d1=0.033, d2=0.099 g=2.041, a1=100, a2=100\n",">1386, d1=0.032, d2=0.002 g=2.190, a1=100, a2=100\n",">1387, d1=0.048, d2=0.163 g=2.949, a1=100, a2=100\n",">1388, d1=0.073, d2=0.003 g=3.165, a1=100, a2=100\n",">1389, d1=0.172, d2=0.570 g=5.581, a1=100, a2=71\n",">1390, d1=0.250, d2=0.001 g=4.742, a1=96, a2=100\n",">1391, d1=0.080, d2=0.008 g=3.132, a1=100, a2=100\n",">1392, d1=0.066, d2=0.014 g=2.123, a1=100, a2=100\n",">1393, d1=0.011, d2=0.015 g=1.761, a1=100, a2=100\n",">1394, d1=0.013, d2=0.022 g=1.592, a1=100, a2=100\n",">1395, d1=0.034, d2=0.019 g=1.401, a1=100, a2=100\n",">1396, d1=0.027, d2=0.034 g=1.292, a1=100, a2=100\n",">1397, d1=0.011, d2=0.021 g=1.374, a1=100, a2=100\n",">1398, d1=0.032, d2=0.022 g=1.278, a1=100, a2=100\n",">1399, d1=0.081, d2=0.017 g=0.730, a1=100, a2=100\n",">1400, d1=0.015, d2=0.353 g=3.273, a1=100, a2=100\n",">1401, d1=0.031, d2=0.000 g=5.313, a1=100, a2=100\n",">1402, d1=0.178, d2=0.116 g=4.679, a1=100, a2=100\n",">1403, d1=0.194, d2=0.053 g=2.216, a1=96, a2=100\n",">1404, d1=0.031, d2=0.094 g=2.310, a1=100, a2=100\n",">1405, d1=0.024, d2=0.035 g=2.798, a1=100, a2=100\n",">1406, d1=0.113, d2=0.004 g=1.706, a1=100, a2=100\n",">1407, d1=0.028, d2=0.562 g=6.122, a1=100, a2=75\n",">1408, d1=0.516, d2=0.000 g=2.410, a1=78, a2=100\n",">1409, d1=0.039, d2=0.122 g=1.811, a1=100, a2=100\n",">1410, d1=0.122, d2=0.018 g=1.085, a1=96, a2=100\n",">1411, d1=0.018, d2=0.005 g=0.863, a1=100, a2=100\n",">1412, d1=0.032, d2=0.030 g=0.667, a1=100, a2=100\n",">1413, d1=0.009, d2=0.040 g=0.882, a1=100, a2=100\n",">1414, d1=0.021, d2=0.015 g=0.907, a1=100, a2=100\n",">1415, d1=0.032, d2=0.107 g=1.647, a1=100, a2=100\n",">1416, d1=0.028, d2=0.000 g=2.087, a1=100, a2=100\n",">1417, d1=0.011, d2=0.075 g=3.262, a1=100, a2=100\n",">1418, d1=0.084, d2=0.000 g=2.227, a1=100, a2=100\n",">1419, d1=0.121, d2=0.005 g=0.575, a1=100, a2=100\n",">1420, d1=0.007, d2=0.121 g=0.907, a1=100, a2=100\n",">1421, d1=0.009, d2=0.053 g=1.769, a1=100, a2=100\n",">1422, d1=0.051, d2=0.044 g=1.927, a1=100, a2=100\n",">1423, d1=0.217, d2=0.136 g=1.083, a1=93, a2=100\n",">1424, d1=0.019, d2=0.241 g=3.777, a1=100, a2=100\n",">1425, d1=0.102, d2=0.002 g=3.970, a1=100, a2=100\n",">1426, d1=0.426, d2=0.710 g=2.735, a1=84, a2=40\n",">1427, d1=0.194, d2=0.115 g=3.210, a1=100, a2=100\n",">1428, d1=0.282, d2=0.002 g=0.991, a1=90, a2=100\n",">1429, d1=0.027, d2=0.005 g=0.436, a1=100, a2=100\n",">1430, d1=0.012, d2=0.008 g=0.289, a1=100, a2=100\n",">1431, d1=0.003, d2=0.009 g=0.272, a1=100, a2=100\n",">1432, d1=0.006, d2=0.022 g=0.293, a1=100, a2=100\n",">1433, d1=0.004, d2=0.100 g=0.721, a1=100, a2=100\n",">1434, d1=0.069, d2=0.006 g=0.734, a1=100, a2=100\n",">1435, d1=0.033, d2=0.097 g=1.107, a1=100, a2=100\n",">1436, d1=0.018, d2=0.010 g=1.450, a1=100, a2=100\n",">1437, d1=0.033, d2=0.019 g=1.242, a1=100, a2=100\n",">1438, d1=0.013, d2=0.017 g=1.248, a1=100, a2=100\n",">1439, d1=0.051, d2=0.069 g=1.324, a1=100, a2=100\n",">1440, d1=0.073, d2=0.101 g=1.724, a1=100, a2=100\n",">1441, d1=0.100, d2=0.018 g=1.175, a1=100, a2=100\n",">1442, d1=0.028, d2=0.185 g=2.620, a1=100, a2=100\n",">1443, d1=0.104, d2=0.001 g=2.352, a1=100, a2=100\n",">1444, d1=0.058, d2=0.001 g=1.361, a1=100, a2=100\n",">1445, d1=0.025, d2=0.004 g=0.840, a1=100, a2=100\n",">1446, d1=0.028, d2=0.053 g=0.839, a1=100, a2=100\n",">1447, d1=0.032, d2=0.017 g=0.884, a1=100, a2=100\n",">1448, d1=0.015, d2=0.008 g=0.892, a1=100, a2=100\n",">1449, d1=0.012, d2=0.028 g=1.008, a1=100, a2=100\n",">1450, d1=0.039, d2=0.024 g=0.908, a1=100, a2=100\n",">1451, d1=0.013, d2=0.027 g=1.173, a1=100, a2=100\n",">1452, d1=0.021, d2=0.001 g=1.053, a1=100, a2=100\n",">1453, d1=0.018, d2=0.054 g=1.338, a1=100, a2=100\n",">1454, d1=0.161, d2=0.012 g=0.208, a1=96, a2=100\n",">1455, d1=0.011, d2=0.017 g=0.155, a1=100, a2=100\n",">1456, d1=0.008, d2=0.014 g=0.154, a1=100, a2=100\n",">1457, d1=0.004, d2=0.029 g=0.236, a1=100, a2=100\n",">1458, d1=0.033, d2=0.054 g=0.372, a1=100, a2=100\n",">1459, d1=0.010, d2=0.014 g=0.590, a1=100, a2=100\n",">1460, d1=0.008, d2=0.016 g=0.809, a1=100, a2=100\n",">1461, d1=0.050, d2=0.016 g=0.513, a1=100, a2=100\n",">1462, d1=0.012, d2=0.018 g=0.536, a1=100, a2=100\n",">1463, d1=0.014, d2=0.010 g=0.588, a1=100, a2=100\n",">1464, d1=0.012, d2=0.009 g=0.582, a1=100, a2=100\n",">1465, d1=0.074, d2=0.021 g=0.195, a1=100, a2=100\n",">1466, d1=0.009, d2=0.018 g=0.206, a1=100, a2=100\n",">1467, d1=0.016, d2=0.007 g=0.190, a1=100, a2=100\n",">1468, d1=0.004, d2=0.032 g=0.313, a1=100, a2=100\n",">1469, d1=0.007, d2=0.005 g=0.415, a1=100, a2=100\n",">1470, d1=0.009, d2=0.005 g=0.415, a1=100, a2=100\n",">1471, d1=0.050, d2=0.010 g=0.185, a1=100, a2=100\n",">1472, d1=0.007, d2=0.005 g=0.149, a1=100, a2=100\n",">1473, d1=0.005, d2=0.019 g=0.196, a1=100, a2=100\n",">1474, d1=0.006, d2=0.007 g=0.246, a1=100, a2=100\n",">1475, d1=0.021, d2=0.003 g=0.187, a1=100, a2=100\n",">1476, d1=0.016, d2=0.005 g=0.155, a1=100, a2=100\n",">1477, d1=0.008, d2=0.008 g=0.147, a1=100, a2=100\n",">1478, d1=0.005, d2=0.007 g=0.151, a1=100, a2=100\n",">1479, d1=0.005, d2=0.004 g=0.163, a1=100, a2=100\n",">1480, d1=0.003, d2=0.005 g=0.174, a1=100, a2=100\n",">1481, d1=0.044, d2=0.015 g=0.090, a1=100, a2=100\n",">1482, d1=0.001, d2=0.006 g=0.098, a1=100, a2=100\n",">1483, d1=0.004, d2=0.005 g=0.107, a1=100, a2=100\n",">1484, d1=0.004, d2=0.028 g=0.174, a1=100, a2=100\n",">1485, d1=0.002, d2=0.005 g=0.289, a1=100, a2=100\n",">1486, d1=0.015, d2=0.003 g=0.238, a1=100, a2=100\n",">1487, d1=0.004, d2=0.001 g=0.224, a1=100, a2=100\n",">1488, d1=0.039, d2=0.004 g=0.101, a1=100, a2=100\n",">1489, d1=0.007, d2=0.013 g=0.096, a1=100, a2=100\n",">1490, d1=0.005, d2=0.019 g=0.136, a1=100, a2=100\n",">1491, d1=0.001, d2=0.005 g=0.194, a1=100, a2=100\n",">1492, d1=0.002, d2=0.001 g=0.226, a1=100, a2=100\n",">1493, d1=0.002, d2=0.001 g=0.221, a1=100, a2=100\n",">1494, d1=0.003, d2=0.003 g=0.211, a1=100, a2=100\n",">1495, d1=0.006, d2=0.001 g=0.200, a1=100, a2=100\n",">1496, d1=0.003, d2=0.005 g=0.209, a1=100, a2=100\n",">1497, d1=0.003, d2=0.003 g=0.216, a1=100, a2=100\n",">1498, d1=0.006, d2=0.002 g=0.210, a1=100, a2=100\n",">1499, d1=0.003, d2=0.003 g=0.186, a1=100, a2=100\n",">1500, d1=0.010, d2=0.002 g=0.161, a1=100, a2=100\n",">1501, d1=0.002, d2=0.004 g=0.172, a1=100, a2=100\n",">1502, d1=0.002, d2=0.002 g=0.181, a1=100, a2=100\n",">1503, d1=0.002, d2=0.010 g=0.212, a1=100, a2=100\n",">1504, d1=0.004, d2=0.003 g=0.227, a1=100, a2=100\n",">1505, d1=0.004, d2=0.004 g=0.269, a1=100, a2=100\n",">1506, d1=0.007, d2=0.006 g=0.264, a1=100, a2=100\n",">1507, d1=0.017, d2=0.002 g=0.212, a1=100, a2=100\n",">1508, d1=0.002, d2=0.003 g=0.189, a1=100, a2=100\n",">1509, d1=0.003, d2=0.002 g=0.181, a1=100, a2=100\n",">1510, d1=0.011, d2=0.005 g=0.173, a1=100, a2=100\n",">1511, d1=0.011, d2=0.003 g=0.137, a1=100, a2=100\n",">1512, d1=0.002, d2=0.008 g=0.157, a1=100, a2=100\n",">1513, d1=0.002, d2=0.011 g=0.217, a1=100, a2=100\n",">1514, d1=0.017, d2=0.007 g=0.173, a1=100, a2=100\n",">1515, d1=0.005, d2=0.003 g=0.185, a1=100, a2=100\n",">1516, d1=0.025, d2=0.006 g=0.101, a1=100, a2=100\n",">1517, d1=0.003, d2=0.013 g=0.120, a1=100, a2=100\n",">1518, d1=0.004, d2=0.008 g=0.153, a1=100, a2=100\n",">1519, d1=0.005, d2=0.007 g=0.175, a1=100, a2=100\n",">1520, d1=0.005, d2=0.005 g=0.203, a1=100, a2=100\n",">1521, d1=0.003, d2=0.003 g=0.219, a1=100, a2=100\n",">1522, d1=0.001, d2=0.002 g=0.249, a1=100, a2=100\n",">1523, d1=0.001, d2=0.006 g=0.249, a1=100, a2=100\n",">1524, d1=0.009, d2=0.003 g=0.227, a1=100, a2=100\n",">1525, d1=0.008, d2=0.004 g=0.202, a1=100, a2=100\n",">1526, d1=0.008, d2=0.003 g=0.180, a1=100, a2=100\n",">1527, d1=0.002, d2=0.003 g=0.185, a1=100, a2=100\n",">1528, d1=0.015, d2=0.005 g=0.127, a1=100, a2=100\n",">1529, d1=0.005, d2=0.009 g=0.137, a1=100, a2=100\n",">1530, d1=0.005, d2=0.006 g=0.166, a1=100, a2=100\n",">1531, d1=0.003, d2=0.007 g=0.205, a1=100, a2=100\n",">1532, d1=0.015, d2=0.005 g=0.187, a1=100, a2=100\n",">1533, d1=0.004, d2=0.009 g=0.213, a1=100, a2=100\n",">1534, d1=0.002, d2=0.006 g=0.274, a1=100, a2=100\n",">1535, d1=0.012, d2=0.024 g=0.393, a1=100, a2=100\n",">1536, d1=0.021, d2=0.003 g=0.317, a1=100, a2=100\n",">1537, d1=0.005, d2=0.004 g=0.303, a1=100, a2=100\n",">1538, d1=0.002, d2=0.009 g=0.337, a1=100, a2=100\n",">1539, d1=0.006, d2=0.002 g=0.367, a1=100, a2=100\n",">1540, d1=0.003, d2=0.006 g=0.372, a1=100, a2=100\n",">1541, d1=0.028, d2=0.010 g=0.191, a1=100, a2=100\n",">1542, d1=0.031, d2=0.011 g=0.092, a1=100, a2=100\n",">1543, d1=0.002, d2=0.021 g=0.154, a1=100, a2=100\n",">1544, d1=0.003, d2=0.005 g=0.246, a1=100, a2=100\n",">1545, d1=0.004, d2=0.002 g=0.269, a1=100, a2=100\n",">1546, d1=0.002, d2=0.009 g=0.329, a1=100, a2=100\n",">1547, d1=0.010, d2=0.004 g=0.342, a1=100, a2=100\n",">1548, d1=0.017, d2=0.010 g=0.295, a1=100, a2=100\n",">1549, d1=0.002, d2=0.004 g=0.298, a1=100, a2=100\n",">1550, d1=0.006, d2=0.003 g=0.305, a1=100, a2=100\n",">1551, d1=0.032, d2=0.016 g=0.171, a1=100, a2=100\n",">1552, d1=0.006, d2=0.012 g=0.210, a1=100, a2=100\n",">1553, d1=0.004, d2=0.011 g=0.291, a1=100, a2=100\n",">1554, d1=0.066, d2=0.006 g=0.053, a1=100, a2=100\n",">1555, d1=0.002, d2=0.004 g=0.040, a1=100, a2=100\n",">1556, d1=0.002, d2=0.015 g=0.056, a1=100, a2=100\n",">1557, d1=0.002, d2=0.026 g=0.150, a1=100, a2=100\n",">1558, d1=0.001, d2=0.005 g=0.327, a1=100, a2=100\n",">1559, d1=0.004, d2=0.011 g=0.472, a1=100, a2=100\n",">1560, d1=0.007, d2=0.002 g=0.515, a1=100, a2=100\n",">1561, d1=0.006, d2=0.088 g=2.629, a1=100, a2=100\n",">1562, d1=0.207, d2=0.000 g=0.222, a1=100, a2=100\n",">1563, d1=0.001, d2=0.000 g=0.053, a1=100, a2=100\n",">1564, d1=0.001, d2=0.000 g=0.029, a1=100, a2=100\n",">1565, d1=0.001, d2=0.000 g=0.024, a1=100, a2=100\n",">1566, d1=0.001, d2=0.000 g=0.021, a1=100, a2=100\n",">1567, d1=0.001, d2=0.000 g=0.019, a1=100, a2=100\n",">1568, d1=0.001, d2=0.000 g=0.020, a1=100, a2=100\n",">1569, d1=0.001, d2=0.001 g=0.019, a1=100, a2=100\n",">1570, d1=0.007, d2=0.009 g=0.018, a1=100, a2=100\n",">1571, d1=0.002, d2=0.004 g=0.022, a1=100, a2=100\n",">1572, d1=0.000, d2=0.003 g=0.025, a1=100, a2=100\n",">1573, d1=0.002, d2=0.002 g=0.027, a1=100, a2=100\n",">1574, d1=0.001, d2=0.002 g=0.030, a1=100, a2=100\n",">1575, d1=0.001, d2=0.025 g=0.058, a1=100, a2=100\n",">1576, d1=0.002, d2=0.006 g=0.108, a1=100, a2=100\n",">1577, d1=0.001, d2=0.008 g=0.160, a1=100, a2=100\n",">1578, d1=0.001, d2=0.014 g=0.248, a1=100, a2=100\n",">1579, d1=0.003, d2=0.015 g=0.410, a1=100, a2=100\n",">1580, d1=0.004, d2=0.002 g=0.523, a1=100, a2=100\n",">1581, d1=0.005, d2=0.002 g=0.561, a1=100, a2=100\n",">1582, d1=0.002, d2=0.006 g=0.723, a1=100, a2=100\n",">1583, d1=0.016, d2=0.017 g=1.125, a1=100, a2=100\n",">1584, d1=0.001, d2=0.000 g=1.022, a1=100, a2=100\n",">1585, d1=0.008, d2=0.001 g=0.581, a1=100, a2=100\n",">1586, d1=0.020, d2=0.011 g=0.388, a1=100, a2=100\n",">1587, d1=0.004, d2=0.020 g=0.500, a1=100, a2=100\n",">1588, d1=0.009, d2=0.005 g=0.589, a1=100, a2=100\n",">1589, d1=0.012, d2=0.028 g=0.904, a1=100, a2=100\n",">1590, d1=0.002, d2=0.007 g=1.418, a1=100, a2=100\n",">1591, d1=0.013, d2=0.005 g=1.459, a1=100, a2=100\n",">1592, d1=0.042, d2=0.004 g=0.755, a1=100, a2=100\n",">1593, d1=0.006, d2=0.048 g=1.281, a1=100, a2=100\n",">1594, d1=0.005, d2=0.000 g=1.840, a1=100, a2=100\n",">1595, d1=0.025, d2=0.002 g=1.096, a1=100, a2=100\n",">1596, d1=0.007, d2=0.013 g=0.902, a1=100, a2=100\n",">1597, d1=0.019, d2=0.177 g=5.688, a1=100, a2=100\n",">1598, d1=0.209, d2=0.000 g=1.303, a1=96, a2=100\n",">1599, d1=0.017, d2=2.217 g=25.099, a1=100, a2=0\n",">1600, d1=11.273, d2=0.000 g=7.398, a1=0, a2=100\n",">1601, d1=0.397, d2=0.000 g=0.110, a1=84, a2=100\n",">1602, d1=0.025, d2=0.011 g=0.012, a1=100, a2=100\n",">1603, d1=0.002, d2=0.028 g=0.011, a1=100, a2=100\n",">1604, d1=0.004, d2=0.029 g=0.025, a1=100, a2=100\n",">1605, d1=0.001, d2=0.001 g=0.038, a1=100, a2=100\n",">1606, d1=0.001, d2=0.012 g=0.047, a1=100, a2=100\n",">1607, d1=0.005, d2=0.014 g=0.066, a1=100, a2=100\n",">1608, d1=0.005, d2=0.003 g=0.077, a1=100, a2=100\n",">1609, d1=0.017, d2=0.004 g=0.058, a1=100, a2=100\n",">1610, d1=0.003, d2=0.002 g=0.053, a1=100, a2=100\n",">1611, d1=0.004, d2=0.001 g=0.047, a1=100, a2=100\n",">1612, d1=0.004, d2=0.013 g=0.052, a1=100, a2=100\n",">1613, d1=0.011, d2=0.002 g=0.048, a1=100, a2=100\n",">1614, d1=0.005, d2=0.003 g=0.049, a1=100, a2=100\n",">1615, d1=0.004, d2=0.003 g=0.044, a1=100, a2=100\n",">1616, d1=0.037, d2=0.004 g=0.024, a1=100, a2=100\n",">1617, d1=0.001, d2=0.005 g=0.023, a1=100, a2=100\n",">1618, d1=0.002, d2=0.003 g=0.024, a1=100, a2=100\n",">1619, d1=0.001, d2=0.004 g=0.026, a1=100, a2=100\n",">1620, d1=0.001, d2=0.013 g=0.036, a1=100, a2=100\n",">1621, d1=0.002, d2=0.004 g=0.044, a1=100, a2=100\n",">1622, d1=0.001, d2=0.004 g=0.048, a1=100, a2=100\n",">1623, d1=0.002, d2=0.022 g=0.074, a1=100, a2=100\n",">1624, d1=0.004, d2=0.003 g=0.089, a1=100, a2=100\n",">1625, d1=0.004, d2=0.001 g=0.091, a1=100, a2=100\n",">1626, d1=0.002, d2=0.001 g=0.084, a1=100, a2=100\n",">1627, d1=0.003, d2=0.001 g=0.071, a1=100, a2=100\n",">1628, d1=0.004, d2=0.006 g=0.074, a1=100, a2=100\n",">1629, d1=0.006, d2=0.005 g=0.069, a1=100, a2=100\n",">1630, d1=0.005, d2=0.000 g=0.063, a1=100, a2=100\n",">1631, d1=0.004, d2=0.008 g=0.065, a1=100, a2=100\n",">1632, d1=0.026, d2=0.031 g=0.069, a1=100, a2=100\n",">1633, d1=0.001, d2=0.011 g=0.102, a1=100, a2=100\n",">1634, d1=0.003, d2=0.005 g=0.110, a1=100, a2=100\n",">1635, d1=0.002, d2=0.008 g=0.129, a1=100, a2=100\n",">1636, d1=0.016, d2=0.028 g=0.142, a1=100, a2=100\n",">1637, d1=0.003, d2=0.003 g=0.175, a1=100, a2=100\n",">1638, d1=0.026, d2=0.023 g=0.148, a1=100, a2=100\n",">1639, d1=0.007, d2=0.028 g=0.222, a1=100, a2=100\n",">1640, d1=0.006, d2=0.029 g=0.373, a1=100, a2=100\n",">1641, d1=0.006, d2=0.097 g=1.306, a1=100, a2=100\n",">1642, d1=0.050, d2=0.001 g=1.941, a1=100, a2=100\n",">1643, d1=0.022, d2=0.042 g=1.729, a1=100, a2=100\n",">1644, d1=0.019, d2=0.034 g=1.909, a1=100, a2=100\n",">1645, d1=0.034, d2=0.115 g=2.739, a1=100, a2=100\n",">1646, d1=0.101, d2=0.065 g=2.232, a1=100, a2=100\n",">1647, d1=0.018, d2=1.668 g=15.016, a1=100, a2=0\n",">1648, d1=5.892, d2=0.005 g=2.717, a1=0, a2=100\n",">1649, d1=0.060, d2=2.405 g=7.360, a1=100, a2=0\n",">1650, d1=1.536, d2=0.105 g=3.326, a1=18, a2=100\n",">1651, d1=0.217, d2=0.327 g=2.595, a1=90, a2=96\n",">1652, d1=0.189, d2=0.735 g=5.504, a1=96, a2=50\n",">1653, d1=0.814, d2=0.147 g=2.495, a1=53, a2=100\n",">1654, d1=0.079, d2=0.296 g=3.150, a1=100, a2=100\n",">1655, d1=0.107, d2=0.023 g=3.008, a1=100, a2=100\n",">1656, d1=0.107, d2=0.178 g=3.146, a1=100, a2=100\n",">1657, d1=0.141, d2=0.049 g=2.669, a1=100, a2=100\n",">1658, d1=0.100, d2=0.014 g=1.742, a1=100, a2=100\n",">1659, d1=0.049, d2=0.055 g=1.377, a1=100, a2=100\n",">1660, d1=0.027, d2=0.007 g=1.205, a1=100, a2=100\n",">1661, d1=0.014, d2=0.017 g=1.086, a1=100, a2=100\n",">1662, d1=0.051, d2=0.002 g=0.705, a1=100, a2=100\n",">1663, d1=0.025, d2=0.005 g=0.523, a1=100, a2=100\n",">1664, d1=0.030, d2=0.042 g=0.523, a1=100, a2=100\n",">1665, d1=0.011, d2=0.008 g=0.500, a1=100, a2=100\n",">1666, d1=0.015, d2=0.001 g=0.441, a1=100, a2=100\n",">1667, d1=0.030, d2=0.002 g=0.329, a1=100, a2=100\n",">1668, d1=0.016, d2=0.002 g=0.269, a1=100, a2=100\n",">1669, d1=0.004, d2=0.014 g=0.231, a1=100, a2=100\n",">1670, d1=0.013, d2=0.010 g=0.234, a1=100, a2=100\n",">1671, d1=0.018, d2=0.010 g=0.229, a1=100, a2=100\n",">1672, d1=0.010, d2=0.043 g=0.307, a1=100, a2=100\n",">1673, d1=0.007, d2=0.003 g=0.333, a1=100, a2=100\n",">1674, d1=0.028, d2=0.005 g=0.280, a1=100, a2=100\n",">1675, d1=0.010, d2=0.005 g=0.233, a1=100, a2=100\n",">1676, d1=0.010, d2=0.007 g=0.226, a1=100, a2=100\n",">1677, d1=0.006, d2=0.003 g=0.207, a1=100, a2=100\n",">1678, d1=0.010, d2=0.022 g=0.215, a1=100, a2=100\n",">1679, d1=0.050, d2=0.020 g=0.155, a1=100, a2=100\n",">1680, d1=0.011, d2=0.011 g=0.144, a1=100, a2=100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N_UU7aUm5TNu","colab_type":"text"},"source":["# WGAN MNIST"]},{"cell_type":"code","metadata":{"id":"bTBHKEBw5Sdb","colab_type":"code","outputId":"32c5a7f4-dabc-4cf8-911a-2646f93fd764","executionInfo":{"status":"ok","timestamp":1588356352403,"user_tz":240,"elapsed":160369,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# example of a wgan for generating handwritten digits\n","from numpy import expand_dims\n","from numpy import mean\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.mnist import load_data\n","from keras import backend\n","from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.initializers import RandomNormal\n","from keras.constraints import Constraint\n","from matplotlib import pyplot\n","\n","# clip model weights to a given hypercube\n","class ClipConstraint(Constraint):\n","\t# set clip value when initialized\n","\tdef __init__(self, clip_value):\n","\t\tself.clip_value = clip_value\n","\n","\t# clip model weights to hypercube\n","\tdef __call__(self, weights):\n","\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n","\n","\t# get the config\n","\tdef get_config(self):\n","\t\treturn {'clip_value': self.clip_value}\n","\n","# calculate wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","\treturn backend.mean(y_true * y_pred)\n","\n","# define the standalone critic model\n","def define_critic(in_shape=(28,28,1)):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# weight constraint\n","\tconst = ClipConstraint(0.01)\n","\t# define model\n","\tmodel = Sequential()\n","\t# downsample to 14x14\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# downsample to 7x7\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# scoring, linear activation\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(1))\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model\n","\n","# define the standalone generator model\n","def define_generator(latent_dim):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# define model\n","\tmodel = Sequential()\n","\t# foundation for 7x7 image\n","\tn_nodes = 128 * 7 * 7\n","\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\tmodel.add(Reshape((7, 7, 128)))\n","\t# upsample to 14x14\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# upsample to 28x28\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# output 28x28x1\n","\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n","\treturn model\n","\n","# define the combined generator and critic model, for updating the generator\n","def define_gan(generator, critic):\n","\t# make weights in the critic not trainable\n","\tcritic.trainable = False\n","\t# connect them\n","\tmodel = Sequential()\n","\t# add generator\n","\tmodel.add(generator)\n","\t# add the critic\n","\tmodel.add(critic)\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model\n","\n","# load images\n","def load_real_samples():\n","\t# load dataset\n","\t(trainX, trainy), (_, _) = load_data()\n","\t# select all of the examples for a given class\n","\t# selected_ix = trainy == 7 \n","\t# selected_ix += trainy == 5\n","\t# X = trainX[selected_ix]\n","\tX = trainX\n","\t# expand to 3d, e.g. add channels\n","\tX = expand_dims(X, axis=-1)\n","\t# convert from ints to floats\n","\tX = X.astype('float32')\n","\t# scale from [0,255] to [-1,1]\n","\tX = (X - 127.5) / 127.5\n","\treturn X\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels, -1 for 'real'\n","\ty = -ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n_samples, latent_dim)\n","\treturn x_input\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels with 1.0 for 'fake'\n","\ty = ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","\t# prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = '/content/gdrive/My Drive/Colab Notebooks/results_baseline/plots/generated_plot_%04d.png' % (step+1)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\t# save the generator model\n","\tfilename2 = '/content/gdrive/My Drive/Colab Notebooks/results_baseline/models/model_%04d.h5' % (step+1)\n","\tg_model.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist):\n","\t# plot history\n","\tpyplot.plot(d1_hist, label='crit_real')\n","\tpyplot.plot(d2_hist, label='crit_fake')\n","\tpyplot.plot(g_hist, label='gen')\n","\tpyplot.legend()\n","\tpyplot.savefig('/content/gdrive/My Drive/Colab Notebooks/results_baseline/plot_line_plot_loss.png')\n","\tpyplot.close()\n","\n","# train the generator and critic\n","def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(dataset.shape[0] / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","\t# calculate the size of half a batch of samples\n","\thalf_batch = int(n_batch / 2)\n","\t# lists for keeping track of loss\n","\tc1_hist, c2_hist, g_hist = list(), list(), list()\n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# update the critic more than the generator\n","\t\tc1_tmp, c2_tmp = list(), list()\n","\t\tfor _ in range(n_critic):\n","\t\t\t# get randomly selected 'real' samples\n","\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t\t\tc1_tmp.append(c_loss1)\n","\t\t\t# generate 'fake' examples\n","\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\t\t\tc2_tmp.append(c_loss2)\n","\t\t# store critic loss\n","\t\tc1_hist.append(mean(c1_tmp))\n","\t\tc2_hist.append(mean(c2_tmp))\n","\t\t# prepare points in latent space as input for the generator\n","\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t\t# create inverted labels for the fake samples\n","\t\ty_gan = -ones((n_batch, 1))\n","\t\t# update the generator via the critic's error\n","\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","\t\tg_hist.append(g_loss)\n","\t\t# summarize loss on this batch\n","\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n","\t\t# evaluate the model performance every 'epoch'\n","\t\tif (i+1) % bat_per_epo == 0:\n","\t\t\tsummarize_performance(i, g_model, latent_dim)\n","\t# line plots of loss\n","\tplot_history(c1_hist, c2_hist, g_hist)\n","\n","\n","#############\n","#############\n","\n","#############\n","\n","#############\n","#############\n","#############\n","\n","from os import makedirs\n","\n","makedirs('/content/gdrive/My Drive/Colab Notebooks/results_baseline/models', exist_ok=True)\n","makedirs('/content/gdrive/My Drive/Colab Notebooks/results_baseline/plots', exist_ok=True)\n","\n","# size of the latent space\n","latent_dim = 50\n","# create the critic\n","critic = define_critic()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, critic)\n","# load image data\n","dataset = load_real_samples()\n","print(dataset.shape)\n","# train model\n","train(generator, critic, gan_model, dataset, latent_dim, n_epochs=1)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["(60000, 28, 28, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":[">1, c1=-1.718, c2=0.005 g=0.025\n",">2, c1=-5.184, c2=0.094 g=-1.438\n",">3, c1=-7.624, c2=0.169 g=-2.715\n",">4, c1=-9.614, c2=0.261 g=-4.388\n",">5, c1=-11.281, c2=0.356 g=-5.908\n",">6, c1=-12.823, c2=0.422 g=-6.878\n",">7, c1=-14.082, c2=0.491 g=-8.075\n",">8, c1=-15.508, c2=0.575 g=-9.145\n",">9, c1=-16.231, c2=0.636 g=-10.047\n",">10, c1=-17.481, c2=0.681 g=-11.073\n",">11, c1=-18.477, c2=0.761 g=-11.931\n",">12, c1=-19.053, c2=0.818 g=-12.843\n",">13, c1=-20.303, c2=0.927 g=-13.803\n",">14, c1=-20.850, c2=1.035 g=-14.831\n",">15, c1=-21.833, c2=1.121 g=-16.081\n",">16, c1=-22.151, c2=1.256 g=-16.652\n",">17, c1=-22.915, c2=1.405 g=-17.536\n",">18, c1=-23.789, c2=1.524 g=-18.504\n",">19, c1=-24.152, c2=1.673 g=-19.227\n",">20, c1=-24.560, c2=1.775 g=-19.946\n",">21, c1=-25.372, c2=1.829 g=-20.137\n",">22, c1=-25.807, c2=1.881 g=-20.353\n",">23, c1=-25.798, c2=1.856 g=-21.082\n",">24, c1=-26.927, c2=1.859 g=-21.469\n",">25, c1=-26.976, c2=1.771 g=-21.995\n",">26, c1=-27.470, c2=1.702 g=-22.421\n",">27, c1=-28.122, c2=1.556 g=-22.219\n",">28, c1=-28.400, c2=1.339 g=-22.832\n",">29, c1=-29.282, c2=1.050 g=-23.265\n",">30, c1=-29.245, c2=0.740 g=-23.961\n",">31, c1=-29.848, c2=0.423 g=-24.489\n",">32, c1=-30.155, c2=0.071 g=-25.056\n",">33, c1=-30.466, c2=-0.517 g=-25.721\n",">34, c1=-31.104, c2=-1.234 g=-26.761\n",">35, c1=-31.391, c2=-1.959 g=-27.920\n",">36, c1=-31.790, c2=-3.097 g=-28.181\n",">37, c1=-31.908, c2=-4.124 g=-28.249\n",">38, c1=-32.895, c2=-5.123 g=-28.923\n",">39, c1=-33.278, c2=-6.520 g=-29.958\n",">40, c1=-33.976, c2=-7.991 g=-30.557\n",">41, c1=-34.311, c2=-8.991 g=-30.794\n",">42, c1=-34.620, c2=-10.810 g=-31.175\n",">43, c1=-35.244, c2=-12.415 g=-32.040\n",">44, c1=-35.533, c2=-14.079 g=-32.304\n",">45, c1=-36.060, c2=-15.651 g=-32.001\n",">46, c1=-36.677, c2=-17.485 g=-32.465\n",">47, c1=-37.066, c2=-19.103 g=-33.168\n",">48, c1=-38.133, c2=-20.541 g=-33.792\n",">49, c1=-38.115, c2=-22.053 g=-34.736\n",">50, c1=-39.029, c2=-23.502 g=-35.117\n",">51, c1=-39.619, c2=-24.926 g=-36.122\n",">52, c1=-40.260, c2=-26.530 g=-36.650\n",">53, c1=-40.547, c2=-27.733 g=-37.739\n",">54, c1=-41.385, c2=-29.102 g=-38.127\n",">55, c1=-41.966, c2=-30.463 g=-39.123\n",">56, c1=-42.328, c2=-31.549 g=-40.170\n",">57, c1=-42.943, c2=-32.838 g=-40.926\n",">58, c1=-43.757, c2=-33.940 g=-41.805\n",">59, c1=-44.923, c2=-35.142 g=-43.022\n",">60, c1=-45.512, c2=-36.153 g=-43.539\n",">61, c1=-45.831, c2=-37.270 g=-44.463\n",">62, c1=-46.355, c2=-38.352 g=-45.008\n",">63, c1=-47.297, c2=-39.529 g=-46.208\n",">64, c1=-47.939, c2=-40.444 g=-47.117\n",">65, c1=-48.408, c2=-41.346 g=-47.607\n",">66, c1=-48.795, c2=-42.285 g=-48.309\n",">67, c1=-49.826, c2=-43.173 g=-49.460\n",">68, c1=-49.979, c2=-43.983 g=-50.231\n",">69, c1=-51.608, c2=-45.100 g=-51.217\n",">70, c1=-52.379, c2=-45.900 g=-51.495\n",">71, c1=-53.120, c2=-46.873 g=-52.171\n",">72, c1=-53.268, c2=-47.588 g=-53.027\n",">73, c1=-53.908, c2=-48.625 g=-54.133\n",">74, c1=-54.951, c2=-49.215 g=-54.891\n",">75, c1=-55.697, c2=-50.252 g=-55.339\n",">76, c1=-56.362, c2=-50.955 g=-56.074\n",">77, c1=-56.605, c2=-51.696 g=-57.173\n",">78, c1=-57.626, c2=-52.572 g=-57.866\n",">79, c1=-58.205, c2=-53.282 g=-57.953\n",">80, c1=-58.682, c2=-54.334 g=-58.877\n",">81, c1=-59.663, c2=-55.013 g=-59.648\n",">82, c1=-60.579, c2=-55.928 g=-60.124\n",">83, c1=-61.093, c2=-56.603 g=-60.948\n",">84, c1=-61.880, c2=-57.507 g=-61.574\n",">85, c1=-62.701, c2=-58.160 g=-62.502\n",">86, c1=-63.588, c2=-59.105 g=-63.144\n",">87, c1=-63.979, c2=-59.718 g=-63.857\n",">88, c1=-64.492, c2=-60.673 g=-64.332\n",">89, c1=-65.573, c2=-61.345 g=-65.346\n",">90, c1=-65.946, c2=-62.249 g=-65.964\n",">91, c1=-66.626, c2=-62.825 g=-66.405\n",">92, c1=-67.822, c2=-63.861 g=-67.343\n",">93, c1=-68.675, c2=-64.460 g=-67.931\n",">94, c1=-68.405, c2=-65.241 g=-68.199\n",">95, c1=-69.583, c2=-66.006 g=-69.275\n",">96, c1=-70.832, c2=-66.827 g=-69.907\n",">97, c1=-71.486, c2=-67.586 g=-70.910\n",">98, c1=-71.718, c2=-68.336 g=-71.508\n",">99, c1=-72.742, c2=-69.198 g=-72.007\n",">100, c1=-73.372, c2=-69.970 g=-72.876\n",">101, c1=-73.828, c2=-70.732 g=-73.047\n",">102, c1=-74.829, c2=-71.554 g=-74.170\n",">103, c1=-75.602, c2=-72.256 g=-74.897\n",">104, c1=-76.388, c2=-73.114 g=-75.975\n",">105, c1=-76.934, c2=-73.778 g=-76.884\n",">106, c1=-77.560, c2=-74.617 g=-77.485\n",">107, c1=-78.479, c2=-75.283 g=-77.807\n",">108, c1=-79.799, c2=-76.224 g=-79.111\n",">109, c1=-79.467, c2=-76.715 g=-79.609\n",">110, c1=-80.359, c2=-77.662 g=-80.337\n",">111, c1=-81.517, c2=-78.226 g=-81.055\n",">112, c1=-82.745, c2=-79.302 g=-82.205\n",">113, c1=-82.148, c2=-79.721 g=-81.735\n",">114, c1=-83.659, c2=-80.897 g=-83.200\n",">115, c1=-84.367, c2=-81.515 g=-83.822\n",">116, c1=-85.392, c2=-82.453 g=-85.051\n",">117, c1=-85.970, c2=-83.066 g=-85.862\n",">118, c1=-86.417, c2=-83.989 g=-86.423\n",">119, c1=-87.367, c2=-84.643 g=-86.984\n",">120, c1=-88.388, c2=-85.532 g=-88.131\n",">121, c1=-88.967, c2=-86.138 g=-88.762\n",">122, c1=-89.770, c2=-87.117 g=-89.111\n",">123, c1=-90.066, c2=-87.861 g=-89.919\n",">124, c1=-90.910, c2=-88.722 g=-90.354\n",">125, c1=-91.776, c2=-89.518 g=-91.114\n",">126, c1=-91.590, c2=-90.194 g=-92.037\n",">127, c1=-93.457, c2=-90.903 g=-92.725\n",">128, c1=-94.116, c2=-91.886 g=-93.352\n",">129, c1=-95.027, c2=-92.459 g=-93.909\n",">130, c1=-95.568, c2=-93.569 g=-95.081\n",">131, c1=-95.636, c2=-93.936 g=-94.505\n",">132, c1=-97.158, c2=-95.137 g=-95.912\n",">133, c1=-97.661, c2=-95.708 g=-96.648\n",">134, c1=-98.306, c2=-96.545 g=-96.914\n",">135, c1=-99.259, c2=-97.353 g=-97.928\n",">136, c1=-100.053, c2=-98.128 g=-98.420\n",">137, c1=-100.705, c2=-98.924 g=-98.930\n",">138, c1=-101.584, c2=-99.708 g=-100.138\n",">139, c1=-102.291, c2=-100.433 g=-100.746\n",">140, c1=-103.332, c2=-101.329 g=-101.391\n",">141, c1=-103.891, c2=-102.081 g=-101.850\n",">142, c1=-104.374, c2=-102.887 g=-102.164\n",">143, c1=-105.677, c2=-103.886 g=-103.378\n",">144, c1=-105.718, c2=-104.256 g=-103.646\n",">145, c1=-107.053, c2=-105.256 g=-104.688\n",">146, c1=-107.953, c2=-105.970 g=-104.967\n",">147, c1=-108.369, c2=-106.853 g=-106.010\n",">148, c1=-109.014, c2=-107.467 g=-105.709\n",">149, c1=-109.982, c2=-108.406 g=-106.525\n",">150, c1=-110.266, c2=-109.218 g=-106.894\n",">151, c1=-111.697, c2=-110.176 g=-108.082\n",">152, c1=-112.401, c2=-110.694 g=-107.894\n",">153, c1=-112.481, c2=-111.658 g=-108.983\n",">154, c1=-113.689, c2=-112.402 g=-109.593\n",">155, c1=-114.587, c2=-113.166 g=-109.795\n",">156, c1=-114.708, c2=-113.922 g=-110.623\n",">157, c1=-116.165, c2=-114.791 g=-111.036\n",">158, c1=-115.904, c2=-115.538 g=-111.479\n",">159, c1=-117.858, c2=-116.149 g=-111.638\n",">160, c1=-118.046, c2=-117.130 g=-112.100\n",">161, c1=-119.004, c2=-117.850 g=-112.443\n",">162, c1=-120.487, c2=-118.689 g=-113.014\n",">163, c1=-120.777, c2=-119.420 g=-112.816\n",">164, c1=-121.436, c2=-120.504 g=-114.305\n",">165, c1=-122.279, c2=-120.851 g=-113.488\n",">166, c1=-122.337, c2=-122.021 g=-114.859\n",">167, c1=-122.770, c2=-122.373 g=-114.271\n",">168, c1=-124.740, c2=-123.422 g=-114.972\n",">169, c1=-125.166, c2=-123.888 g=-114.708\n",">170, c1=-126.454, c2=-124.724 g=-116.049\n",">171, c1=-125.323, c2=-125.171 g=-116.609\n",">172, c1=-126.851, c2=-125.897 g=-117.050\n",">173, c1=-128.274, c2=-126.629 g=-118.014\n",">174, c1=-128.586, c2=-127.734 g=-117.055\n",">175, c1=-128.633, c2=-127.828 g=-118.655\n",">176, c1=-130.477, c2=-128.511 g=-119.500\n",">177, c1=-130.215, c2=-128.222 g=-123.651\n",">178, c1=-131.693, c2=-128.939 g=-124.570\n",">179, c1=-131.919, c2=-130.146 g=-124.641\n",">180, c1=-132.609, c2=-130.641 g=-125.455\n",">181, c1=-133.779, c2=-131.430 g=-124.983\n",">182, c1=-133.975, c2=-132.165 g=-124.361\n",">183, c1=-134.962, c2=-132.598 g=-124.812\n",">184, c1=-135.592, c2=-133.914 g=-126.107\n",">185, c1=-135.170, c2=-133.986 g=-124.538\n",">186, c1=-135.775, c2=-134.259 g=-124.281\n",">187, c1=-137.060, c2=-134.876 g=-124.420\n",">188, c1=-137.739, c2=-135.093 g=-124.133\n",">189, c1=-136.278, c2=-136.055 g=-123.492\n",">190, c1=-138.375, c2=-135.711 g=-121.263\n",">191, c1=-140.493, c2=-136.390 g=-121.641\n",">192, c1=-140.367, c2=-136.938 g=-120.150\n",">193, c1=-139.534, c2=-137.884 g=-119.813\n",">194, c1=-140.862, c2=-136.885 g=-116.538\n",">195, c1=-142.547, c2=-139.595 g=-114.762\n",">196, c1=-141.831, c2=-137.994 g=-109.852\n",">197, c1=-142.811, c2=-139.493 g=-107.716\n",">198, c1=-143.102, c2=-139.292 g=-103.355\n",">199, c1=-143.766, c2=-140.358 g=-99.949\n",">200, c1=-145.079, c2=-139.490 g=-99.746\n",">201, c1=-146.148, c2=-138.658 g=-110.259\n",">202, c1=-144.978, c2=-139.986 g=-101.773\n",">203, c1=-146.426, c2=-140.929 g=-98.120\n",">204, c1=-146.446, c2=-143.056 g=-88.834\n",">205, c1=-148.105, c2=-143.575 g=-81.696\n",">206, c1=-149.267, c2=-144.907 g=-76.059\n",">207, c1=-148.475, c2=-145.669 g=-62.759\n",">208, c1=-150.667, c2=-145.878 g=-57.711\n",">209, c1=-150.037, c2=-146.833 g=-43.402\n",">210, c1=-151.932, c2=-147.248 g=-27.316\n",">211, c1=-152.648, c2=-147.900 g=-20.193\n",">212, c1=-152.147, c2=-146.540 g=-20.101\n",">213, c1=-150.057, c2=-143.884 g=-4.681\n",">214, c1=-154.029, c2=-147.756 g=12.324\n",">215, c1=-154.170, c2=-150.597 g=31.301\n",">216, c1=-155.293, c2=-153.293 g=38.131\n",">217, c1=-156.087, c2=-154.842 g=47.871\n",">218, c1=-158.120, c2=-156.212 g=55.719\n",">219, c1=-160.163, c2=-157.713 g=66.276\n",">220, c1=-160.827, c2=-159.457 g=76.297\n",">221, c1=-161.319, c2=-160.687 g=84.927\n",">222, c1=-161.890, c2=-161.426 g=94.079\n",">223, c1=-164.330, c2=-163.938 g=100.914\n",">224, c1=-164.723, c2=-165.094 g=108.426\n",">225, c1=-166.546, c2=-166.355 g=109.705\n",">226, c1=-167.927, c2=-167.692 g=99.752\n",">227, c1=-168.336, c2=-168.601 g=106.356\n",">228, c1=-168.498, c2=-170.723 g=109.404\n",">229, c1=-171.424, c2=-171.781 g=112.076\n",">230, c1=-171.417, c2=-173.131 g=115.976\n",">231, c1=-173.319, c2=-174.595 g=118.198\n",">232, c1=-174.030, c2=-176.037 g=123.095\n",">233, c1=-176.147, c2=-177.289 g=126.828\n",">234, c1=-175.329, c2=-178.190 g=129.060\n",">235, c1=-176.908, c2=-179.066 g=133.493\n",">236, c1=-178.460, c2=-181.046 g=137.503\n",">237, c1=-179.176, c2=-181.988 g=140.292\n",">238, c1=-180.307, c2=-182.889 g=143.933\n",">239, c1=-181.804, c2=-184.304 g=147.999\n",">240, c1=-182.892, c2=-185.470 g=149.861\n",">241, c1=-183.289, c2=-185.951 g=150.949\n",">242, c1=-185.042, c2=-187.091 g=151.303\n",">243, c1=-186.550, c2=-187.791 g=149.320\n",">244, c1=-186.724, c2=-188.660 g=146.501\n",">245, c1=-187.506, c2=-189.047 g=137.587\n",">246, c1=-188.924, c2=-189.868 g=129.850\n",">247, c1=-189.245, c2=-191.238 g=132.628\n",">248, c1=-189.612, c2=-192.308 g=133.632\n",">249, c1=-190.782, c2=-193.637 g=135.297\n",">250, c1=-192.489, c2=-195.122 g=134.762\n",">251, c1=-193.828, c2=-196.442 g=132.267\n",">252, c1=-193.995, c2=-197.439 g=130.178\n",">253, c1=-194.868, c2=-198.640 g=130.463\n",">254, c1=-196.914, c2=-199.347 g=131.997\n",">255, c1=-197.768, c2=-200.429 g=137.909\n",">256, c1=-198.562, c2=-202.177 g=138.003\n",">257, c1=-198.875, c2=-202.584 g=143.802\n",">258, c1=-200.587, c2=-203.808 g=143.848\n",">259, c1=-201.701, c2=-204.884 g=143.811\n",">260, c1=-202.873, c2=-205.207 g=122.951\n",">261, c1=-203.275, c2=-204.076 g=104.868\n",">262, c1=-203.818, c2=-205.805 g=118.293\n",">263, c1=-205.289, c2=-207.609 g=115.836\n",">264, c1=-206.499, c2=-208.651 g=115.156\n",">265, c1=-206.983, c2=-209.345 g=126.900\n",">266, c1=-208.216, c2=-210.391 g=142.402\n",">267, c1=-209.439, c2=-211.743 g=146.682\n",">268, c1=-210.498, c2=-212.551 g=154.770\n",">269, c1=-211.524, c2=-212.339 g=103.534\n",">270, c1=-212.054, c2=-210.209 g=92.985\n",">271, c1=-212.005, c2=-212.668 g=110.749\n",">272, c1=-213.374, c2=-214.779 g=119.575\n",">273, c1=-214.865, c2=-216.346 g=134.629\n",">274, c1=-215.888, c2=-217.345 g=144.236\n",">275, c1=-216.809, c2=-218.918 g=159.943\n",">276, c1=-218.174, c2=-220.209 g=166.412\n",">277, c1=-218.952, c2=-221.577 g=171.111\n",">278, c1=-220.990, c2=-222.493 g=180.870\n",">279, c1=-221.512, c2=-223.950 g=185.697\n",">280, c1=-222.409, c2=-224.956 g=189.769\n",">281, c1=-224.800, c2=-226.919 g=194.588\n",">282, c1=-225.129, c2=-227.658 g=194.407\n",">283, c1=-225.957, c2=-228.683 g=198.036\n",">284, c1=-227.971, c2=-229.831 g=193.987\n",">285, c1=-228.398, c2=-230.698 g=193.254\n",">286, c1=-229.491, c2=-231.502 g=195.917\n",">287, c1=-230.641, c2=-232.827 g=196.551\n",">288, c1=-231.193, c2=-233.743 g=200.588\n",">289, c1=-232.229, c2=-234.772 g=202.005\n",">290, c1=-234.218, c2=-235.705 g=206.357\n",">291, c1=-235.091, c2=-236.735 g=209.429\n",">292, c1=-236.776, c2=-238.300 g=213.877\n",">293, c1=-237.596, c2=-239.381 g=216.737\n",">294, c1=-238.668, c2=-240.840 g=215.780\n",">295, c1=-238.967, c2=-241.507 g=220.550\n",">296, c1=-240.693, c2=-242.801 g=223.439\n",">297, c1=-241.419, c2=-244.017 g=223.360\n",">298, c1=-243.275, c2=-244.980 g=224.422\n",">299, c1=-244.339, c2=-245.900 g=223.294\n",">300, c1=-245.389, c2=-246.462 g=220.558\n",">301, c1=-245.852, c2=-247.625 g=223.285\n",">302, c1=-247.892, c2=-248.847 g=226.364\n",">303, c1=-248.538, c2=-249.978 g=228.313\n",">304, c1=-249.946, c2=-250.906 g=230.192\n",">305, c1=-250.351, c2=-252.390 g=232.513\n",">306, c1=-251.953, c2=-253.368 g=233.633\n",">307, c1=-252.345, c2=-254.070 g=233.666\n",">308, c1=-253.861, c2=-255.459 g=235.785\n",">309, c1=-255.193, c2=-256.630 g=235.880\n",">310, c1=-256.038, c2=-257.737 g=239.055\n",">311, c1=-257.392, c2=-258.708 g=240.677\n",">312, c1=-258.076, c2=-259.757 g=241.881\n",">313, c1=-258.912, c2=-261.231 g=243.751\n",">314, c1=-260.360, c2=-262.307 g=244.219\n",">315, c1=-261.868, c2=-263.362 g=246.558\n",">316, c1=-263.104, c2=-264.424 g=246.167\n",">317, c1=-263.964, c2=-265.557 g=247.690\n",">318, c1=-264.939, c2=-266.523 g=248.283\n",">319, c1=-266.408, c2=-267.539 g=249.526\n",">320, c1=-267.560, c2=-268.624 g=251.923\n",">321, c1=-268.364, c2=-269.636 g=254.768\n",">322, c1=-269.803, c2=-270.767 g=256.733\n",">323, c1=-270.684, c2=-271.841 g=257.317\n",">324, c1=-271.644, c2=-272.784 g=258.638\n",">325, c1=-272.741, c2=-274.008 g=259.846\n",">326, c1=-274.073, c2=-274.860 g=260.461\n",">327, c1=-275.197, c2=-276.121 g=261.572\n",">328, c1=-276.160, c2=-277.060 g=262.694\n",">329, c1=-277.038, c2=-277.962 g=264.371\n",">330, c1=-278.271, c2=-279.120 g=265.419\n",">331, c1=-279.676, c2=-280.445 g=267.339\n",">332, c1=-280.954, c2=-281.436 g=268.427\n",">333, c1=-281.590, c2=-282.622 g=270.129\n",">334, c1=-282.965, c2=-283.301 g=271.446\n",">335, c1=-283.801, c2=-284.656 g=273.023\n",">336, c1=-284.981, c2=-285.450 g=274.272\n",">337, c1=-286.114, c2=-286.578 g=275.222\n",">338, c1=-286.738, c2=-287.633 g=276.702\n",">339, c1=-288.537, c2=-288.746 g=276.881\n",">340, c1=-289.397, c2=-289.734 g=278.662\n",">341, c1=-290.082, c2=-290.935 g=279.233\n",">342, c1=-291.546, c2=-291.822 g=280.007\n",">343, c1=-292.400, c2=-292.818 g=281.458\n",">344, c1=-293.103, c2=-294.125 g=282.623\n",">345, c1=-295.084, c2=-295.024 g=283.902\n",">346, c1=-296.000, c2=-296.208 g=285.154\n",">347, c1=-296.665, c2=-297.234 g=286.330\n",">348, c1=-298.014, c2=-298.049 g=287.637\n",">349, c1=-298.883, c2=-299.261 g=288.813\n",">350, c1=-300.091, c2=-300.129 g=289.782\n",">351, c1=-301.082, c2=-301.226 g=291.103\n",">352, c1=-301.969, c2=-301.831 g=291.622\n",">353, c1=-303.864, c2=-302.916 g=292.043\n",">354, c1=-304.746, c2=-303.712 g=292.428\n",">355, c1=-305.631, c2=-304.646 g=293.723\n",">356, c1=-306.913, c2=-305.342 g=294.807\n",">357, c1=-307.922, c2=-306.425 g=295.890\n",">358, c1=-309.059, c2=-307.016 g=295.904\n",">359, c1=-309.818, c2=-307.845 g=297.013\n",">360, c1=-311.066, c2=-308.544 g=296.652\n",">361, c1=-312.244, c2=-309.159 g=297.865\n",">362, c1=-312.835, c2=-310.786 g=298.584\n",">363, c1=-313.974, c2=-311.589 g=298.769\n",">364, c1=-314.684, c2=-312.622 g=300.242\n",">365, c1=-315.973, c2=-313.769 g=301.433\n",">366, c1=-317.221, c2=-314.924 g=302.090\n",">367, c1=-318.592, c2=-316.225 g=303.291\n",">368, c1=-319.428, c2=-316.903 g=305.037\n",">369, c1=-320.382, c2=-318.286 g=306.569\n",">370, c1=-321.918, c2=-319.634 g=307.759\n",">371, c1=-322.721, c2=-320.479 g=308.411\n",">372, c1=-323.781, c2=-321.555 g=310.470\n",">373, c1=-325.373, c2=-322.629 g=310.694\n",">374, c1=-326.068, c2=-323.817 g=312.184\n",">375, c1=-327.182, c2=-325.012 g=313.756\n",">376, c1=-328.600, c2=-326.551 g=314.832\n",">377, c1=-329.773, c2=-327.346 g=316.488\n",">378, c1=-330.882, c2=-328.190 g=317.416\n",">379, c1=-332.279, c2=-329.478 g=318.964\n",">380, c1=-333.501, c2=-330.577 g=320.340\n",">381, c1=-334.075, c2=-331.531 g=320.697\n",">382, c1=-335.591, c2=-332.678 g=322.379\n",">383, c1=-336.583, c2=-333.794 g=323.660\n",">384, c1=-337.927, c2=-334.704 g=324.962\n",">385, c1=-339.320, c2=-336.006 g=326.488\n",">386, c1=-340.503, c2=-337.142 g=327.916\n",">387, c1=-341.522, c2=-338.011 g=329.060\n",">388, c1=-342.835, c2=-338.896 g=330.315\n",">389, c1=-343.317, c2=-340.089 g=331.373\n",">390, c1=-345.159, c2=-340.951 g=332.375\n",">391, c1=-346.771, c2=-341.860 g=333.538\n",">392, c1=-346.828, c2=-342.961 g=334.630\n",">393, c1=-348.712, c2=-344.069 g=335.684\n",">394, c1=-349.983, c2=-345.069 g=337.048\n",">395, c1=-351.115, c2=-346.131 g=338.236\n",">396, c1=-352.297, c2=-347.267 g=339.219\n",">397, c1=-353.563, c2=-348.184 g=340.390\n",">398, c1=-354.474, c2=-349.255 g=341.663\n",">399, c1=-355.996, c2=-350.159 g=342.786\n",">400, c1=-357.114, c2=-351.293 g=343.987\n",">401, c1=-357.913, c2=-352.343 g=344.937\n",">402, c1=-358.912, c2=-353.335 g=346.023\n",">403, c1=-360.599, c2=-354.331 g=347.031\n",">404, c1=-361.910, c2=-355.106 g=348.370\n",">405, c1=-362.622, c2=-356.119 g=349.428\n",">406, c1=-364.038, c2=-357.063 g=350.599\n",">407, c1=-365.268, c2=-358.096 g=351.891\n",">408, c1=-366.272, c2=-358.959 g=353.177\n",">409, c1=-367.555, c2=-359.924 g=354.051\n",">410, c1=-368.831, c2=-360.957 g=354.953\n",">411, c1=-369.824, c2=-361.531 g=351.794\n",">412, c1=-371.027, c2=-357.471 g=342.820\n",">413, c1=-372.115, c2=-357.013 g=346.721\n",">414, c1=-372.873, c2=-358.530 g=349.856\n",">415, c1=-373.897, c2=-361.212 g=352.586\n",">416, c1=-375.189, c2=-363.146 g=355.212\n",">417, c1=-376.358, c2=-364.778 g=356.458\n",">418, c1=-377.186, c2=-365.781 g=355.250\n",">419, c1=-378.510, c2=-362.370 g=347.726\n",">420, c1=-379.182, c2=-353.971 g=341.480\n",">421, c1=-380.245, c2=-357.909 g=346.773\n",">422, c1=-381.250, c2=-361.245 g=351.771\n",">423, c1=-382.414, c2=-365.880 g=356.562\n",">424, c1=-383.375, c2=-368.907 g=360.617\n",">425, c1=-385.052, c2=-370.791 g=363.301\n",">426, c1=-385.500, c2=-372.818 g=366.024\n",">427, c1=-387.416, c2=-374.411 g=367.884\n",">428, c1=-388.698, c2=-376.044 g=369.719\n",">429, c1=-389.654, c2=-377.301 g=371.294\n",">430, c1=-391.396, c2=-378.953 g=373.014\n",">431, c1=-392.215, c2=-380.094 g=374.566\n",">432, c1=-393.826, c2=-381.294 g=375.760\n",">433, c1=-395.027, c2=-382.362 g=377.251\n",">434, c1=-396.048, c2=-383.432 g=378.693\n",">435, c1=-397.270, c2=-384.699 g=380.062\n",">436, c1=-398.548, c2=-385.711 g=381.910\n",">437, c1=-399.980, c2=-386.600 g=382.773\n",">438, c1=-401.089, c2=-388.200 g=384.256\n",">439, c1=-402.021, c2=-389.257 g=385.488\n",">440, c1=-403.424, c2=-390.192 g=386.777\n",">441, c1=-404.386, c2=-390.795 g=387.995\n",">442, c1=-405.578, c2=-392.419 g=388.970\n",">443, c1=-407.210, c2=-392.737 g=390.347\n",">444, c1=-408.044, c2=-393.988 g=391.398\n",">445, c1=-409.487, c2=-394.967 g=392.381\n",">446, c1=-410.310, c2=-394.526 g=392.663\n",">447, c1=-411.907, c2=-394.454 g=394.092\n",">448, c1=-412.479, c2=-395.107 g=394.505\n",">449, c1=-413.755, c2=-395.581 g=395.099\n",">450, c1=-414.700, c2=-394.352 g=396.626\n",">451, c1=-415.644, c2=-396.027 g=396.935\n",">452, c1=-416.973, c2=-395.929 g=397.948\n",">453, c1=-418.256, c2=-395.628 g=397.144\n",">454, c1=-418.565, c2=-396.734 g=397.393\n",">455, c1=-419.716, c2=-396.035 g=396.776\n",">456, c1=-420.763, c2=-395.112 g=397.058\n",">457, c1=-421.600, c2=-398.368 g=397.833\n",">458, c1=-421.840, c2=-396.047 g=397.844\n",">459, c1=-423.516, c2=-397.101 g=400.458\n",">460, c1=-424.714, c2=-397.996 g=402.975\n",">461, c1=-425.939, c2=-395.999 g=399.960\n",">462, c1=-425.317, c2=-392.206 g=401.873\n",">463, c1=-426.007, c2=-386.981 g=401.188\n",">464, c1=-426.112, c2=-375.617 g=394.112\n",">465, c1=-424.485, c2=-378.095 g=395.994\n",">466, c1=-426.165, c2=-384.320 g=398.752\n",">467, c1=-425.455, c2=-384.385 g=395.955\n",">468, c1=-425.201, c2=-383.429 g=396.760\n",">469, c1=-426.454, c2=-392.101 g=401.054\n",">470, c1=-427.376, c2=-386.173 g=395.325\n",">471, c1=-428.890, c2=-394.031 g=401.980\n",">472, c1=-431.051, c2=-400.434 g=403.400\n",">473, c1=-431.295, c2=-397.097 g=396.979\n",">474, c1=-432.409, c2=-399.289 g=406.604\n",">475, c1=-433.287, c2=-403.703 g=404.179\n",">476, c1=-434.458, c2=-397.571 g=411.692\n",">477, c1=-434.304, c2=-399.091 g=414.564\n",">478, c1=-436.272, c2=-402.744 g=404.463\n",">479, c1=-436.695, c2=-398.235 g=410.792\n",">480, c1=-436.806, c2=-404.888 g=412.305\n",">481, c1=-439.119, c2=-405.606 g=416.549\n",">482, c1=-438.862, c2=-410.294 g=418.419\n",">483, c1=-440.814, c2=-409.001 g=419.625\n",">484, c1=-441.450, c2=-406.182 g=415.574\n",">485, c1=-443.971, c2=-411.074 g=420.661\n",">486, c1=-445.479, c2=-411.789 g=418.974\n",">487, c1=-446.683, c2=-420.303 g=423.771\n",">488, c1=-449.741, c2=-421.361 g=426.069\n",">489, c1=-450.195, c2=-421.078 g=427.527\n",">490, c1=-452.017, c2=-423.252 g=424.937\n",">491, c1=-454.309, c2=-424.795 g=430.262\n",">492, c1=-454.977, c2=-426.581 g=430.964\n",">493, c1=-457.157, c2=-430.822 g=433.968\n",">494, c1=-456.032, c2=-432.589 g=434.675\n",">495, c1=-459.406, c2=-432.611 g=435.921\n",">496, c1=-461.760, c2=-434.227 g=436.661\n",">497, c1=-462.838, c2=-437.628 g=437.907\n",">498, c1=-463.742, c2=-438.477 g=438.135\n",">499, c1=-464.840, c2=-439.537 g=442.214\n",">500, c1=-467.098, c2=-441.468 g=442.324\n",">501, c1=-469.052, c2=-444.116 g=445.828\n",">502, c1=-470.460, c2=-445.455 g=445.425\n",">503, c1=-471.121, c2=-448.166 g=446.937\n",">504, c1=-471.903, c2=-445.683 g=445.924\n",">505, c1=-474.144, c2=-447.922 g=447.624\n",">506, c1=-476.332, c2=-450.326 g=449.283\n",">507, c1=-477.440, c2=-452.706 g=450.512\n",">508, c1=-479.154, c2=-453.335 g=451.670\n",">509, c1=-480.182, c2=-454.819 g=453.953\n",">510, c1=-481.828, c2=-456.962 g=453.739\n",">511, c1=-482.927, c2=-459.346 g=454.461\n",">512, c1=-484.710, c2=-461.034 g=457.216\n",">513, c1=-486.199, c2=-462.528 g=456.736\n",">514, c1=-487.719, c2=-462.506 g=457.858\n",">515, c1=-489.372, c2=-465.226 g=459.356\n",">516, c1=-490.430, c2=-467.948 g=461.033\n",">517, c1=-491.999, c2=-468.758 g=463.193\n",">518, c1=-493.866, c2=-470.937 g=464.315\n",">519, c1=-494.325, c2=-473.013 g=467.385\n",">520, c1=-497.377, c2=-474.289 g=466.712\n",">521, c1=-497.771, c2=-476.067 g=470.089\n",">522, c1=-500.227, c2=-477.557 g=471.997\n",">523, c1=-500.936, c2=-479.318 g=473.634\n",">524, c1=-502.634, c2=-481.348 g=475.236\n",">525, c1=-503.952, c2=-482.386 g=478.604\n",">526, c1=-504.637, c2=-483.820 g=479.762\n",">527, c1=-507.026, c2=-484.864 g=479.940\n",">528, c1=-507.982, c2=-486.086 g=482.832\n",">529, c1=-509.259, c2=-487.036 g=482.477\n",">530, c1=-511.248, c2=-488.800 g=482.691\n",">531, c1=-512.359, c2=-490.013 g=486.967\n",">532, c1=-513.720, c2=-490.661 g=488.604\n",">533, c1=-515.370, c2=-492.837 g=489.832\n",">534, c1=-516.624, c2=-494.204 g=492.091\n",">535, c1=-517.954, c2=-495.922 g=492.566\n",">536, c1=-519.250, c2=-496.918 g=494.357\n",">537, c1=-520.639, c2=-498.779 g=497.608\n",">538, c1=-522.334, c2=-499.881 g=498.523\n",">539, c1=-523.667, c2=-501.049 g=500.156\n",">540, c1=-525.137, c2=-502.590 g=501.847\n",">541, c1=-526.368, c2=-503.832 g=503.005\n",">542, c1=-527.791, c2=-504.797 g=502.476\n",">543, c1=-528.873, c2=-505.266 g=506.293\n",">544, c1=-530.241, c2=-506.938 g=506.578\n",">545, c1=-532.188, c2=-507.733 g=508.580\n",">546, c1=-533.396, c2=-509.182 g=509.628\n",">547, c1=-534.310, c2=-509.802 g=508.958\n",">548, c1=-535.896, c2=-509.541 g=512.314\n",">549, c1=-536.635, c2=-510.726 g=512.607\n",">550, c1=-538.369, c2=-511.680 g=514.478\n",">551, c1=-539.222, c2=-511.577 g=513.549\n",">552, c1=-540.463, c2=-510.913 g=515.528\n",">553, c1=-541.142, c2=-511.676 g=517.036\n",">554, c1=-542.405, c2=-505.273 g=517.510\n",">555, c1=-543.153, c2=-508.269 g=517.091\n",">556, c1=-544.739, c2=-509.345 g=516.580\n",">557, c1=-545.270, c2=-512.893 g=516.919\n",">558, c1=-545.227, c2=-512.661 g=518.517\n",">559, c1=-547.899, c2=-516.108 g=520.668\n",">560, c1=-548.385, c2=-515.971 g=521.606\n",">561, c1=-550.684, c2=-515.182 g=522.071\n",">562, c1=-552.034, c2=-519.017 g=524.114\n",">563, c1=-551.277, c2=-515.067 g=521.889\n",">564, c1=-553.300, c2=-519.525 g=525.506\n",">565, c1=-555.223, c2=-523.739 g=526.996\n",">566, c1=-555.845, c2=-523.611 g=525.950\n",">567, c1=-557.566, c2=-525.339 g=527.730\n",">568, c1=-558.937, c2=-526.638 g=530.264\n",">569, c1=-560.076, c2=-529.054 g=531.946\n",">570, c1=-561.464, c2=-531.986 g=534.240\n",">571, c1=-563.492, c2=-533.455 g=535.012\n",">572, c1=-565.108, c2=-536.194 g=536.177\n",">573, c1=-566.315, c2=-537.341 g=539.233\n",">574, c1=-568.372, c2=-540.115 g=540.944\n",">575, c1=-569.427, c2=-539.264 g=542.452\n",">576, c1=-570.251, c2=-541.869 g=544.234\n",">577, c1=-572.609, c2=-544.110 g=544.608\n",">578, c1=-573.826, c2=-545.815 g=547.077\n",">579, c1=-575.471, c2=-547.326 g=547.514\n",">580, c1=-576.373, c2=-549.554 g=550.724\n",">581, c1=-578.593, c2=-551.674 g=550.719\n",">582, c1=-579.041, c2=-551.771 g=553.815\n",">583, c1=-581.233, c2=-553.833 g=553.803\n",">584, c1=-582.341, c2=-553.138 g=556.293\n",">585, c1=-584.018, c2=-555.540 g=557.382\n",">586, c1=-585.178, c2=-556.882 g=555.775\n",">587, c1=-586.220, c2=-557.214 g=559.088\n",">588, c1=-587.506, c2=-559.439 g=561.367\n",">589, c1=-588.371, c2=-560.267 g=562.949\n",">590, c1=-590.738, c2=-561.930 g=564.752\n",">591, c1=-591.826, c2=-562.987 g=565.859\n",">592, c1=-593.061, c2=-564.465 g=564.629\n",">593, c1=-594.072, c2=-563.931 g=567.857\n",">594, c1=-596.270, c2=-565.601 g=568.636\n",">595, c1=-597.196, c2=-566.467 g=568.102\n",">596, c1=-598.798, c2=-566.869 g=571.075\n",">597, c1=-600.241, c2=-569.054 g=571.749\n",">598, c1=-600.556, c2=-570.453 g=574.466\n",">599, c1=-602.328, c2=-570.367 g=570.430\n",">600, c1=-602.450, c2=-570.325 g=575.398\n",">601, c1=-604.288, c2=-570.692 g=576.918\n",">602, c1=-605.823, c2=-573.120 g=577.570\n",">603, c1=-607.461, c2=-574.907 g=578.888\n",">604, c1=-607.780, c2=-573.799 g=579.800\n",">605, c1=-609.945, c2=-575.165 g=580.151\n",">606, c1=-610.395, c2=-571.803 g=580.126\n",">607, c1=-609.871, c2=-558.483 g=579.456\n",">608, c1=-611.947, c2=-561.471 g=579.623\n",">609, c1=-613.163, c2=-561.841 g=574.656\n",">610, c1=-613.506, c2=-559.232 g=577.115\n",">611, c1=-614.771, c2=-561.599 g=577.535\n",">612, c1=-613.906, c2=-555.459 g=572.757\n",">613, c1=-614.549, c2=-563.231 g=576.964\n",">614, c1=-614.976, c2=-559.373 g=576.008\n",">615, c1=-615.812, c2=-557.648 g=576.413\n",">616, c1=-615.038, c2=-554.799 g=578.133\n",">617, c1=-617.807, c2=-561.319 g=578.657\n",">618, c1=-618.957, c2=-567.831 g=579.181\n",">619, c1=-619.031, c2=-568.380 g=582.271\n",">620, c1=-622.672, c2=-570.272 g=583.802\n",">621, c1=-622.185, c2=-572.492 g=584.917\n",">622, c1=-623.420, c2=-569.906 g=585.358\n",">623, c1=-625.014, c2=-571.543 g=586.145\n",">624, c1=-625.277, c2=-577.805 g=587.073\n",">625, c1=-626.152, c2=-569.563 g=585.865\n",">626, c1=-628.252, c2=-574.973 g=580.942\n",">627, c1=-629.284, c2=-567.810 g=585.478\n",">628, c1=-629.011, c2=-562.781 g=582.884\n",">629, c1=-628.671, c2=-569.302 g=581.714\n",">630, c1=-629.518, c2=-554.380 g=579.924\n",">631, c1=-631.086, c2=-558.624 g=577.174\n",">632, c1=-630.062, c2=-554.294 g=577.681\n",">633, c1=-630.229, c2=-559.046 g=578.909\n",">634, c1=-634.600, c2=-569.198 g=578.657\n",">635, c1=-634.601, c2=-562.449 g=573.646\n",">636, c1=-635.948, c2=-552.690 g=570.373\n",">637, c1=-634.842, c2=-555.907 g=570.384\n",">638, c1=-636.185, c2=-550.930 g=569.545\n",">639, c1=-634.019, c2=-543.908 g=559.560\n",">640, c1=-634.559, c2=-520.110 g=545.602\n",">641, c1=-632.241, c2=-512.795 g=520.419\n",">642, c1=-630.005, c2=-500.156 g=502.818\n",">643, c1=-630.143, c2=-500.970 g=478.831\n",">644, c1=-629.905, c2=-493.183 g=480.728\n",">645, c1=-629.125, c2=-496.684 g=484.427\n",">646, c1=-628.924, c2=-504.499 g=494.171\n",">647, c1=-632.352, c2=-516.991 g=509.902\n",">648, c1=-632.507, c2=-527.238 g=510.583\n",">649, c1=-634.944, c2=-533.956 g=511.974\n",">650, c1=-636.589, c2=-535.794 g=528.344\n",">651, c1=-639.823, c2=-541.966 g=526.949\n",">652, c1=-643.334, c2=-543.194 g=531.731\n",">653, c1=-645.586, c2=-549.746 g=529.790\n",">654, c1=-647.075, c2=-555.013 g=539.838\n",">655, c1=-651.071, c2=-561.699 g=537.284\n",">656, c1=-652.854, c2=-567.806 g=533.178\n",">657, c1=-654.206, c2=-574.208 g=530.535\n",">658, c1=-655.450, c2=-579.525 g=512.359\n",">659, c1=-656.863, c2=-585.066 g=498.938\n",">660, c1=-658.022, c2=-588.224 g=467.379\n",">661, c1=-658.082, c2=-588.667 g=425.410\n",">662, c1=-658.679, c2=-589.438 g=388.286\n",">663, c1=-658.359, c2=-592.765 g=346.450\n",">664, c1=-658.524, c2=-594.447 g=357.535\n",">665, c1=-658.815, c2=-598.259 g=352.882\n",">666, c1=-659.753, c2=-600.222 g=387.653\n",">667, c1=-659.699, c2=-600.948 g=390.632\n",">668, c1=-660.772, c2=-602.497 g=384.846\n",">669, c1=-662.344, c2=-606.646 g=370.589\n",">670, c1=-665.534, c2=-609.845 g=354.759\n",">671, c1=-666.509, c2=-609.964 g=346.240\n",">672, c1=-668.293, c2=-606.630 g=401.370\n",">673, c1=-666.760, c2=-613.102 g=388.742\n",">674, c1=-668.788, c2=-612.180 g=336.690\n",">675, c1=-673.634, c2=-618.108 g=344.123\n",">676, c1=-671.824, c2=-613.545 g=302.023\n",">677, c1=-674.222, c2=-610.770 g=268.182\n",">678, c1=-672.413, c2=-605.786 g=235.895\n",">679, c1=-669.049, c2=-595.562 g=210.330\n",">680, c1=-668.297, c2=-586.427 g=255.775\n",">681, c1=-665.279, c2=-579.502 g=274.305\n",">682, c1=-658.549, c2=-565.971 g=275.409\n",">683, c1=-660.063, c2=-568.659 g=335.079\n",">684, c1=-655.256, c2=-585.760 g=308.070\n",">685, c1=-660.120, c2=-585.568 g=319.952\n",">686, c1=-654.086, c2=-587.555 g=340.516\n",">687, c1=-659.372, c2=-588.263 g=394.327\n",">688, c1=-658.891, c2=-592.467 g=396.877\n",">689, c1=-658.358, c2=-595.109 g=385.690\n",">690, c1=-660.176, c2=-584.464 g=453.676\n",">691, c1=-656.076, c2=-592.624 g=443.534\n",">692, c1=-660.446, c2=-592.723 g=445.344\n",">693, c1=-663.610, c2=-599.498 g=430.939\n",">694, c1=-662.815, c2=-600.934 g=424.732\n",">695, c1=-664.569, c2=-609.252 g=414.327\n",">696, c1=-664.483, c2=-606.128 g=423.395\n",">697, c1=-666.230, c2=-612.332 g=368.930\n",">698, c1=-668.137, c2=-610.450 g=375.512\n",">699, c1=-669.519, c2=-603.158 g=368.366\n",">700, c1=-669.625, c2=-601.990 g=337.370\n",">701, c1=-666.775, c2=-596.408 g=274.755\n",">702, c1=-664.671, c2=-595.815 g=321.649\n",">703, c1=-663.298, c2=-593.434 g=308.225\n",">704, c1=-664.410, c2=-581.478 g=237.148\n",">705, c1=-660.073, c2=-573.763 g=136.489\n",">706, c1=-667.221, c2=-539.639 g=60.264\n",">707, c1=-654.766, c2=-495.768 g=-65.104\n",">708, c1=-641.037, c2=-425.365 g=-199.825\n",">709, c1=-621.965, c2=-332.945 g=-304.691\n",">710, c1=-585.445, c2=-183.054 g=-327.246\n",">711, c1=-556.794, c2=-143.836 g=-349.264\n",">712, c1=-523.275, c2=-86.667 g=-273.764\n",">713, c1=-498.076, c2=-29.663 g=-194.189\n",">714, c1=-476.841, c2=-78.775 g=-106.087\n",">715, c1=-468.001, c2=-91.040 g=-23.653\n",">716, c1=-457.443, c2=-143.823 g=50.849\n",">717, c1=-449.989, c2=-161.829 g=111.416\n",">718, c1=-443.239, c2=-210.405 g=196.032\n",">719, c1=-423.583, c2=-257.438 g=280.583\n",">720, c1=-418.944, c2=-289.025 g=356.576\n",">721, c1=-405.078, c2=-352.790 g=421.593\n",">722, c1=-400.776, c2=-352.377 g=448.268\n",">723, c1=-387.562, c2=-379.620 g=492.497\n",">724, c1=-388.100, c2=-366.570 g=498.189\n",">725, c1=-406.984, c2=-367.585 g=518.410\n",">726, c1=-396.549, c2=-360.567 g=514.380\n",">727, c1=-416.367, c2=-346.251 g=510.475\n",">728, c1=-407.057, c2=-313.342 g=504.537\n",">729, c1=-411.191, c2=-290.658 g=501.028\n",">730, c1=-413.225, c2=-272.457 g=471.798\n",">731, c1=-417.314, c2=-278.526 g=434.586\n",">732, c1=-420.936, c2=-321.700 g=398.815\n",">733, c1=-453.947, c2=-356.413 g=359.222\n",">734, c1=-465.075, c2=-398.565 g=333.787\n",">735, c1=-487.247, c2=-381.980 g=318.768\n",">736, c1=-493.185, c2=-409.683 g=291.563\n",">737, c1=-502.225, c2=-431.522 g=280.566\n",">738, c1=-508.642, c2=-418.122 g=307.208\n",">739, c1=-510.537, c2=-408.020 g=304.214\n",">740, c1=-496.584, c2=-370.657 g=322.525\n",">741, c1=-482.360, c2=-410.510 g=369.660\n",">742, c1=-499.132, c2=-421.299 g=403.787\n",">743, c1=-478.493, c2=-432.819 g=414.292\n",">744, c1=-492.907, c2=-433.275 g=435.801\n",">745, c1=-495.632, c2=-428.524 g=464.622\n",">746, c1=-487.299, c2=-428.850 g=473.287\n",">747, c1=-494.827, c2=-450.147 g=475.124\n",">748, c1=-486.330, c2=-453.465 g=486.038\n",">749, c1=-496.530, c2=-471.597 g=479.151\n",">750, c1=-509.863, c2=-491.001 g=509.710\n",">751, c1=-513.754, c2=-509.248 g=510.549\n",">752, c1=-521.900, c2=-513.523 g=538.102\n",">753, c1=-534.718, c2=-515.648 g=537.768\n",">754, c1=-520.948, c2=-534.474 g=588.985\n",">755, c1=-514.485, c2=-546.514 g=572.626\n",">756, c1=-525.345, c2=-516.833 g=580.242\n",">757, c1=-518.119, c2=-503.000 g=571.868\n",">758, c1=-498.582, c2=-469.319 g=540.590\n",">759, c1=-508.236, c2=-524.987 g=561.146\n",">760, c1=-504.096, c2=-462.014 g=528.336\n",">761, c1=-487.334, c2=-457.762 g=489.477\n",">762, c1=-510.924, c2=-489.398 g=514.531\n",">763, c1=-489.666, c2=-395.598 g=481.889\n",">764, c1=-460.640, c2=-366.712 g=471.316\n",">765, c1=-419.910, c2=-226.096 g=343.903\n",">766, c1=-404.718, c2=-97.464 g=122.808\n",">767, c1=-428.789, c2=-70.215 g=25.804\n",">768, c1=-460.464, c2=-74.129 g=-25.942\n",">769, c1=-458.669, c2=-50.027 g=-33.087\n",">770, c1=-477.937, c2=-42.573 g=-66.419\n",">771, c1=-469.728, c2=-119.628 g=-57.633\n",">772, c1=-480.163, c2=-183.242 g=-85.963\n",">773, c1=-478.156, c2=-245.968 g=-17.926\n",">774, c1=-489.654, c2=-305.340 g=21.313\n",">775, c1=-495.686, c2=-396.072 g=-22.531\n",">776, c1=-530.242, c2=-504.634 g=-123.136\n",">777, c1=-546.623, c2=-485.002 g=-173.787\n",">778, c1=-559.974, c2=-528.599 g=-315.017\n",">779, c1=-592.492, c2=-552.064 g=-383.948\n",">780, c1=-596.244, c2=-548.386 g=-426.912\n",">781, c1=-621.830, c2=-549.055 g=-448.550\n",">782, c1=-631.337, c2=-605.451 g=-464.019\n",">783, c1=-643.413, c2=-633.905 g=-541.030\n",">784, c1=-669.623, c2=-662.383 g=-569.961\n",">785, c1=-681.597, c2=-667.038 g=-576.152\n",">786, c1=-684.913, c2=-656.205 g=-615.880\n",">787, c1=-694.832, c2=-677.864 g=-637.874\n",">788, c1=-703.650, c2=-680.710 g=-637.086\n",">789, c1=-710.055, c2=-656.898 g=-647.814\n",">790, c1=-713.652, c2=-699.003 g=-681.017\n",">791, c1=-726.423, c2=-695.051 g=-675.044\n",">792, c1=-724.273, c2=-712.754 g=-703.158\n",">793, c1=-739.867, c2=-722.943 g=-708.900\n",">794, c1=-745.769, c2=-735.741 g=-717.846\n",">795, c1=-736.546, c2=-712.614 g=-734.519\n",">796, c1=-753.452, c2=-724.294 g=-731.165\n",">797, c1=-753.813, c2=-735.173 g=-744.331\n",">798, c1=-760.575, c2=-731.397 g=-724.034\n",">799, c1=-759.714, c2=-732.939 g=-754.562\n",">800, c1=-753.267, c2=-722.680 g=-752.584\n",">801, c1=-764.810, c2=-731.640 g=-752.688\n",">802, c1=-769.624, c2=-744.462 g=-759.080\n",">803, c1=-758.862, c2=-724.764 g=-767.951\n",">804, c1=-771.168, c2=-760.158 g=-764.551\n",">805, c1=-777.567, c2=-767.552 g=-773.148\n",">806, c1=-784.617, c2=-762.376 g=-759.347\n",">807, c1=-785.298, c2=-777.161 g=-756.549\n",">808, c1=-776.416, c2=-715.443 g=-784.666\n",">809, c1=-777.941, c2=-723.017 g=-788.545\n",">810, c1=-787.200, c2=-743.262 g=-781.661\n",">811, c1=-776.592, c2=-747.647 g=-782.379\n",">812, c1=-782.975, c2=-751.582 g=-792.299\n",">813, c1=-781.937, c2=-744.437 g=-791.358\n",">814, c1=-774.928, c2=-754.415 g=-800.773\n",">815, c1=-789.210, c2=-751.818 g=-787.581\n",">816, c1=-791.326, c2=-762.819 g=-795.263\n",">817, c1=-779.707, c2=-698.566 g=-790.412\n",">818, c1=-783.365, c2=-724.236 g=-796.287\n",">819, c1=-742.825, c2=-639.581 g=-793.221\n",">820, c1=-747.366, c2=-662.153 g=-796.124\n",">821, c1=-712.248, c2=-587.968 g=-788.484\n",">822, c1=-733.232, c2=-642.471 g=-776.450\n",">823, c1=-724.836, c2=-645.667 g=-790.881\n",">824, c1=-643.180, c2=-420.878 g=-778.226\n",">825, c1=-650.126, c2=-514.390 g=-781.838\n",">826, c1=-696.022, c2=-595.632 g=-775.357\n",">827, c1=-689.535, c2=-650.547 g=-772.495\n",">828, c1=-709.202, c2=-685.905 g=-792.824\n",">829, c1=-714.226, c2=-690.245 g=-801.942\n",">830, c1=-676.748, c2=-521.557 g=-783.192\n",">831, c1=-714.459, c2=-650.699 g=-779.845\n",">832, c1=-677.261, c2=-669.938 g=-788.090\n",">833, c1=-732.523, c2=-702.676 g=-783.586\n",">834, c1=-696.941, c2=-718.009 g=-804.214\n",">835, c1=-739.710, c2=-712.219 g=-811.603\n",">836, c1=-718.349, c2=-524.310 g=-795.135\n",">837, c1=-713.301, c2=-641.536 g=-791.271\n",">838, c1=-724.521, c2=-635.016 g=-791.057\n",">839, c1=-654.698, c2=-519.508 g=-786.321\n",">840, c1=-573.172, c2=-311.377 g=-762.181\n",">841, c1=-508.649, c2=-142.823 g=-769.943\n",">842, c1=-438.378, c2=313.639 g=-760.439\n",">843, c1=-485.759, c2=42.147 g=-749.039\n",">844, c1=-523.352, c2=589.421 g=-721.115\n",">845, c1=-568.762, c2=596.535 g=-638.839\n",">846, c1=-592.795, c2=554.127 g=-545.857\n",">847, c1=-582.554, c2=485.744 g=-461.515\n",">848, c1=-572.797, c2=481.189 g=-427.114\n",">849, c1=-553.565, c2=354.699 g=-514.623\n",">850, c1=-514.140, c2=277.817 g=-572.991\n",">851, c1=-492.759, c2=501.578 g=-436.923\n",">852, c1=-554.073, c2=414.003 g=-482.479\n",">853, c1=-457.204, c2=183.851 g=-543.952\n",">854, c1=-394.169, c2=227.537 g=-520.916\n",">855, c1=-358.314, c2=121.108 g=-521.268\n",">856, c1=-394.051, c2=311.873 g=-467.175\n",">857, c1=-372.771, c2=310.159 g=-457.028\n",">858, c1=-444.770, c2=378.914 g=-428.043\n",">859, c1=-407.142, c2=320.752 g=-436.466\n",">860, c1=-423.311, c2=232.860 g=-476.288\n",">861, c1=-394.455, c2=276.318 g=-444.166\n",">862, c1=-395.883, c2=352.880 g=-355.612\n",">863, c1=-405.703, c2=343.387 g=-384.837\n",">864, c1=-431.252, c2=373.897 g=-404.613\n",">865, c1=-434.597, c2=137.770 g=-461.194\n",">866, c1=-357.365, c2=247.215 g=-405.944\n",">867, c1=-426.257, c2=114.971 g=-366.850\n",">868, c1=-369.918, c2=210.335 g=-276.229\n",">869, c1=-385.232, c2=200.131 g=-299.699\n",">870, c1=-377.145, c2=352.458 g=-225.844\n",">871, c1=-412.631, c2=296.138 g=-260.814\n",">872, c1=-420.182, c2=192.020 g=-275.630\n",">873, c1=-360.351, c2=182.629 g=-242.533\n",">874, c1=-392.416, c2=215.785 g=-269.578\n",">875, c1=-370.062, c2=289.202 g=-226.449\n",">876, c1=-394.815, c2=246.372 g=-233.535\n",">877, c1=-379.681, c2=237.183 g=-204.725\n",">878, c1=-327.647, c2=216.045 g=-203.961\n",">879, c1=-322.209, c2=303.995 g=-195.564\n",">880, c1=-337.026, c2=219.813 g=-254.352\n",">881, c1=-346.513, c2=163.508 g=-199.698\n",">882, c1=-328.260, c2=91.808 g=-113.675\n",">883, c1=-318.957, c2=-7.885 g=-15.945\n",">884, c1=-348.911, c2=-11.604 g=27.910\n",">885, c1=-346.210, c2=-21.558 g=69.275\n",">886, c1=-303.858, c2=23.220 g=82.209\n",">887, c1=-308.974, c2=10.991 g=107.736\n",">888, c1=-293.041, c2=22.352 g=114.972\n",">889, c1=-277.431, c2=81.347 g=71.642\n",">890, c1=-281.701, c2=126.121 g=42.443\n",">891, c1=-262.690, c2=162.071 g=-8.642\n",">892, c1=-263.036, c2=131.536 g=-8.119\n",">893, c1=-270.380, c2=78.353 g=11.650\n",">894, c1=-257.294, c2=42.456 g=36.229\n",">895, c1=-256.456, c2=0.513 g=24.313\n",">896, c1=-239.628, c2=5.119 g=-3.460\n",">897, c1=-263.015, c2=25.797 g=43.377\n",">898, c1=-276.342, c2=-101.914 g=113.290\n",">899, c1=-256.537, c2=-141.208 g=163.889\n",">900, c1=-262.754, c2=-123.176 g=199.096\n",">901, c1=-269.386, c2=-123.870 g=170.527\n",">902, c1=-266.806, c2=-66.440 g=155.325\n",">903, c1=-256.433, c2=39.481 g=111.413\n",">904, c1=-237.883, c2=127.042 g=9.908\n",">905, c1=-287.070, c2=149.457 g=32.295\n",">906, c1=-288.162, c2=95.977 g=27.478\n",">907, c1=-279.067, c2=41.038 g=89.163\n",">908, c1=-279.046, c2=-19.556 g=125.158\n",">909, c1=-257.191, c2=-53.559 g=134.902\n",">910, c1=-242.284, c2=-32.868 g=160.460\n",">911, c1=-225.636, c2=-24.354 g=122.564\n",">912, c1=-222.850, c2=-4.658 g=145.521\n",">913, c1=-184.504, c2=-20.169 g=189.092\n",">914, c1=-194.254, c2=-51.256 g=192.179\n",">915, c1=-185.012, c2=-76.305 g=227.285\n",">916, c1=-170.070, c2=-104.809 g=219.489\n",">917, c1=-193.147, c2=-108.080 g=237.411\n",">918, c1=-206.265, c2=-84.129 g=204.598\n",">919, c1=-185.889, c2=-59.671 g=173.568\n",">920, c1=-190.528, c2=-34.819 g=187.977\n",">921, c1=-197.465, c2=-20.674 g=183.808\n",">922, c1=-200.015, c2=-18.005 g=158.430\n",">923, c1=-201.692, c2=-46.750 g=186.418\n",">924, c1=-196.103, c2=-73.322 g=204.840\n",">925, c1=-180.247, c2=-66.122 g=184.222\n",">926, c1=-211.624, c2=-99.735 g=225.907\n",">927, c1=-169.420, c2=-86.708 g=234.311\n",">928, c1=-161.035, c2=-96.163 g=233.296\n",">929, c1=-157.465, c2=-87.851 g=231.780\n",">930, c1=-156.066, c2=-114.457 g=257.451\n",">931, c1=-148.417, c2=-123.469 g=252.925\n",">932, c1=-148.092, c2=-146.079 g=276.848\n",">933, c1=-147.545, c2=-114.826 g=277.592\n",">934, c1=-149.297, c2=-94.313 g=232.016\n",">935, c1=-151.827, c2=-52.012 g=202.506\n",">936, c1=-152.332, c2=-33.001 g=180.987\n",">937, c1=-163.531, c2=-64.194 g=206.601\n",">Saved: /content/gdrive/My Drive/Colab Notebooks/results_baseline/plots/generated_plot_0937.png and /content/gdrive/My Drive/Colab Notebooks/results_baseline/models/model_0937.h5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uNtm9EigvvGB","colab_type":"text"},"source":["# WGAN Kannada"]},{"cell_type":"code","metadata":{"id":"la1jDgIk1vJb","colab_type":"code","colab":{}},"source":["# example of a wgan for generating handwritten digits\n","from numpy import expand_dims\n","from numpy import mean\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.mnist import load_data\n","from keras import backend\n","from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.initializers import RandomNormal\n","from keras.constraints import Constraint\n","from matplotlib import pyplot\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","latent_dim = 50\n","n_epochs = 20\n","n_batch = 16\n","\n","out_dir = \"/content/gdrive/My Drive/Colab Notebooks/wgan_output/kannada_wganwc_epoch\"+ str(n_epochs) + \"_bsize\"+ str(n_batch)+ \"_latent_dim\" + str(latent_dim)+ \"/\"\n","\n","# clip model weights to a given hypercube\n","class ClipConstraint(Constraint):\n","\t# set clip value when initialized\n","\tdef __init__(self, clip_value):\n","\t\tself.clip_value = clip_value\n","\n","\t# clip model weights to hypercube\n","\tdef __call__(self, weights):\n","\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n","\n","\t# get the config\n","\tdef get_config(self):\n","\t\treturn {'clip_value': self.clip_value}\n","\n","# calculate wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","\treturn backend.mean(y_true * y_pred)\n","\n","# define the standalone critic model\n","def define_critic(in_shape=(28,28,1)):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# weight constraint\n","\tconst = ClipConstraint(0.01)\n","\t# define model\n","\tmodel = Sequential()\n","\t# downsample to 14x14\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# downsample to 7x7\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# scoring, linear activation\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(1))\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model\n","\n","# define the standalone generator model\n","def define_generator(latent_dim):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# define model\n","\tmodel = Sequential()\n","\t# foundation for 7x7 image\n","\tn_nodes = 128 * 7 * 7\n","\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\tmodel.add(Reshape((7, 7, 128)))\n","\t# upsample to 14x14\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# upsample to 28x28\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# output 28x28x1\n","\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n","\treturn model\n","\n","# define the combined generator and critic model, for updating the generator\n","def define_gan(generator, critic):\n","\t# make weights in the critic not trainable\n","\tcritic.trainable = False\n","\t# connect them\n","\tmodel = Sequential()\n","\t# add generator\n","\tmodel.add(generator)\n","\t# add the critic\n","\tmodel.add(critic)\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model\n","\n","# load images\n","def load_real_samples():\n","  kannada_train_url = \"https://raw.githubusercontent.com/JimmyJHickey/ST790-Project/master/Kannada-MNIST/train.csv\"\n","\n","  train = pd.read_csv(kannada_train_url)\n","\n","  X=train.iloc[:,1:].values \n","  Y=train.iloc[:,0].values \n","\n","  X = X.reshape(X.shape[0], 28, 28,1) \n","\n","  X = X.astype('float32')\n","\n","  # scale from [0,255] to [-1,1]\n","  X = (X - 127.5) / 127.5\n","  return X\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels, -1 for 'real'\n","\ty = -ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n_samples, latent_dim)\n","\treturn x_input\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels with 1.0 for 'fake'\n","\ty = ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","\t# prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = out_dir + 'plots/generated_plot_%09d.png' % (step+1)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\t# save the generator model\n","\tfilename2 = out_dir + 'models/model_%04d.h5' % (step+1)\n","\tg_model.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist):\n","\t# plot history\n","\tpyplot.plot(d1_hist, label='crit_real')\n","\tpyplot.plot(d2_hist, label='crit_fake')\n","\tpyplot.plot(g_hist, label='gen')\n","\tpyplot.legend()\n","\tpyplot.savefig(out_dir + 'plot_line_plot_loss.png')\n","\tpyplot.close()\n","\n","# train the generator and critic\n","def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(dataset.shape[0] / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","\t# calculate the size of half a batch of samples\n","\thalf_batch = int(n_batch / 2)\n","\t# lists for keeping track of loss\n","\tc1_hist, c2_hist, g_hist = list(), list(), list()\n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# update the critic more than the generator\n","\t\tc1_tmp, c2_tmp = list(), list()\n","\t\tfor _ in range(n_critic):\n","\t\t\t# get randomly selected 'real' samples\n","\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t\t\tc1_tmp.append(c_loss1)\n","\t\t\t# generate 'fake' examples\n","\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\t\t\tc2_tmp.append(c_loss2)\n","\t\t# store critic loss\n","\t\tc1_hist.append(mean(c1_tmp))\n","\t\tc2_hist.append(mean(c2_tmp))\n","\t\t# prepare points in latent space as input for the generator\n","\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t\t# create inverted labels for the fake samples\n","\t\ty_gan = -ones((n_batch, 1))\n","\t\t# update the generator via the critic's error\n","\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","\t\tg_hist.append(g_loss)\n","\t\t# summarize loss on this batch\n","\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n","\t\t# evaluate the model performance every 'epoch'\n","\t\tif (i+1) % bat_per_epo == 0:\n","\t\t\tsummarize_performance(i, g_model, latent_dim)\n","\t# line plots of loss\n","\tplot_history(c1_hist, c2_hist, g_hist)\n","\n","\n","#############\n","#############\n","\n","#############\n","\n","#############\n","#############\n","#############\n","\n","from os import makedirs\n","\n","print(out_dir)\n","makedirs(out_dir)\n","\n","makedirs(out_dir + 'models', exist_ok=True)\n","makedirs(out_dir + 'plots', exist_ok=True)\n","\n","# size of the latent space\n","\n","# create the critic\n","critic = define_critic()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, critic)\n","# load image data\n","dataset = load_real_samples()\n","\n","# train model\n","start_time = datetime.now()\n","train(generator, critic, gan_model, dataset, latent_dim, n_epochs=n_epochs, n_batch = n_batch)\n","end_time = datetime.now()\n","print(end_time - start_time)\n","\n","time_file = out_dir + \"time.txt\"\n","with open(time_file, \"w\") as f:\n","\t\tf.write(\"total time\\n\")\n","\t\tf.write(str(end_time - start_time))\n","\n","\n","model_json = generator.to_json()\n","model_json_path = out_dir + \"model.json\"\n","\n","with open(model_json_path, \"w\") as json_file:\n","    json_file.write(model_json)\n","print(\"Saved model to disk\")\n","\n","\n","\n","weight_file = out_dir + \"final_weights.txt\"\n","\n","weights = critic.get_weights()\n","\n","flat_list = [item for sublist in weights for item in sublist]\n","\n","flatter_list = list()\n","\n","for i in flat_list:\n","  flatter_list += i.flatten().tolist() \n","\n","\n","with open(weight_file, 'w') as f:\n","  for item in flatter_list:\n","    f.write(\"%s\\n\" % item)\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","plot_file = \"/content/gdrive/My Drive/Colab Notebooks/wgan_output/weight_plots/kannada_wganwc_epoch\"+ str(n_epochs) + \"_bsize\"+ str(n_batch)+ \"_latentdim\" + str(latent_dim)+ \".png\"\n","\n","num_bins = 1000\n","n, bins, patches = plt.hist(flatter_list, num_bins, facecolor='red', range = [-0.015, 0.015])\n","plt.xlabel('Critic Weights')\n","plt.ylabel('Count')\n","plt.title(r'WGAN-WC Critic Weights: Epochs=' + str(n_epochs)+ \" Batches=\" + str(n_batch))\n","plt.show()\n","\n","print('WGAN-WC_Critic_Weights:_Epochs={}_Batches={}'.format(str(n_epochs), str(n_batch)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5MVmmvLa7v-","colab_type":"text"},"source":["# Vanilla KANNADA"]},{"cell_type":"code","metadata":{"id":"V2PM4XwBa-s8","colab_type":"code","colab":{}},"source":["# example of training a stable gan for generating a handwritten digit\n","from os import makedirs\n","from numpy import expand_dims\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from tensorflow.keras.datasets.mnist import load_data\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.initializers import RandomNormal\n","from matplotlib import pyplot\n","import pandas as pd\n","from tensorflow.keras import backend as K\n","from datetime import datetime\n","\n","\n","# define the standalone discriminator model\n","def define_discriminator(in_shape=(28, 28, 1)):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # define model\n","    model = Sequential()\n","    # downsample to 14x14\n","    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init, input_shape=in_shape))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    # downsample to 7x7\n","    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.2))\n","    # classifier\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","\n","def define_generator(latent_dim):\n","    \"\"\"Creates a generator model that takes a 100-dimensional noise vector as a \"seed\",\n","    and outputs images of size 28x28x1.\"\"\"\n","    model = Sequential()\n","    model.add(Dense(1024, input_dim=latent_dim))\n","    model.add(LeakyReLU())\n","    model.add(Dense(128 * 7 * 7))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","    if K.image_data_format() == 'channels_first':\n","        model.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n","        bn_axis = 1\n","    else:\n","        model.add(Reshape((7, 7, 128), input_shape=(128 * 7 * 7,)))\n","        bn_axis = -1\n","    model.add(Conv2DTranspose(128, (5, 5), strides=2, padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","    model.add(Conv2D(64, (5, 5), padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","    model.add(Conv2DTranspose(64, (5, 5), strides=2, padding='same'))\n","    model.add(BatchNormalization(axis=bn_axis))\n","    model.add(LeakyReLU())\n","\n","    # Because we normalized training inputs to lie in the range [-1, 1],\n","    # the tanh function should be used for the output of the generator to ensure\n","    # its output also lies in this range.\n","    model.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n","    return model\n","\n","# define the combined generator and discriminator model, for updating the generator\n","def define_gan(generator, discriminator):\n","    # make weights in the discriminator not trainable\n","    discriminator.trainable = False\n","    # connect them\n","    model = Sequential()\n","    # add generator\n","    model.add(generator)\n","    # add the discriminator\n","    model.add(discriminator)\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n","\n","\n","# load mnist images\n","def load_real_samples(n=60000):\n","    # load dataset\n","  kannada_train_url = \"https://raw.githubusercontent.com/JimmyJHickey/ST790-Project/master/Kannada-MNIST/train.csv\"\n","\n","  train = pd.read_csv(kannada_train_url)\n","\n","  X=train.iloc[:,1:].values \n","  Y=train.iloc[:,0].values \n","  selected_ix = Y==7\n","  X=X[selected_ix]\n","\n","  X = X.reshape(X.shape[0], 28, 28,1) \n","\n","\n","  X = X.astype('float32')\n","  \n","  # scale from [0,255] to [-1,1]\n","  X = (X - 127.5) / 127.5\n","  return X\n","\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","    # choose random instances\n","    ix = randint(0, dataset.shape[0], n_samples)\n","    # select images\n","    X = dataset[ix]\n","    # generate class labels\n","    y = ones((n_samples, 1))\n","    return X, y\n","\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","    # generate points in the latent space\n","    x_input = randn(latent_dim * n_samples)\n","    # reshape into a batch of inputs for the network\n","    x_input = x_input.reshape(n_samples, latent_dim)\n","    return x_input\n","\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","    # generate points in latent space\n","    x_input = generate_latent_points(latent_dim, n_samples)\n","    # predict outputs\n","    X = generator.predict(x_input)\n","    # create class labels\n","    y = zeros((n_samples, 1))\n","    return X, y\n","\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","    # prepare fake examples\n","    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","    # scale from [-1,1] to [0,1]\n","    X = (X + 1) / 2.0\n","    # plot images\n","    for i in range(10 * 10):\n","        # define subplot\n","        pyplot.subplot(10, 10, 1 + i)\n","        # turn off axis\n","        pyplot.axis('off')\n","        # plot raw pixel data\n","        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","    # save plot to file\n","    pyplot.savefig('results_baseline/generated_plot_%03d.png' % (step + 1))\n","    pyplot.close()\n","    # save the generator model\n","    g_model.save('results_baseline/model_%03d.h5' % (step + 1))\n","\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n","    # plot loss\n","    pyplot.subplot(2, 1, 1)\n","    pyplot.plot(d1_hist, label='d-real')\n","    pyplot.plot(d2_hist, label='d-fake')\n","    pyplot.plot(g_hist, label='gen')\n","    pyplot.legend()\n","    # plot discriminator accuracy\n","    pyplot.subplot(2, 1, 2)\n","    pyplot.plot(a1_hist, label='acc-real')\n","    pyplot.plot(a2_hist, label='acc-fake')\n","    pyplot.legend()\n","    # save plot to file\n","    pyplot.savefig('results_baseline/plot_line_plot_loss.png')\n","    pyplot.close()\n","\n","\n","# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1, n_batch=128):\n","    # calculate the number of batches per epoch\n","    bat_per_epo = int(dataset.shape[0] / n_batch)\n","    # calculate the total iterations based on batch and epoch\n","    n_steps = bat_per_epo * n_epochs\n","    # calculate the number of samples in half a batch\n","    half_batch = int(n_batch / 2)\n","    # prepare lists for storing stats each iteration\n","    d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n","    # manually enumerate epochs\n","    for i in range(n_steps):\n","        # get randomly selected 'real' samples\n","        X_real, y_real = generate_real_samples(dataset, half_batch)\n","        # update discriminator model weights\n","        d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n","        # generate 'fake' examples\n","        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","        # update discriminator model weights\n","        d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n","        # prepare points in latent space as input for the generator\n","        X_gan = generate_latent_points(latent_dim, n_batch)\n","        # create inverted labels for the fake samples\n","        y_gan = ones((n_batch, 1))\n","        # update the generator via the discriminator's error\n","        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n","        # summarize loss on this batch\n","        print('>%d, d1=%.3f, d2=%.3f g=%.3f, a1=%d, a2=%d' %\n","              (i + 1, d_loss1, d_loss2, g_loss, int(100 * d_acc1), int(100 * d_acc2)))\n","        # record history\n","        d1_hist.append(d_loss1)\n","        d2_hist.append(d_loss2)\n","        g_hist.append(g_loss)\n","        a1_hist.append(d_acc1)\n","        a2_hist.append(d_acc2)\n","        # evaluate the model performance every 'epoch'\n","        if (i + 1) % bat_per_epo == 0:\n","            summarize_performance(i, g_model, latent_dim)\n","    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n","\n","\n","# make folder for results\n","makedirs('results_baseline', exist_ok=True)\n","# size of the latent space\n","latent_dim = 50\n","n_epochs = 20\n","n_batch = 64\n","\n","startTime = datetime.now()\n","# create the discriminator\n","discriminator = define_discriminator()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, discriminator)\n","# load image data\n","dataset = load_real_samples()\n","print(dataset.shape)\n","#train model\n","train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=n_epochs,n_batch=n_batch)\n","\n","print(datetime.now() - startTime)\n","\n","weight_file = out_dir + \"final_weights.txt\"\n","\n","weights = discriminator.get_weights()\n","\n","flat_list = [item for sublist in weights for item in sublist]\n","\n","flatter_list = list()\n","\n","for i in flat_list:\n","  flatter_list += i.flatten().tolist() \n","\n","\n","with open(weight_file, 'w') as f:\n","  for item in flatter_list:\n","    f.write(\"%s\\n\" % item)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCYp8ysOhWiH","colab_type":"code","outputId":"69422383-bc0e-4902-a177-f31219bad5d7","executionInfo":{"status":"ok","timestamp":1588005868723,"user_tz":240,"elapsed":7027,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","plot_file = \"/content/gdrive/My Drive/Colab Notebooks/wgan_output/weight_plots/kannada_wganwc_epoch\"+ str(n_epochs) + \"_bsize\"+ str(n_batch)+ \"_latentdim\" + str(latent_dim)+ \".png\"\n","\n","num_bins = 1000\n","n, bins, patches = plt.hist(flatter_list, num_bins, facecolor='red',range = [-0.2, 0.2])\n","plt.xlabel('Discriminator Weights')\n","plt.ylabel('Count')\n","plt.title(r'Base GAN Discriminator Weights: Epochs=' + str(n_epochs)+ \" Batches=\" + str(n_batch))\n","plt.show()\n","\n","print('Base_GAN_Discriminator_Weights:_Epochs={}_Batches={}'.format(str(n_epochs), str(n_batch)))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Base_GAN_Discriminator_Weights:_Epochs=20_Batches=64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ZWeBEM6HJGF","colab_type":"text"},"source":["# RUN SAVED MODEL"]},{"cell_type":"code","metadata":{"id":"KvsY_AcXHLgW","colab_type":"code","outputId":"2b8823b6-e842-4db9-99b1-5e2a3ae35272","executionInfo":{"status":"ok","timestamp":1585835766148,"user_tz":240,"elapsed":714,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.models import model_from_json\n","\n","\n","# load json and create model\n","json_file = open('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_full_200epoch/model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","\n","\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_full_200epoch/models/model_187400.h5\")\n","print(\"Loaded model from disk\")\n","\n","makedirs('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_full_200epoch/test_output', exist_ok=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded model from disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u9hfDPKWI9-W","colab_type":"code","colab":{}},"source":["def make_output_file(name, g_model, n_samples=100):\n","\tlatent_dim = 50\n","  \n","  # prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = '/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_full_200epoch/test_output/%s.png' % (name)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\tprint('>Saved: %s' % (filename1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsfEaG1kLYXY","colab_type":"code","outputId":"8d15e88b-db87-4f78-b185-3437bad677d1","executionInfo":{"status":"ok","timestamp":1585836593681,"user_tz":240,"elapsed":2189,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["make_output_file(\"test3\", loaded_model, 100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":[">Saved: /content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_full_200epoch/test_output/test3.png\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CTbcBnTYXEcs","colab_type":"text"},"source":["# In Progress Gradient Penalty Implementations\n","\n","Below are our attempts to get the gradient penalty implemented as well as the sources we were basing our code off of.\n","\n","The Kaggle one is the closest to working."]},{"cell_type":"markdown","metadata":{"id":"qpF7cRPSXcwQ","colab_type":"text"},"source":["## Github\n","https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py\n"]},{"cell_type":"code","metadata":{"id":"c6pn3P7zXD9Y","colab_type":"code","outputId":"6ae59355-6ceb-4226-af7a-8d0f29bfa85c","executionInfo":{"status":"error","timestamp":1587910235121,"user_tz":240,"elapsed":4297,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":412}},"source":["# example of a wgan for generating handwritten digits\n","from numpy import expand_dims\n","from numpy import mean\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.mnist import load_data\n","from keras import backend as K\n","from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.layers import Input\n","from keras.layers.merge import _Merge\n","from keras.initializers import RandomNormal\n","from keras.constraints import Constraint\n","from matplotlib import pyplot\n","import pandas as pd\n","\n","from functools import partial\n","\n","\n","# https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py\n","class RandomWeightedAverage(_Merge):\n","    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n","    def _merge_function(self, inputs):\n","        alpha = K.random_uniform((32, 1, 1, 1))\n","        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n","\n","# clip model weights to a given hypercube\n","class ClipConstraint(Constraint):\n","\t# set clip value when initialized\n","\tdef __init__(self, clip_value):\n","\t\tself.clip_value = clip_value\n","\n","\t# clip model weights to hypercube\n","\tdef __call__(self, weights):\n","\t\treturn K.clip(weights, -self.clip_value, self.clip_value)\n","\n","\t# get the config\n","\tdef get_config(self):\n","\t\treturn {'clip_value': self.clip_value}\n","\n","# calculate wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","  return K.mean(y_true * y_pred)\n","\n","# https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py\n","def gradient_penalty_loss(y_true, y_pred, real_img, fake_img, lam=10):\n","\t\"\"\"\n","\tComputes gradient penalty based on prediction and weighted real / fake samples\n","\t\"\"\"\n","\tinterpolated_img = RandomWeightedAverage()([real_img, fake_img])\n","\tgradients = K.gradients(y_pred, interpolated_img)[0]\n","\t# compute the euclidean norm by squaring ...\n","\tgradients_sqr = K.square(gradients)\n","\t#   ... summing over the rows ...\n","\tgradients_sqr_sum = K.sum(gradients_sqr,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\taxis=np.arange(1, len(gradients_sqr.shape)))\n","\t#   ... and sqrt\n","\tgradient_l2_norm = K.sqrt(gradients_sqr_sum)\n","\t# compute (||grad|| - 1)^2 still for each single sample\n","\texpected_parameter =  K.square(gradient_l2_norm - 1)\n","\t# compute mean\n","\texpected_value = K.mean(expected_parameter)\n","\t# multiply by lambda for final penalty\n","\tgradient_penalty = lam * expected_value\n","\treturn gradient_penalty\n","\n","def wgan_gp_loss(y_true, y_pred, real_img, fake_img, lam=10):\n","\treturn wasserstein_loss(y_true, y_pred) + gradient_penalty_loss(y_true, y_pred, real_img, fake_img, lam)\n","\n","\n","# define the standalone critic model\n","def define_critic(in_shape=(28,28,1)):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# weight constraint\n","\tconst = ClipConstraint(0.01)\n","\t# define model\n","\tmodel = Sequential()\n","\t# downsample to 14x14\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n","\t# model.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# downsample to 7x7\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n","\t# model.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# scoring, linear activation\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(1))\n","\n","\treturn model\n","\n","# define the standalone generator model\n","def define_generator(latent_dim):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# define model\n","\tmodel = Sequential()\n","\t# foundation for 7x7 image\n","\tn_nodes = 128 * 7 * 7\n","\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\tmodel.add(Reshape((7, 7, 128)))\n","\t# upsample to 14x14\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# upsample to 28x28\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# output 28x28x1\n","\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n","\treturn model\n","\n","# define the combined generator and critic model, for updating the generator\n","def define_gan(generator, critic, latent_dim, penalty_lambda):\n","\t# make weights in the critic not trainable\n","\tcritic.trainable = False\n","\n","\n","\tz_shape = (latent_dim, )\n","\n","\t# hard coded for testing\n","\timg_shape = (28,28,1)\n","\n","\tz = Input(shape=z_shape)\n","\n","\t# Fake image\n","\tf_img = generator(z)\n","\tf_out = critic(f_img)\n","\n","\t# real image\n","\tr_img = Input(shape = img_shape)\n","\tr_out = critic(r_img)\n","\n","\t# average image\n","\tepsilon = K.placeholder(shape=(None,1,1,1))\n","\ta_img = Input(shape=(img_shape),\n","                  tensor = epsilon * r_img + (1-epsilon) * f_img)\n","\ta_out = critic(a_img)\n","\n","\t# real and fake loss\n","\tr_loss = K.mean(r_out)\n","\tf_loss = K.mean(f_out)\n","\n","\t# gradient penalty\n","\tgrad_mixed = K.gradients(a_out, [a_img])[0]\n","\tnorm_grad_mixed = K.sqrt( K.sum( K.square(grad_mixed), axis=[1,2,3] ) )\n","\tgrad_penalty = penalty_lambda * K.mean( K.square( norm_grad_mixed - 1 ) )\n","\n","\tloss = (f_loss - r_loss) + grad_penalty\n","\n","\t# put together the loss functions and compile models\n","\topt = RMSprop(lr=0.00005)\n","\tcritic.compile(loss=loss, optimizer=opt)\n"," \n","\t# connect them\n","\tmodel = Sequential()\n","\t# add generator\n","\tmodel.add(generator)\n","\t# add the critic\n","\tmodel.add(critic)\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=loss, optimizer=opt)\n","\treturn model\n","\n","# load images\n","def load_real_samples():\n","  kannada_train_url = \"https://raw.githubusercontent.com/JimmyJHickey/ST790-Project/master/Kannada-MNIST/train.csv\"\n","\n","  train = pd.read_csv(kannada_train_url)\n","\n","  X=train.iloc[:,1:].values \n","  Y=train.iloc[:,0].values \n","\n","  X = X.reshape(X.shape[0], 28, 28,1) \n","\n","  # load dataset\n","  # (trainX, trainy), (_, _) = load_data()\n","  # select all of the examples for a given class\n","  # selected_ix = trainy == 7 \n","  # selected_ix += trainy == 5\n","  # X = trainX[selected_ix]\n","  # X = trainX\n","  # expand to 3d, e.g. add channels\n","  # X = expand_dims(X, axis=-1)\n","  # convert from ints to floats\n","\n","  X = X.astype('float32')\n","\n","  # scale from [0,255] to [-1,1]\n","  X = (X - 127.5) / 127.5\n","  return X\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels, -1 for 'real'\n","\ty = -ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n_samples, latent_dim)\n","\treturn x_input\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels with 1.0 for 'fake'\n","\ty = ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","\t# prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = '/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/plots/generated_plot_%04d.png' % (step+1)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\t# save the generator model\n","\tfilename2 = '/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/models/model_%04d.h5' % (step+1)\n","\tg_model.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist):\n","\t# plot history\n","\tpyplot.plot(d1_hist, label='crit_real')\n","\tpyplot.plot(d2_hist, label='crit_fake')\n","\tpyplot.plot(g_hist, label='gen')\n","\tpyplot.legend()\n","\tpyplot.savefig('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/plot_line_plot_loss.png')\n","\tpyplot.close()\n","\n","# train the generator and critic\n","def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(dataset.shape[0] / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","\t# calculate the size of half a batch of samples\n","\thalf_batch = int(n_batch / 2)\n","\t# lists for keeping track of loss\n","\tc1_hist, c2_hist, g_hist = list(), list(), list()\n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# update the critic more than the generator\n","\t\tc1_tmp, c2_tmp = list(), list()\n","\t\tfor _ in range(n_critic):\n","\t\t\t# get randomly selected 'real' samples\n","\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t\t\tc1_tmp.append(c_loss1)\n","\t\t\t# generate 'fake' examples\n","\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\t\t\tc2_tmp.append(c_loss2)\n","\t\t# store critic loss\n","\t\tc1_hist.append(mean(c1_tmp))\n","\t\tc2_hist.append(mean(c2_tmp))\n","\t\t# prepare points in latent space as input for the generator\n","\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t\t# create inverted labels for the fake samples\n","\t\ty_gan = -ones((n_batch, 1))\n","\t\t# update the generator via the critic's error\n","\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","\t\tg_hist.append(g_loss)\n","\t\t# summarize loss on this batch\n","\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n","\t\t# evaluate the model performance every 'epoch'\n","\t\tif (i+1) % bat_per_epo == 0:\n","\t\t\tsummarize_performance(i, g_model, latent_dim)\n","\t# line plots of loss\n","\tplot_history(c1_hist, c2_hist, g_hist)\n","\n","\n","#############\n","#############\n","\n","#############\n","\n","#############\n","#############\n","#############\n","\n","from os import makedirs\n","\n","makedirs('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/models', exist_ok=True)\n","makedirs('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/plots', exist_ok=True)\n","\n","dataset = load_real_samples()\n","print(dataset.shape)\n","\n","# size of the latent space\n","latent_dim = 50\n","n_epochs = 10\n","lam = 10\n","# create the critic\n","critic = define_critic()\n","# create the generator\n","generator = define_generator(latent_dim)\n","\n","# create the gan\n","gan_model = define_gan(generator, critic, latent_dim, lam)\n","\n","# train model\n","train(generator, critic, gan_model, dataset, latent_dim, n_epochs=n_epochs)\n","\n","model_json = generator.to_json()\n","model_json_path = \"/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/model.json\"\n","\n","with open(model_json_path, \"w\") as json_file:\n","    json_file.write(model_json)\n","print(\"Saved model to disk\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000, 28, 28, 1)\n"],"name":"stdout"},{"output_type":"error","ename":"OperatorNotAllowedInGraphError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-df8dce0bdfea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;31m# create the gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-df8dce0bdfea>\u001b[0m in \u001b[0;36mdefine_gan\u001b[0;34m(generator, critic, latent_dim, penalty_lambda)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# put together the loss functions and compile models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# connect them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    535\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    536\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."]}]},{"cell_type":"markdown","metadata":{"id":"5E05mvxC0-dS","colab_type":"text"},"source":["## Kaggle\n","https://www.kaggle.com/amanooo/wgan-gp-keras"]},{"cell_type":"code","metadata":{"id":"GSQWHHoP08eC","colab_type":"code","outputId":"4b9c906c-77c1-4734-ed10-e6c5ccfbda1e","executionInfo":{"status":"error","timestamp":1587994144374,"user_tz":240,"elapsed":33125,"user":{"displayName":"Jimmy Hickey","photoUrl":"","userId":"18081850499597359205"}},"colab":{"base_uri":"https://localhost:8080/","height":531}},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import sys\n","from tqdm import tqdm, tqdm_notebook\n","import glob\n","import shutil\n","import time      # time.perf_counter()\n","import random\n","\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","import xml.etree.ElementTree as ET\n","\n","from keras.layers import Input, Dense, Reshape, Flatten\n","from keras.layers import BatchNormalization, Activation\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import image\n","from keras.initializers import RandomNormal\n","\n","# example of a wgan for generating handwritten digits\n","from numpy import expand_dims\n","from numpy import mean\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.mnist import load_data\n","from keras import backend as K\n","from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.layers import Input\n","from keras.layers.merge import _Merge\n","from keras.initializers import RandomNormal\n","from keras.constraints import Constraint\n","from matplotlib import pyplot\n","import pandas as pd\n","\n","kernel_start = time.perf_counter()\n","kernel_time_limit = 60*60*8.5        #### running time\n","\n","# image size\n","img_size = 28\n","channels = 1\n","img_shape = (img_size, img_size, channels)  \n","\n","# z(latent variable) size\n","z_dim = 100\n","z_shape = (z_dim,)\n","\n","# gradient penalty coefficient \"λ\"\n","penaltyLambda = 10    # d_loss = f_loss - r_loss + λ･penalty\n","\n","# critic(discriminator) iterations per generator iteration\n","trainRatio = 5\n","\n","batch_size = 32        # 16 or 64 better?\n","rec_interval = 10000\n","\n","\n","\n","def build_generator2():\n","  input = Input(shape=z_shape)\n","  x = Dense(4*img_size*img_size, activation=\"relu\")(input)\n","  x = Reshape((img_size//4, img_size//4, -1))(x)\n","  x = UpSampling2D((2, 2))(x)\n","  x = Conv2D(img_size*2, kernel_size=3, strides=1, padding=\"same\",\n","                  use_bias=False)(x)\n","  x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n","  # x = Conv2D(img_size*4, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n","  # x = Activation(\"relu\")(x)\n","  # x = Conv2D(img_size*4, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n","  # x = Activation(\"relu\")(x)\n","  # x = UpSampling2D((2, 2))(x)\n","  # x = Conv2D(img_size*2, kernel_size=3, strides=1, padding=\"same\",\n","  #             use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n","  # x = Conv2D(img_size*2, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n","  # x = Activation(\"relu\")(x)\n","  # x = Conv2D(img_size*2, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n","  # x = Activation(\"relu\")(x)\n","  # x = UpSampling2D((2, 2))(x)\n","  # x = Conv2D(img_size*1, kernel_size=3, strides=1, padding=\"same\",\n","  #                 use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x,training=1)\n","  # x = Conv2D(img_size*1, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n","  # x = Activation(\"relu\")(x)\n","  # x = Conv2D(img_size*1, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  # x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n","  x = Activation(\"relu\")(x)\n","  x = Conv2D(channels, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\",\n","              use_bias=False,)(x)\n","\n","  model = Model(input, x)\n","  # print(\"●generator\")\n","  # model.summary()\n","  return model\n","\n","\n","def build_generator():\n","  init = RandomNormal(stddev=0.02)\n","  # define model\n","  model = Sequential()\n","  # foundation for 7x7 image\n","  n_nodes = 128 * 7 * 7\n","\n","  input = Input(shape = z_shape)\n","\n","  x = Dense(n_nodes, kernel_initializer=init)(input)\n","  x = LeakyReLU(alpha=0.2)(x)\n","  x = Reshape((7, 7, 128))(x)\n","\n","  # upsample to 14x14\n","  x = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(x)\n","  x = BatchNormalization()(x)\n","  x = LeakyReLU(alpha=0.2)(x)\n","  # upsample to 28x28\n","  x = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(x)\n","  x = BatchNormalization()(x)\n","  x = LeakyReLU(alpha=0.2)(x)\n","  # output 28x28x1\n","  x = Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init)(x)\n","\n","  model = Model(input, x)\n","  # print(\"●generator\")\n","  # model.summary()\n","  return model\n","\n","def build_discriminator():\n","  input = Input(shape=img_shape)\n","  x = Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(input)\n","  x = LeakyReLU(0.2)(x)\n","  x = Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n","  x = LeakyReLU(0.2)(x)\n","  x = Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n","  x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  x = LeakyReLU(0.2)(x)\n","  x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n","  x = LeakyReLU(0.2)(x)\n","  x = Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n","  x = LeakyReLU(0.2)(x)\n","  x = Conv2D(1, kernel_size=1, strides=1, padding=\"same\", use_bias=False)(x)\n","  x = Flatten()(x)\n","  x = Dense(units=1, activation=None)(x)   # activation = None\n","  \n","  model = Model(input, x)\n","  # print(\"●discriminator\")\n","  # model.summary()\n","  return model\n","\n","\n","def build_WGANgp(generator, discriminator):\n","  #### model\n","  # generator image(fake image)\n","  z = Input(shape=z_shape)\n","  f_img = generator(z)\n","  f_out = discriminator(f_img)\n","  # real image\n","  r_img = Input(shape=img_shape)\n","  r_out = discriminator(r_img)\n","  # average image\n","  epsilon = K.placeholder(shape=(None,1,1,1))\n","  a_img = Input(shape=(img_shape),\n","                tensor = epsilon * r_img + (1-epsilon) * f_img)\n","  a_out = discriminator(a_img)\n","\n","  #### loss\n","  # original critic(discriminator) loss\n","  r_loss = K.mean(r_out)\n","  f_loss = K.mean(f_out)\n","  # gradient penalty  <this is point of WGAN-gp>\n","  grad_mixed = K.gradients(a_out, [a_img])[0]\n","  norm_grad_mixed = K.sqrt(K.sum(K.square(grad_mixed), axis=[1,2,3]))\n","  grad_penalty = K.mean(K.square(norm_grad_mixed -1))\n","  penalty = penaltyLambda * grad_penalty\n","  # d loss\n","  d_loss = f_loss - r_loss + penalty\n","  \n","  #### discriminator update function\n","  d_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n","              get_updates(discriminator.trainable_weights,[],d_loss)\n","  d_train = K.function([r_img, z, epsilon],\n","                        [r_loss, f_loss, penalty, d_loss],\n","                        d_updates)\n","  \n","  #### generator update function\n","  g_loss = -1. * f_loss\n","  g_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n","              get_updates(generator.trainable_weights,[],g_loss)\n","  g_train = K.function([z], [g_loss], g_updates)\n","\n","  return g_train, d_train\n","  \n","\n","\n","\n","\n","### \n","\n","\n","def load_real_samples():\n","  kannada_train_url = \"https://raw.githubusercontent.com/JimmyJHickey/ST790-Project/master/Kannada-MNIST/train.csv\"\n","\n","  train = pd.read_csv(kannada_train_url)\n","\n","  X=train.iloc[:,1:].values \n","  Y=train.iloc[:,0].values \n","\n","  X = X.reshape(X.shape[0], 28, 28,1) \n","\n","  # load dataset\n","  # (trainX, trainy), (_, _) = load_data()\n","  # select all of the examples for a given class\n","  # selected_ix = trainy == 7 \n","  # selected_ix += trainy == 5\n","  # X = trainX[selected_ix]\n","  # X = trainX\n","  # expand to 3d, e.g. add channels\n","  # X = expand_dims(X, axis=-1)\n","  # convert from ints to floats\n","\n","  X = X.astype('float32')\n","\n","  # scale from [0,255] to [-1,1]\n","  X = (X - 127.5) / 127.5\n","  return X\n","\n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels, -1 for 'real'\n","\ty = -ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n_samples, latent_dim)\n","\treturn x_input\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels with 1.0 for 'fake'\n","\ty = ones((n_samples, 1))\n","\treturn X, y\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","\t# prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = '/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/plots/generated_plot_%04d.png' % (step+1)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\t# save the generator model\n","\tfilename2 = '/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/models/model_%04d.h5' % (step+1)\n","\tg_model.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist):\n","\t# plot history\n","\tpyplot.plot(d1_hist, label='crit_real')\n","\tpyplot.plot(d2_hist, label='crit_fake')\n","\tpyplot.plot(g_hist, label='gen')\n","\tpyplot.legend()\n","\tpyplot.savefig('/content/gdrive/My Drive/Colab Notebooks/kannada_results_baseline_gp/plot_line_plot_loss.png')\n","\tpyplot.close()\n","\n","def sumple_images(imgs, rows=3, cols=3, figsize=(12,10)):\n","    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n","    for indx, axis in enumerate(axes.flatten()):\n","        img = image.array_to_img(imgs[indx])    # ndarray → PIL\n","        imgplot = axis.imshow(img)\n","        axis.set_axis_off()\n","    plt.tight_layout()\n","\n","\n","\n","dataset = load_real_samples()\n","\n","# generator Model\n","generator = build_generator()\n","# discriminator Model\n","discriminator = build_discriminator()\n","# WGAN-gp Training Model\n","G_train, D_train = build_WGANgp(generator, discriminator)\n","\n","# fixed z for confirmation of generated image\n","z_fix = np.random.normal(0, 1, (64, z_dim)) \n","\n","# list for store learning progress data\n","g_loss_list = []\n","r_loss_list = []\n","f_loss_list = []\n","f_r_loss_list = []\n","penalty_list = []\n","d_loss_list = []\n","\n","X_train = load_real_samples()\n","\n","\n","\n","\n","\n","iteration = 0\n","# while time.perf_counter() - kernel_start < kernel_time_limit:\n","\n","while iteration < 100:\n","  print(\"iter: \" , iteration)\n","\n","  #### Discriminator\n","  for j in range(trainRatio):\n","      print(\"discrim: \" , j)\n","      # Generator in\n","      z = np.random.normal(0, 1, (batch_size, z_dim))\n","      # Generator out Images\n","      f_imgs = generator.predict(z)\n","      # Real Images\n","      idx = np.random.randint(0, X_train.shape[0], batch_size)\n","      r_imgs = X_train[idx]\n","      # train the discriminator\n","      epsilon = np.random.uniform(size = (batch_size, 1,1,1))\n","      print(\"epsilon: \", len(epsilon))\n","      r_loss, f_loss, penalty, d_loss = D_train([r_imgs, z, epsilon])\n","\n","  #### Generator\n","  # Generator in\n","  z = np.random.normal(0, 1, (batch_size, z_dim))\n","  # train the generator\n","  g_loss = G_train([z])\n","\n","  #### Record of learning progress\n","  # loss\n","  r_loss_list.append(r_loss)\n","  f_loss_list.append(f_loss)\n","  f_r_loss_list.append(f_loss - r_loss)\n","  penalty_list.append(penalty)\n","  d_loss_list.append(d_loss)\n","  # generated image sumple\n","  if (iteration in [100, 1000]) or (iteration % rec_interval == 0):\n","      print(f'iteration:{iteration} / d_loss:{d_loss:.3f} / g_loss:{sum(g_loss)/len(g_loss):.3f}')\n","      g_imgs = generator.predict(z_fix)\n","      imgs = g_imgs * 127.5 + 127.5\n","      sumple_images(imgs, rows=1, cols=7)\n","      plt.show()\n","\n","  iteration += 1\n","    \n","print(\"last iteration:\",iteration - 1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["iter:  0\n","discrim:  0\n","epsilon:  32\n"],"name":"stdout"},{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e9498cc592a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsilon: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0mr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;31m#### Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m:  Error while reading resource variable _AnonymousVar44 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar44/N10tensorflow3VarE does not exist.\n\t [[node mul_31/ReadVariableOp (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_2581]\n\nFunction call stack:\nkeras_scratch_graph\n"]}]}]}